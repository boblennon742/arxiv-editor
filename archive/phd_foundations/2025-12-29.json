[
    {
        "id": "http://arxiv.org/abs/2512.23425v1",
        "title": "A general framework for deep learning",
        "summary": "This paper develops a general approach for deep learning for a setting that includes nonparametric regression and classification. We perform a framework from data that fulfills a generalized Bernstein-type inequality, including independent, $φ$-mixing, strongly mixing and $\\mathcal{C}$-mixing observations. Two estimators are proposed: a non-penalized deep neural network estimator (NPDNN) and a sparse-penalized deep neural network estimator (SPDNN). For each of these estimators, bounds of the expected excess risk on the class of Hölder smooth functions and composition Hölder functions are established. Applications to independent data, as well as to $φ$-mixing, strongly mixing, $\\mathcal{C}$-mixing processes are considered. For each of these examples, the upper bounds of the expected excess risk of the proposed NPDNN and SPDNN predictors are derived. It is shown that both the NPDNN and SPDNN estimators are minimax optimal (up to a logarithmic factor) in many classical settings.",
        "authors": "William Kengne, Modou Wade",
        "url": "http://arxiv.org/abs/2512.23425v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23425v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文为深度学习提出了一个通用框架，适用于多种数据类型（如φ-mixing, strongly mixing等），并建立了预期超额风险的界限，证明了所提估计器（NPDNN和SPDNN）在许多经典设置下是极小极大最优的。其理论基础极其强大，数学推导严谨，与您对理论严谨性和数学推导的偏好高度契合。"
    },
    {
        "id": "http://arxiv.org/abs/2512.23619v1",
        "title": "The N-5 Scaling Law: Topological Dimensionality Reduction in the Optimal Design of Fully-actuated Multirotors",
        "summary": "The geometric design of fully-actuated and omnidirectional N-rotor aerial vehicles is conventionally formulated as a parametric optimization problem, seeking a single optimal set of N orientations within a fixed architectural family. This work departs from that paradigm to investigate the intrinsic topological structure of the optimization landscape itself. We formulate the design problem on the product manifold of Projective Lines \\RP^2^N, fixing the rotor positions to the vertices of polyhedral chassis while varying their lines of action. By minimizing a coordinate-invariant Log-Volume isotropy metric, we reveal that the topology of the global optima is governed strictly by the symmetry of the chassis. For generic (irregular) vertex arrangements, the solutions appear as a discrete set of isolated points. However, as the chassis geometry approaches regularity, the solution space undergoes a critical phase transition, collapsing onto an N-dimensional Torus of the lines tangent at the vertexes to the circumscribing sphere of the chassis, and subsequently reducing to continuous 1-dimensional curves driven by Affine Phase Locking. We synthesize these observations into the N-5 Scaling Law: an empirical relationship holding for all examined regular planar polygons and Platonic solids (N <= 10), where the space of optimal configurations consists of K=N-5 disconnected 1D topological branches. We demonstrate that these locking patterns correspond to a sequence of admissible Star Polygons {N/q}, allowing for the exact prediction of optimal phases for arbitrary N. Crucially, this topology reveals a design redundancy that enables optimality-preserving morphing: the vehicle can continuously reconfigure along these branches while preserving optimal isotropic control authority.",
        "authors": "Antonio Franchi",
        "url": "http://arxiv.org/abs/2512.23619v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23619v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "这篇论文从拓扑结构角度研究多旋翼飞行器设计优化问题，提出了N-5缩放定律。它将设计问题公式化为在射影线乘积流形上的优化，并最小化坐标不变的Log-Volume各向同性度量。涉及流形、拓扑、相变等概念，数学推导和理论严谨性极强，是纯数学与工程结合的典范。"
    },
    {
        "id": "http://arxiv.org/abs/2512.23324v1",
        "title": "On Conformant Planning and Model-Checking of $\\exists^*\\forall^*$ Hyperproperties",
        "summary": "We study the connection of two problems within the planning and verification community: Conformant planning and model-checking of hyperproperties. Conformant planning is the task of finding a sequential plan that achieves a given objective independent of non-deterministic action effects during the plan's execution. Hyperproperties are system properties that relate multiple execution traces of a system and, e.g., capture information-flow and fairness policies. In this paper, we show that model-checking of $\\exists^*\\forall^*$ hyperproperties is closely related to the problem of computing a conformant plan. Firstly, we show that we can efficiently reduce a hyperproperty model-checking instance to a conformant planning instance, and prove that our encoding is sound and complete. Secondly, we establish the converse direction: Every conformant planning problem is, itself, a hyperproperty model-checking task.",
        "authors": "Raven Beutner, Bernd Finkbeiner",
        "url": "http://arxiv.org/abs/2512.23324v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23324v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "该论文从结构和拓扑角度研究视觉表示学习，提出了视觉语言假设。从该假设推导了两个理论结果，涉及纤维丛、商空间、拓扑变化等概念。理论严谨性极强，为理解视觉学习提供了深刻的数学基础，非常符合您对严谨数学逻辑应用于AI系统的兴趣。"
    },
    {
        "id": "http://arxiv.org/abs/2512.23602v1",
        "title": "Distribution-Free Process Monitoring with Conformal Prediction",
        "summary": "Traditional Statistical Process Control (SPC) is essential for quality management but is limited by its reliance on often violated statistical assumptions, leading to unreliable monitoring in modern, complex manufacturing environments. This paper introduces a hybrid framework that enhances SPC by integrating the distribution free, model agnostic guarantees of Conformal Prediction. We propose two novel applications: Conformal-Enhanced Control Charts, which visualize process uncertainty and enable proactive signals like 'uncertainty spikes', and Conformal-Enhanced Process Monitoring, which reframes multivariate control as a formal anomaly detection problem using an intuitive p-value chart. Our framework provides a more robust and statistically rigorous approach to quality control while maintaining the interpretability and ease of use of classic methods.",
        "authors": "Christopher Burger",
        "url": "http://arxiv.org/abs/2512.23602v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23602v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文提出了一个结合Conformal Prediction的混合框架，用于免分布过程监控。Conformal Prediction提供了模型无关的统计保证，使得监控更加鲁棒和严谨。其强大的统计理论基础和清晰的数学推导，以及对“统计保证”的强调，使其成为您的理想选择。"
    },
    {
        "id": "http://arxiv.org/abs/2512.23596v1",
        "title": "The Nonstationarity-Complexity Tradeoff in Return Prediction",
        "summary": "We investigate machine learning models for stock return prediction in non-stationary environments, revealing a fundamental nonstationarity-complexity tradeoff: complex models reduce misspecification error but require longer training windows that introduce stronger non-stationarity. We resolve this tension with a novel model selection method that jointly optimizes model class and training window size using a tournament procedure that adaptively evaluates candidates on non-stationary validation data. Our theoretical analysis demonstrates that this approach balances misspecification error, estimation variance, and non-stationarity, performing close to the best model in hindsight. Applying our method to 17 industry portfolio returns, we consistently outperform standard rolling-window benchmarks, improving out-of-sample $R^2$ by 14-23% on average. During NBER-designated recessions, improvements are substantial: our method achieves positive $R^2$ during the Gulf War recession while benchmarks are negative, and improves $R^2$ in absolute terms by at least 80bps during the 2001 recession as well as superior performance during the 2008 Financial Crisis. Economically, a trading strategy based on our selected model generates 31% higher cumulative returns averaged across the industries.",
        "authors": "Agostino Capponi, Chengpiao Huang, J. Antonio Sidaoui, Kaizheng Wang, Jiacheng Zou",
        "url": "http://arxiv.org/abs/2512.23596v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23596v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文揭示了非平稳性-复杂性权衡，并提出了一个联合优化模型类别和训练窗口大小的新型模型选择方法。理论分析证明了该方法在平衡误差、方差和非平稳性方面的有效性。具有强大的统计和时间序列理论基础，对金融或经济领域的AI应用有重要意义。"
    },
    {
        "id": "http://arxiv.org/abs/2512.23467v1",
        "title": "Propensity Patchwork Kriging for Scalable Inference on Heterogeneous Treatment Effects",
        "summary": "Gaussian process-based models are attractive for estimating heterogeneous treatment effects (HTE), but their computational cost limits scalability in causal inference settings. In this work, we address this challenge by extending Patchwork Kriging into the causal inference framework. Our proposed method partitions the data according to the estimated propensity score and applies Patchwork Kriging to enforce continuity of HTE estimates across adjacent regions. By imposing continuity constraints only along the propensity score dimension, rather than the full covariate space, the proposed approach substantially reduces computational cost while avoiding discontinuities inherent in simple local approximations. The resulting method can be interpreted as a smoothing extension of stratification and provides an efficient approach to HTE estimation. The proposed method is demonstrated through simulation studies and a real data application.",
        "authors": "Hajime Ogawa, Shonosuke Sugasawa",
        "url": "http://arxiv.org/abs/2512.23467v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23467v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文将Patchwork Kriging扩展到因果推断框架，通过倾向得分划分数据并强制HTE估计的连续性。该方法提供了高效的HTE估计方法，具有强大的因果推断和统计理论基础，数学推导严谨，非常符合您对因果逻辑和统计保证的追求。"
    },
    {
        "id": "http://arxiv.org/abs/2512.23461v1",
        "title": "Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance",
        "summary": "Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, leading response length to become one of the inevitable inductive biases. A limited number of prior RM debiasing approaches either target a single specific type of bias or model the problem with only simple linear correlations, \\textit{e.g.}, Pearson coefficients. To mitigate more complex and diverse inductive biases in reward modeling, we introduce a novel information-theoretic debiasing method called \\textbf{D}ebiasing via \\textbf{I}nformation optimization for \\textbf{R}M (DIR). Inspired by the information bottleneck (IB), we maximize the mutual information (MI) between RM scores and human preference pairs, while minimizing the MI between RM outputs and biased attributes of preference inputs. With theoretical justification from information theory, DIR can handle more sophisticated types of biases with non-linear correlations, broadly extending the real-world application scenarios for RM debiasing methods. In experiments, we verify the effectiveness of DIR with three types of inductive biases: \\textit{response length}, \\textit{sycophancy}, and \\textit{format}. We discover that DIR not only effectively mitigates target inductive biases but also enhances RLHF performance across diverse benchmarks, yielding better generalization abilities. The code and training recipes are available at https://github.com/Qwen-Applications/DIR.",
        "authors": "Zhuo Li, Pengyu Cheng, Zhechao Yu, Feifei Tong, Anningzhe Gao, Tsung-Hui Chang, Xiang Wan, Erchao Zhao, Xiaoxi Jiang, Guanjun Jiang",
        "url": "http://arxiv.org/abs/2512.23461v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23461v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文提出了一个受信息瓶颈启发的、基于信息论的去偏方法DIR，用于奖励模型。通过最大化和最小化互信息来处理复杂的非线性偏差。具有强大的信息论和统计理论基础，数学推导严谨，对于理解和改进现代AI系统的偏差问题具有深远的理论价值。"
    },
    {
        "id": "http://arxiv.org/abs/2512.23130v1",
        "title": "PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion",
        "summary": "We present PathoSyn, a unified generative framework for Magnetic Resonance Imaging (MRI) image synthesis that reformulates imaging-pathology as a disentangled additive deviation on a stable anatomical manifold. Current generative models typically operate in the global pixel domain or rely on binary masks, these paradigms often suffer from feature entanglement, leading to corrupted anatomical substrates or structural discontinuities. PathoSyn addresses these limitations by decomposing the synthesis task into deterministic anatomical reconstruction and stochastic deviation modeling. Central to our framework is a Deviation-Space Diffusion Model designed to learn the conditional distribution of pathological residuals, thereby capturing localized intensity variations while preserving global structural integrity by construction. To ensure spatial coherence, the diffusion process is coupled with a seam-aware fusion strategy and an inference-time stabilization module, which collectively suppress boundary artifacts and produce high-fidelity internal lesion heterogeneity. PathoSyn provides a mathematically principled pipeline for generating high-fidelity patient-specific synthetic datasets, facilitating the development of robust diagnostic algorithms in low-data regimes. By allowing interpretable counterfactual disease progression modeling, the framework supports precision intervention planning and provides a controlled environment for benchmarking clinical decision-support systems. Quantitative and qualitative evaluations on tumor imaging benchmarks demonstrate that PathoSyn significantly outperforms holistic diffusion and mask-conditioned baselines in both perceptual realism and anatomical fidelity. The source code of this work will be made publicly available.",
        "authors": "Jian Wang, Sixing Rong, Jiarui Xing, Yuling Xu, Weide Liu",
        "url": "http://arxiv.org/abs/2512.23130v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23130v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文提出了优化二元分类中广义度量（如Fβ-measure）的原则性算法METRO。将问题重构为广义成本敏感学习，并设计了具有可证明H-一致性保证的替代损失函数。具有强大的统计学习理论和优化理论基础，数学推导严谨，对解决实际分类问题中的度量优化挑战具有重要意义。"
    },
    {
        "id": "http://arxiv.org/abs/2512.23627v1",
        "title": "Joint Modeling of Longitudinal and Survival Data: A Bayesian Approach for Predicting Disease Progression",
        "summary": "Joint modeling of longitudinal and survival data has become increasingly important in medical research, particularly for understanding disease progression in chronic conditions where both repeated biomarker measurements and time-to-event outcomes are available. Traditional two-stage methods, which analyze longitudinal and survival components separately, often result in biased estimates and suboptimal predictions due to failure to account for their interdependence.   In this study, we propose a Bayesian hierarchical joint modeling framework with an emphasis on predictive evaluation and clinical interpretability. The model simultaneously characterizes the longitudinal trajectory of a biomarker and the associated survival outcome through shared random effects, capturing the intrinsic association between disease dynamics and event risk. The Bayesian formulation allows flexible incorporation of prior information, accommodates irregular measurement times and missing data, and provides full posterior distributions for uncertainty quantification via credible intervals.   We evaluate the proposed framework using both simulated data designed to mimic realistic patient trajectories and a real-world clinical dataset involving patients with chronic liver disease. Results demonstrate that the Bayesian joint model consistently outperforms conventional two-stage approaches in terms of parameter estimation accuracy and predictive performance, as measured by time-dependent area under the curve and Brier scores. The proposed approach provides a robust and interpretable tool for dynamic, patient-specific prognosis, supporting clinical decision-making in personalized medicine.",
        "authors": "Nithisha Suryadevara, Vivek Reddy Srigiri",
        "url": "http://arxiv.org/abs/2512.23627v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23627v1",
        "scores": {
            "Novelty": 3,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文提出了一个贝叶斯分层联合模型，用于纵向数据和生存数据的联合建模。模型同时刻画生物标志物轨迹和生存结果，通过共享随机效应捕捉关联。贝叶斯框架、不确定性量化、参数估计准确性等都体现了强大的统计理论基础和严谨的数学推导，对医学研究和预测有重要价值。"
    },
    {
        "id": "http://arxiv.org/abs/2512.23444v1",
        "title": "Uncertainty calibration for latent-variable regression models",
        "summary": "Uncertainty quantification is essential for scientific analysis, as it allows for the evaluation and interpretation of variability and reliability in complex systems and datasets. In their original form, multivariate statistical regression models (partial least-squares regression, PLS, principal component regression, PCR) along with their kernelized versions (kernel partial least-squares regression, K-PLS, kernel principal component regression, K-PCR), do not incorporate uncertainty quantification as part of their output. In this study, we propose a method inspired by conformal inference to estimate and calibrate the uncertainty of multivariate statistical models. The result of this method is a point prediction accompanied by prediction intervals that depend on the input data. We tested the proposed method on both traditional and kernelized versions of PLS and PCR. The method is demonstrated using synthetic data, as well as laboratory near-infrared (NIR) and airborne hyperspectral regression models for estimating functional plant traits. The model was able to successfully identify the uncertain regions in the simulated data and match the magnitude of the uncertainty. In real-case scenarios, the optimised model was not overconfident nor underconfident when estimating from test data: for example, for a 95% prediction interval, 95% of the true observations were inside the prediction interval.",
        "authors": "Zina-Sabrina Duma, Otto Lamminpää, Jouni Susiluoto, Heikki Haario, Ting Zheng, Tuomas Sihvonen, Amy Braverman, Philip A. Townsend, Satu-Pia Reinikainen",
        "url": "http://arxiv.org/abs/2512.23444v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23444v1",
        "scores": {
            "Novelty": 3,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文提出了一种受共形推断启发的校准方法，用于估计和校准潜在变量回归模型的不确定性。该方法提供了带预测区间的点预测，具有强大的统计保证和不确定性量化能力，数学推导严谨，对于科学分析中的可靠性评估至关重要。"
    },
    {
        "id": "http://arxiv.org/abs/2512.23408v1",
        "title": "Probabilistic Modelling is Sufficient for Causal Inference",
        "summary": "Causal inference is a key research area in machine learning, yet confusion reigns over the tools needed to tackle it. There are prevalent claims in the machine learning literature that you need a bespoke causal framework or notation to answer causal questions. In this paper, we want to make it clear that you \\emph{can} answer any causal inference question within the realm of probabilistic modelling and inference, without causal-specific tools or notation. Through concrete examples, we demonstrate how causal questions can be tackled by writing down the probability of everything. Lastly, we reinterpret causal tools as emerging from standard probabilistic modelling and inference, elucidating their necessity and utility.",
        "authors": "Bruno Mlodozeniec, David Krueger, Richard E. Turner",
        "url": "http://arxiv.org/abs/2512.23408v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23408v1",
        "scores": {
            "Novelty": 3,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 5
        },
        "reason_zh": "这篇论文提出并论证了因果推断问题可以在概率建模和推断的框架内解决，无需特定的因果工具或符号。通过具体例子重新解释了因果工具的必要性和效用。具有极强的理论严谨性和概念清晰度，对理解因果推断的数学基础非常有价值，是您深入研究因果逻辑的良好起点。"
    },
    {
        "id": "http://arxiv.org/abs/2512.23405v1",
        "title": "On the Sample Complexity of Learning for Blind Inverse Problems",
        "summary": "Blind inverse problems arise in many experimental settings where the forward operator is partially or entirely unknown. In this context, methods developed for the non-blind case cannot be adapted in a straightforward manner. Recently, data-driven approaches have been proposed to address blind inverse problems, demonstrating strong empirical performance and adaptability. However, these methods often lack interpretability and are not supported by rigorous theoretical guarantees, limiting their reliability in applied domains such as imaging inverse problems. In this work, we shed light on learning in blind inverse problems within the simplified yet insightful framework of Linear Minimum Mean Square Estimators (LMMSEs). We provide an in-depth theoretical analysis, deriving closed-form expressions for optimal estimators and extending classical results. In particular, we establish equivalences with suitably chosen Tikhonov-regularized formulations, where the regularization depends explicitly on the distributions of the unknown signal, the noise, and the random forward operators. We also prove convergence results under appropriate source condition assumptions. Furthermore, we derive rigorous finite-sample error bounds that characterize the performance of learned estimators as a function of the noise level, problem conditioning, and number of available samples. These bounds explicitly quantify the impact of operator randomness and reveal the associated convergence rates as this randomness vanishes. Finally, we validate our theoretical findings through illustrative numerical experiments that confirm the predicted convergence behavior.",
        "authors": "Nathan Buskulic, Luca Calatroni, Lorenzo Rosasco, Silvia Villa",
        "url": "http://arxiv.org/abs/2512.23405v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23405v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "该论文在线性最小均方估计器（LMMSE）框架内，对盲逆问题中的学习进行了深入的理论分析。推导了最优估计器的闭式表达式、与Tikhonov正则化的等价性、收敛性结果以及严格的有限样本误差界。理论严谨性极强，数学推导清晰，对于理解和解决逆问题中的学习挑战具有基础性贡献。"
    },
    {
        "id": "http://arxiv.org/abs/2512.23312v1",
        "title": "Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants",
        "summary": "Deep neural networks have accelerated inverse-kinematics (IK) inference to the point where low cost manipulators can execute complex trajectories in real time, yet the opaque nature of these models contradicts the transparency and safety requirements emerging in responsible AI regulation. This study proposes an explainability centered workflow that integrates Shapley-value attribution with physics-based obstacle avoidance evaluation for the ROBOTIS OpenManipulator-X. Building upon the original IKNet, two lightweight variants-Improved IKNet with residual connections and Focused IKNet with position-orientation decoupling are trained on a large, synthetically generated pose-joint dataset. SHAP is employed to derive both global and local importance rankings, while the InterpretML toolkit visualizes partial-dependence patterns that expose non-linear couplings between Cartesian poses and joint angles. To bridge algorithmic insight and robotic safety, each network is embedded in a simulator that subjects the arm to randomized single and multi-obstacle scenes; forward kinematics, capsule-based collision checks, and trajectory metrics quantify the relationship between attribution balance and physical clearance. Qualitative heat maps reveal that architectures distributing importance more evenly across pose dimensions tend to maintain wider safety margins without compromising positional accuracy. The combined analysis demonstrates that explainable AI(XAI) techniques can illuminate hidden failure modes, guide architectural refinements, and inform obstacle aware deployment strategies for learning based IK. The proposed methodology thus contributes a concrete path toward trustworthy, data-driven manipulation that aligns with emerging responsible-AI standards.",
        "authors": "Sheng-Kai Chen, Yi-Ling Tsai, Chun-Chih Chang, Yan-Chen Chen, Po-Chiang Lin",
        "url": "http://arxiv.org/abs/2512.23312v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23312v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "论文研究了顺应性规划和超性质模型检查之间的联系。证明了超性质模型检查实例可以高效地归约到顺应性规划实例，并证明了编码的完备性和正确性。反之亦然。具有强大的形式化方法和逻辑理论严谨性，对于您在AI系统中应用严谨数学逻辑的兴趣非常匹配。"
    },
    {
        "id": "http://arxiv.org/abs/2512.23161v1",
        "title": "Diffusion-based Decentralized Federated Multi-Task Representation Learning",
        "summary": "Representation learning is a widely adopted framework for learning in data-scarce environments to obtain a feature extractor or representation from various different yet related tasks. Despite extensive research on representation learning, decentralized approaches remain relatively underexplored. This work develops a decentralized projected gradient descent-based algorithm for multi-task representation learning. We focus on the problem of multi-task linear regression in which multiple linear regression models share a common, low-dimensional linear representation. We present an alternating projected gradient descent and minimization algorithm for recovering a low-rank feature matrix in a diffusion-based decentralized and federated fashion. We obtain constructive, provable guarantees that provide a lower bound on the required sample complexity and an upper bound on the iteration complexity of our proposed algorithm. We analyze the time and communication complexity of our algorithm and show that it is fast and communication-efficient. We performed numerical simulations to validate the performance of our algorithm and compared it with benchmark algorithms.",
        "authors": "Donghwa Kang, Shana Moothedath",
        "url": "http://arxiv.org/abs/2512.23161v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23161v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "该论文提出了一个基于扩散的去中心化投影梯度下降算法，用于多任务表示学习。提供了可证明的保证，包括样本复杂度和迭代复杂度的上下界，并分析了时间和通信复杂度。理论严谨性极强，数学推导清晰，对于分布式学习和优化理论有重要贡献。"
    }
]