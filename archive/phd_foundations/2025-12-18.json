[
    {
        "id": "http://arxiv.org/abs/2512.16857v1",
        "title": "Identification and efficient estimation of compliance and network causal effects in cluster-randomized trials",
        "summary": "Treatment noncompliance is pervasive in infectious disease cluster-randomized trials. Although all individuals within a cluster are assigned the same treatment condition, the treatment uptake status may vary across individuals due to noncompliance. We propose a semiparametric framework to evaluate the individual compliance effect and network assignment effect within principal stratum exhibiting different patterns of noncompliance. The individual compliance effect captures the portion of the treatment effect attributable to changes in treatment receipt, while the network assignment effect reflects the pure impact of treatment assignment and spillover among individuals within the same cluster. Unlike prior efforts which either empirically identify or interval identify these estimands, we characterize new structural assumptions for nonparametric point identification. We then develop semiparametrically efficient estimators that combine data-adaptive machine learning methods with efficient influence functions to enable more robust inference. Additionally, we introduce sensitivity analysis methods to study the impact under assumption violations, and apply the proposed methods to reanalyze a cluster-randomized trial in Kenya that evaluated the impact of school-based mass deworming on disease transmission.",
        "authors": "Chao Cheng, Georgia Papadogeorgou, Fan Li",
        "url": "http://arxiv.org/abs/2512.16857v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16857v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "这篇论文专注于因果推断和统计估计，提出了半参数框架、非参数点识别、高效影响函数和敏感性分析。其对“非参数点识别”和“半参数高效估计量”的明确提及，以及对“高效影响函数”的运用，完美契合您对统计保证、因果逻辑和清晰数学推导的偏好。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16733v1",
        "title": "Discovering and Learning Probabilistic Models of Black-Box AI Capabilities",
        "summary": "Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.",
        "authors": "Daniel Bramblett, Rushang Karia, Adrian Ciotinga, Ruthvick Suresh, Pulkit Verma, YooJung Choi, Siddharth Srivastava",
        "url": "http://arxiv.org/abs/2512.16733v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16733v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文旨在为黑盒AI系统学习PDDL风格的概率模型，其“理论结果展示了学习模型的健全性、完备性和收敛性”。这直接满足了您对优化收敛性和理论保证的严格要求。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16430v1",
        "title": "Multi-Fidelity Delayed Acceptance: hierarchical MCMC sampling for Bayesian inverse problems combining multiple solvers through deep neural networks",
        "summary": "Inverse uncertainty quantification (UQ) tasks such as parameter estimation are computationally demanding whenever dealing with physics-based models, and typically require repeated evaluations of complex numerical solvers. When partial differential equations are involved, full-order models such as those based on the Finite Element Method can make traditional sampling approaches like Markov Chain Monte Carlo (MCMC) computationally infeasible. Although data-driven surrogate models may help reduce evaluation costs, their utility is often limited by the expense of generating high-fidelity data. In contrast, low-fidelity data can be produced more efficiently, although relying on them alone may degrade the accuracy of the inverse UQ solution.   To address these challenges, we propose a Multi-Fidelity Delayed Acceptance scheme for Bayesian inverse problems. Extending the Multi-Level Delayed Acceptance framework, the method introduces multi-fidelity neural networks that combine the predictions of solvers of varying fidelity, with high fidelity evaluations restricted to an offline training stage. During the online phase, likelihood evaluations are obtained by evaluating the coarse solvers and passing their outputs to the trained neural networks, thereby avoiding additional high-fidelity simulations.   This construction allows heterogeneous coarse solvers to be incorporated consistently within the hierarchy, providing greater flexibility than standard Multi-Level Delayed Acceptance. The proposed approach improves the approximation accuracy of the low fidelity solvers, leading to longer sub-chain lengths, better mixing, and accelerated posterior inference. The effectiveness of the strategy is demonstrated on two benchmark inverse problems involving (i) steady isotropic groundwater flow, (ii) an unsteady reaction-diffusion system, for which substantial computational savings are obtained.",
        "authors": "Filippo Zacchei, Paolo Conti, Attilio Alberto Frangi, Andrea Manzoni",
        "url": "http://arxiv.org/abs/2512.16430v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16430v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文扩展了多层延迟接受框架，将多保真神经网络引入MCMC采样，用于贝叶斯逆问题。MCMC方法的理论基础和收敛性分析是统计学中的核心内容，非常符合您对理论严谨性的要求。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16386v1",
        "title": "Quantitative Verification of Fairness in Tree Ensembles",
        "summary": "This work focuses on quantitative verification of fairness in tree ensembles. Unlike traditional verification approaches that merely return a single counterexample when the fairness is violated, quantitative verification estimates the ratio of all counterexamples and characterizes the regions where they occur, which is important information for diagnosing and mitigating bias. To date, quantitative verification has been explored almost exclusively for deep neural networks (DNNs). Representative methods, such as DeepGemini and FairQuant, all build on the core idea of Counterexample-Guided Abstraction Refinement, a generic framework that could be adapted to other model classes. We extended the framework into a model-agnostic form, but discovered two limitations: (i) it can provide only lower bounds, and (ii) its performance scales poorly. Exploiting the discrete structure of tree ensembles, our work proposes an efficient quantification technique that delivers any-time upper and lower bounds. Experiments on five widely used datasets demonstrate its effectiveness and efficiency. When applied to fairness testing, our quantification method significantly outperforms state-of-the-art testing techniques.",
        "authors": "Zhenjiang Zhao, Takahisa Toda, Takashi Kitamura",
        "url": "http://arxiv.org/abs/2512.16386v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16386v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文关注树集成模型公平性的定量验证，提出了能够提供上下界的有效量化技术。这种对模型属性进行“定量验证”和“提供上下界”的方法，体现了强大的形式化验证和数学分析基础，高度符合您的偏好。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16383v1",
        "title": "Multivariate Uncertainty Quantification with Tomographic Quantile Forests",
        "summary": "Quantifying predictive uncertainty is essential for safe and trustworthy real-world AI deployment. Yet, fully nonparametric estimation of conditional distributions remains challenging for multivariate targets. We propose Tomographic Quantile Forests (TQF), a nonparametric, uncertainty-aware, tree-based regression model for multivariate targets. TQF learns conditional quantiles of directional projections $\\mathbf{n}^{\\top}\\mathbf{y}$ as functions of the input $\\mathbf{x}$ and the unit direction $\\mathbf{n}$. At inference, it aggregates quantiles across many directions and reconstructs the multivariate conditional distribution by minimizing the sliced Wasserstein distance via an efficient alternating scheme with convex subproblems. Unlike classical directional-quantile approaches that typically produce only convex quantile regions and require training separate models for different directions, TQF covers all directions with a single model without imposing convexity restrictions. We evaluate TQF on synthetic and real-world datasets, and release the source code on GitHub.",
        "authors": "Takuya Kanazawa",
        "url": "http://arxiv.org/abs/2512.16383v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16383v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "该论文提出了Tomographic Quantile Forests (TQF) 用于多元不确定性量化，通过最小化切片Wasserstein距离重构条件分布。其在非参数统计、条件分布估计和优化理论（Sliced Wasserstein距离、凸子问题）方面的严谨性非常突出，且清晰度高。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16363v1",
        "title": "Empirical Likelihood Meets Prediction-Powered Inference",
        "summary": "We study inference with a small labeled sample, a large unlabeled sample, and high-quality predictions from an external model. We link prediction-powered inference with empirical likelihood by stacking supervised estimating equations based on labeled outcomes with auxiliary moment conditions built from predictions, and then optimizing empirical likelihood under these joint constraints. The resulting empirical likelihood-based prediction-powered inference (EPI) estimator is asymptotically normal, has asymptotic variance no larger than the fully supervised estimator, and attains the semiparametric efficiency bound when the auxiliary functions span the predictable component of the supervised score. For hypothesis testing and confidence sets, empirical likelihood ratio statistics admit chi-squared-type limiting distributions. As a by-product, the empirical likelihood weights induce a calibrated empirical distribution that integrates supervised and prediction-based information, enabling estimation and uncertainty quantification for general functionals beyond parameters defined by estimating equations. We present two practical implementations: one based on basis expansions in the predictions and covariates, and one that learns an approximately optimal auxiliary function by cross-fitting. In simulations and applications, EPI reduces mean squared error and shortens confidence intervals while maintaining nominal coverage.",
        "authors": "Guanghui Wang, Mengtao Wen, Changliang Zou",
        "url": "http://arxiv.org/abs/2512.16363v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16363v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "这篇论文将预测驱动推断与经验似然结合，证明了估计量的渐近正态性、渐近方差界和半参数效率界。这些是统计推断中的核心理论结果，具有极高的数学严谨性，完美符合您的研究方向。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16336v1",
        "title": "Hazard-based distributional regression via ordinary differential equations",
        "summary": "The hazard function is central to the formulation of commonly used survival regression models such as the proportional hazards and accelerated failure time models. However, these models rely on a shared baseline hazard, which, when specified parametrically, can only capture limited shapes. To overcome this limitation, we propose a general class of parametric survival regression models obtained by modelling the hazard function using autonomous systems of ordinary differential equations (ODEs). Covariate information is incorporated via transformed linear predictors on the parameters of the ODE system. Our framework capitalises on the interpretability of parameters in common ODE systems, enabling the identification of covariate values that produce qualitatively distinct hazard shapes associated with different attractors of the system of ODEs. This provides deeper insights into how covariates influence survival dynamics. We develop efficient Bayesian computational tools, including parallelised evaluation of the log-posterior, which facilitates integration with general-purpose Markov Chain Monte Carlo samplers. We also derive conditions for posterior asymptotic normality, enabling fast approximations of the posterior. A central contribution of our work lies in the case studies. We demonstrate the methodology using clinical trial data with crossing survival curves, and a study of cancer recurrence times where our approach reveals how the efficacy of interventions (treatments) on hazard and survival are influenced by patient characteristics.",
        "authors": "J. A. Christen, F. J. Rubio",
        "url": "http://arxiv.org/abs/2512.16336v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16336v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "论文通过常微分方程（ODEs）对风险函数进行建模，推导了后验渐近正态性的条件，并结合了贝叶斯计算工具。这种将统计建模与ODE理论、贝叶斯推断相结合的方法，具有强大的理论深度和数学推导，清晰度也极高。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16231v1",
        "title": "An Efficient Framework for Robust Sample Size Determination",
        "summary": "In many settings, robust data analysis involves computational methods for uncertainty quantification and statistical inference. To design frequentist studies that leverage robust analysis methods, suitable sample sizes to achieve desired power are often found by estimating sampling distributions of p-values via intensive simulation. Moreover, most sample size recommendations rely heavily on assumptions about a single data-generating process. Consequently, robustness in data analysis does not by itself imply robustness in study design, as examining sample size sensitivity to data-generating assumptions typically requires further simulations. We propose an economical alternative for determining sample sizes that are robust to multiple data-generating mechanisms. Applying our theoretical results that model p-values as a function of the sample size, we assess power across the sample size space using simulations conducted at only two sample sizes for each data-generating mechanism. We demonstrate the broad applicability of our methodology to study design based on M-estimators in both experimental and observational settings through a varied set of clinical examples.",
        "authors": "Luke Hagar, Andrew J. Martin",
        "url": "http://arxiv.org/abs/2512.16231v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16231v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "该论文提出了鲁棒样本量确定的高效框架，基于p值作为样本量函数的理论结果，并适用于M-估计量。其在统计推断、功效分析和M-估计量理论方面的贡献，展现了极强的统计学严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16227v1",
        "title": "An Information-Theoretic Framework for Robust Large Language Model Editing",
        "summary": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.",
        "authors": "Qizhou Chen, Chengyu Wang, Taolin Zhang, Xiaofeng He",
        "url": "http://arxiv.org/abs/2512.16227v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16227v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "这篇论文提出了一个基于信息瓶颈理论的LLM编辑框架，旨在精确压缩和隔离信息以实现鲁棒的模型编辑。信息瓶颈理论提供了强大的信息论基础，非常符合您对理论基础的追求。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16185v1",
        "title": "Weighted K-Harmonic Means Clustering: Convergence Analysis and Applications to Wireless Communications",
        "summary": "We propose the \\emph{weighted K-harmonic means} (WKHM) clustering algorithm, a regularized variant of K-harmonic means designed to ensure numerical stability while enabling soft assignments through inverse-distance weighting. Unlike classical K-means and constrained K-means, WKHM admits a direct interpretation in wireless networks: its weights are exactly equivalent to fractional user association based on received signal strength. We establish rigorous convergence guarantees under both deterministic and stochastic settings, addressing key technical challenges arising from non-convexity and random initialization. Specifically, we prove monotone descent to a local minimum under fixed initialization, convergence in probability under Binomial Point Process (BPP) initialization, and almost sure convergence under mild decay conditions. These results provide the first stochastic convergence guarantees for harmonic-mean-based clustering. Finally, through extensive simulations with diverse user distributions, we show that WKHM achieves a superior tradeoff between minimum signal strength and load fairness compared to classical and modern clustering baselines, making it a principled tool for joint radio node placement and user association in wireless networks.",
        "authors": "Gourab Ghatak",
        "url": "http://arxiv.org/abs/2512.16185v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16185v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "论文提出了加权K-调和均值聚类算法，并建立了严格的收敛性保证，包括单调下降、概率收敛和几乎必然收敛。这种对算法收敛性的深入数学分析，是您所看重的理论严谨性的典范。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16141v1",
        "title": "Existence of Solutions for Non-monotone VIs and Implications for Games",
        "summary": "In this paper, we study the existence of solutions in non-monotone variational inequalities (VIs) through the normal mapping properties. In particular, we show that when the normal mapping $F_K^{\\rm nor}(\\cdot)$ is norm coercive over a set $K$, and the generalized Jacobian of the normal mapping has a full rank at points $x$ where $F_K^{\\rm nor}(x)\\ne0$, then the VI$(K,F)$ has a solution. We then investigate conditions on the mapping $F(\\cdot)$ and its Jacobian that imply the full rank condition for the generalized Jacobian, such as the uniform P-function and the uniform P-matrix condition. Subsequently, we focus on VIs arising from games and interpret our main result in a game setting. Based on the P$_Υ$-matrix condition, we provide a sufficient condition for a game to have a Nash equilibrium. Additionally, through examples we show that our sufficient conditions can be used to assert the existence of a solution to a VI, or a quasi-Nash in a game, while the existing results relying on the uniform P-function property or the P$_Υ$-matrix condition cannot be employed.",
        "authors": "Sina Arefizadeh, Angelia Nedić",
        "url": "http://arxiv.org/abs/2512.16141v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16141v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "这是一篇纯理论数学论文，研究非单调变分不等式解的存在性及其对博弈论的影响，通过正规映射性质和广义雅可比矩阵。其极强的数学严谨性，是您作为数理统计博士生会非常欣赏的。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16046v1",
        "title": "CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting",
        "summary": "Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.",
        "authors": "Shu Wan, Reepal Shah, John Sabo, Huan Liu, K. Selçuk Candan",
        "url": "http://arxiv.org/abs/2512.16046v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16046v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "该论文提出了因果时空表示学习框架，共同学习因果图和路由图，并建立了非参数设置下这些因果结构的可识别性条件。其对因果推断、图学习和可识别性条件的深入理论分析，与您的因果逻辑偏好高度契合。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16515v1",
        "title": "The Universe Learning Itself: On the Evolution of Dynamics from the Big Bang to Machine Intelligence",
        "summary": "We develop a unified, dynamical-systems narrative of the universe that traces a continuous chain of structure formation from the Big Bang to contemporary human societies and their artificial learning systems. Rather than treating cosmology, astrophysics, geophysics, biology, cognition, and machine intelligence as disjoint domains, we view each as successive regimes of dynamics on ever-richer state spaces, stitched together by phase transitions, symmetry-breaking events, and emergent attractors. Starting from inflationary field dynamics and the growth of primordial perturbations, we describe how gravitational instability sculpts the cosmic web, how dissipative collapse in baryonic matter yields stars and planets, and how planetary-scale geochemical cycles define long-lived nonequilibrium attractors. Within these attractors, we frame the origin of life as the emergence of self-maintaining reaction networks, evolutionary biology as flow on high-dimensional genotype-phenotype-environment manifolds, and brains as adaptive dynamical systems operating near critical surfaces. Human culture and technology-including modern machine learning and artificial intelligence-are then interpreted as symbolic and institutional dynamics that implement and refine engineered learning flows which recursively reshape their own phase space. Throughout, we emphasize recurring mathematical motifs-instability, bifurcation, multiscale coupling, and constrained flows on measure-zero subsets of the accessible state space. Our aim is not to present any new cosmological or biological model, but a cross-scale, theoretical perspective: a way of reading the universe's history as the evolution of dynamics itself, culminating (so far) in biological and artificial systems capable of modeling, predicting, and deliberately perturbing their own future trajectories.",
        "authors": "Pradeep Singh, Mudasani Rushikesh, Bezawada Sri Sai Anurag, Balasubramanian Raman",
        "url": "http://arxiv.org/abs/2512.16515v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16515v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "这篇论文提出了一种从宇宙大爆炸到机器智能的统一动力学系统叙事，强调了不稳定性、分岔、多尺度耦合和吸引子等数学主题。虽然不是直接的AI模型推导，但其高度的理论和概念严谨性，以及对“动力学”的深刻洞察，与您对严谨数学逻辑的追求相符，且清晰度极高。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16452v1",
        "title": "Smart Data Portfolios: A Quantitative Framework for Input Governance in AI",
        "summary": "Growing concerns about fairness, privacy, robustness, and transparency have made it a central expectation of AI governance that automated decisions be explainable by institutions and intelligible to affected parties. We introduce the Smart Data Portfolio (SDP) framework, which treats data categories as productive but risk-bearing assets, formalizing input governance as an information-risk trade-off. Within this framework, we define two portfolio-level quantities, Informational Return and Governance-Adjusted Risk, whose interaction characterizes data mixtures and generates a Governance-Efficient Frontier. Regulators shape this frontier through risk caps, admissible categories, and weight bands that translate fairness, privacy, robustness, and provenance requirements into measurable constraints on data allocation while preserving model flexibility. A telecommunications illustration shows how different AI services require distinct portfolios within a common governance structure. The framework offers a familiar portfolio logic as an input-level explanation layer suited to the large-scale deployment of AI systems.",
        "authors": "A. Talha Yalta, A. Yasemin Yalta",
        "url": "http://arxiv.org/abs/2512.16452v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16452v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文提出了一个量化框架Smart Data Portfolio用于AI输入治理，将数据类别视为风险资产，形式化了信息-风险权衡，并定义了新的量化指标和治理效率前沿。这种将AI治理问题形式化为量化框架的方法，具有强大的理论基础和数学建模思想。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16277v1",
        "title": "Sharpness-aware Second-order Latent Factor Model for High-dimensional and Incomplete Data",
        "summary": "Second-order Latent Factor (SLF) model, a class of low-rank representation learning methods, has proven effective at extracting node-to-node interaction patterns from High-dimensional and Incomplete (HDI) data. However, its optimization is notoriously difficult due to its bilinear and non-convex nature. Sharpness-aware Minimization (SAM) has recently proposed to find flat local minima when minimizing non-convex objectives, thereby improving the generalization of representation-learning models. To address this challenge, we propose a Sharpness-aware SLF (SSLF) model. SSLF embodies two key ideas: (1) acquiring second-order information via Hessian-vector products; and (2) injecting a sharpness term into the curvature (Hessian) through the designed Hessian-vector products. Experiments on multiple industrial datasets demonstrate that the proposed model consistently outperforms state-of-the-art baselines.",
        "authors": "Jialiang Wang, Xueyan Bao, Hao Wu",
        "url": "http://arxiv.org/abs/2512.16277v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16277v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文提出了Sharpness-aware Second-order Latent Factor (SSLF) 模型，通过Hessian-向量积获取二阶信息并注入锐度项，解决了非凸优化难题。其对优化理论的深入探讨和对二阶信息的利用，与您对优化收敛性和数学推导的兴趣高度吻合。"
    }
]