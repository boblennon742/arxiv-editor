[
    {
        "id": "http://arxiv.org/abs/2602.16612v1",
        "title": "Causal and Compositional Abstraction",
        "summary": "Abstracting from a low level to a more explanatory high level of description, and ideally while preserving causal structure, is fundamental to scientific practice, to causal inference problems, and to robust, efficient and interpretable AI. We present a general account of abstractions between low and high level models as natural transformations, focusing on the case of causal models. This provides a new formalisation of causal abstraction, unifying several notions in the literature, including constructive causal abstraction, Q-$τ$ consistency, abstractions based on interchange interventions, and `distributed' causal abstractions. Our approach is formalised in terms of category theory, and uses the general notion of a compositional model with a given set of queries and semantics in a monoidal, cd- or Markov category; causal models and their queries such as interventions being special cases. We identify two basic notions of abstraction: downward abstractions mapping queries from high to low level; and upward abstractions, mapping concrete queries such as Do-interventions from low to high. Although usually presented as the latter, we show how common causal abstractions may, more fundamentally, be understood in terms of the former. Our approach also leads us to consider a new stronger notion of `component-level' abstraction, applying to the individual components of a model. In particular, this yields a novel, strengthened form of constructive causal abstraction at the mechanism-level, for which we prove characterisation results. Finally, we show that abstraction can be generalised to further compositional models, including those with a quantum semantics implemented by quantum circuits, and we take first steps in exploring abstractions between quantum compositional circuit models and high-level classical causal models as a means to explainable quantum AI.",
        "authors": "Robin Lorenz, Sean Tull",
        "url": "http://arxiv.org/abs/2602.16612v1",
        "pdf_url": "https://arxiv.org/pdf/2602.16612v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文在理论创新性上表现卓越，它利用范畴论（Category Theory）为因果抽象（Causal Abstraction）提供了一个统一的、形式化的框架。这对于理解和构建更鲁棒、高效和可解释的AI系统至关重要，与您作为数理统计博士生对理论严谨性的追求高度契合。它不仅统一了现有概念，还提出了新的更强的抽象形式，具有深远的理论意义。"
    },
    {
        "id": "http://arxiv.org/abs/2602.16109v1",
        "title": "Federated Graph AGI for Cross-Border Insider Threat Intelligence in Government Financial Schemes",
        "summary": "Cross-border insider threats pose a critical challenge to government financial schemes, particularly when dealing with distributed, privacy-sensitive data across multiple jurisdictions. Existing approaches face fundamental limitations: they cannot effectively share intelligence across borders due to privacy constraints, lack reasoning capabilities to understand complex multi-step attack patterns, and fail to capture intricate graph-structured relationships in financial networks. We introduce FedGraph-AGI, a novel federated learning framework integrating Artificial General Intelligence (AGI) reasoning with graph neural networks for privacy-preserving cross-border insider threat detection. Our approach combines: (1) federated graph neural networks preserving data sovereignty; (2) Mixture-of-Experts (MoE) aggregation for heterogeneous jurisdictions; and (3) AGI-powered reasoning via Large Action Models (LAM) performing causal inference over graph data. Through experiments on a 50,000-transaction dataset across 10 jurisdictions, FedGraph-AGI achieves 92.3% accuracy, significantly outperforming federated baselines (86.1%) and centralized approaches (84.7%). Our ablation studies reveal AGI reasoning contributes 6.8% improvement, while MoE adds 4.4%. The system maintains epsilon = 1.0 differential privacy while achieving near-optimal performance and scales efficiently to 50+ clients. This represents the first integration of AGI reasoning with federated graph learning for insider threat detection, opening new directions for privacy-preserving cross-border intelligence sharing.",
        "authors": "Srikumar Nayak, James Walmesley",
        "url": "http://arxiv.org/abs/2602.16109v1",
        "pdf_url": "https://arxiv.org/pdf/2602.16109v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "这篇论文在创新性和实践影响力上都非常突出。它提出了FedGraph-AGI框架，将联邦图神经网络、专家混合（MoE）聚合以及AGI驱动的因果推理（通过大型行动模型LAM）结合起来，用于跨国界内部威胁检测。这不仅解决了数据隐私和复杂推理的实际瓶颈，还在理论上融合了多种前沿AI范式，具有极高的理论和应用价值。"
    },
    {
        "id": "http://arxiv.org/abs/2602.16698v1",
        "title": "Causality is Key for Interpretability Claims to Generalise",
        "summary": "Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.",
        "authors": "Shruti Joshi, Aaron Mueller, David Klindt, Wieland Brendel, Patrik Reizinger, Dhanya Sridhar",
        "url": "http://arxiv.org/abs/2602.16698v1",
        "pdf_url": "https://arxiv.org/pdf/2602.16698v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "该论文深入探讨了LLM可解释性中的核心问题，即因果解释的有效性。它引入了Pearl的因果层级和因果表示学习（CRL）来严格界定可解释性研究的证据支持范围。这对于构建可信赖的AI至关重要，具有极高的理论严谨性和实践指导意义，完美契合您对理论创新性和解决实际瓶颈（可解释性）的需求。"
    },
    {
        "id": "http://arxiv.org/abs/2602.16634v1",
        "title": "Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models",
        "summary": "The rare-event sampling problem has long been the central limiting factor in molecular dynamics (MD), especially in biomolecular simulation. Recently, diffusion models such as BioEmu have emerged as powerful equilibrium samplers that generate independent samples from complex molecular distributions, eliminating the cost of sampling rare transition events. However, a sampling problem remains when computing observables that rely on states which are rare in equilibrium, for example folding free energies. Here, we introduce enhanced diffusion sampling, enabling efficient exploration of rare-event regions while preserving unbiased thermodynamic estimators. The key idea is to perform quantitatively accurate steering protocols to generate biased ensembles and subsequently recover equilibrium statistics via exact reweighting. We instantiate our framework in three algorithms: UmbrellaDiff (umbrella sampling with diffusion models), $Δ$G-Diff (free-energy differences via tilted ensembles), and MetaDiff (a batchwise analogue for metadynamics). Across toy systems, protein folding landscapes and folding free energies, our methods achieve fast, accurate, and scalable estimation of equilibrium properties within GPU-minutes to hours per system -- closing the rare-event sampling gap that remained after the advent of diffusion-model equilibrium samplers.",
        "authors": "Yu Xie, Ludwig Winkler, Lixin Sun, Sarah Lewis, Adam E. Foster, José Jiménez Luna, Tim Hempel, Michael Gastegger, Yaoyi Chen, Iryna Zaporozhets, Cecilia Clementi, Christopher M. Bishop, Frank Noé",
        "url": "http://arxiv.org/abs/2602.16634v1",
        "pdf_url": "https://arxiv.org/pdf/2602.16634v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "这篇论文在扩散模型（Diffusion Models）领域带来了显著的理论和实践突破。它提出了“增强扩散采样”方法，通过精确的重加权技术，高效地解决了分子动力学中稀有事件采样和自由能计算的长期瓶颈。这不仅具有高度的理论创新性，还为生物分子模拟等科学计算领域提供了强大的新工具，直接解决了数据效率问题。"
    },
    {
        "id": "http://arxiv.org/abs/2602.16601v1",
        "title": "Error Propagation and Model Collapse in Diffusion Models: A Theoretical Study",
        "summary": "Machine learning models are increasingly trained or fine-tuned on synthetic data. Recursively training on such data has been observed to significantly degrade performance in a wide range of tasks, often characterized by a progressive drift away from the target distribution. In this work, we theoretically analyze this phenomenon in the setting of score-based diffusion models. For a realistic pipeline where each training round uses a combination of synthetic data and fresh samples from the target distribution, we obtain upper and lower bounds on the accumulated divergence between the generated and target distributions. This allows us to characterize different regimes of drift, depending on the score estimation error and the proportion of fresh data used in each generation. We also provide empirical results on synthetic data and images to illustrate the theory.",
        "authors": "Nail B. Khelifa, Richard E. Turner, Ramji Venkataramanan",
        "url": "http://arxiv.org/abs/2602.16601v1",
        "pdf_url": "https://arxiv.org/pdf/2602.16601v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "该论文对扩散模型中的错误传播和模型崩溃现象进行了理论分析，尤其是在递归使用合成数据进行训练的场景下。它提供了生成分布与目标分布之间累积散度的上下界，并刻画了漂移的不同机制。这对于理解和缓解生成式AI（尤其是扩散模型）中的核心算法问题具有重要的理论严谨性和指导意义。"
    },
    {
        "id": "http://arxiv.org/abs/2602.16570v1",
        "title": "Steering diffusion models with quadratic rewards: a fine-grained analysis",
        "summary": "Inference-time algorithms are an emerging paradigm in which pre-trained models are used as subroutines to solve downstream tasks. Such algorithms have been proposed for tasks ranging from inverse problems and guided image generation to reasoning. However, the methods currently deployed in practice are heuristics with a variety of failure modes -- and we have very little understanding of when these heuristics can be efficiently improved.   In this paper, we consider the task of sampling from a reward-tilted diffusion model -- that is, sampling from $p^{\\star}(x) \\propto p(x) \\exp(r(x))$ -- given a reward function $r$ and pre-trained diffusion oracle for $p$. We provide a fine-grained analysis of the computational tractability of this task for quadratic rewards $r(x) = x^\\top A x + b^\\top x$. We show that linear-reward tilts are always efficiently sampleable -- a simple result that seems to have gone unnoticed in the literature. We use this as a building block, along with a conceptually new ingredient -- the Hubbard-Stratonovich transform -- to provide an efficient algorithm for sampling from low-rank positive-definite quadratic tilts, i.e. $r(x) = x^\\top A x$ where $A$ is positive-definite and of rank $O(1)$. For negative-definite tilts, i.e. $r(x) = - x^\\top A x$ where $A$ is positive-definite, we prove that the problem is intractable even if $A$ is of rank 1 (albeit with exponentially-large entries).",
        "authors": "Ankur Moitra, Andrej Risteski, Dhruv Rohatgi",
        "url": "http://arxiv.org/abs/2602.16570v1",
        "pdf_url": "https://arxiv.org/pdf/2602.16570v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文对奖励倾斜扩散模型（reward-tilted diffusion models）的计算可处理性进行了细致的分析，特别是针对二次奖励函数。它引入了Hubbard-Stratonovich变换，并证明了线性奖励和低秩正定二次奖励的可高效采样性，同时证明了负定二次奖励的不可处理性。这在理论上极具创新性和严谨性，对于理解和改进扩散模型的引导生成算法至关重要。"
    },
    {
        "id": "http://arxiv.org/abs/2602.16498v1",
        "title": "Fast and Scalable Analytical Diffusion",
        "summary": "Analytical diffusion models offer a mathematically transparent path to generative modeling by formulating the denoising score as an empirical-Bayes posterior mean. However, this interpretability comes at a prohibitive cost: the standard formulation necessitates a full-dataset scan at every timestep, scaling linearly with dataset size. In this work, we present the first systematic study addressing this scalability bottleneck. We challenge the prevailing assumption that the entire training data is necessary, uncovering the phenomenon of Posterior Progressive Concentration: the effective golden support of the denoising score is not static but shrinks asymptotically from the global manifold to a local neighborhood as the signal-to-noise ratio increases. Capitalizing on this, we propose Dynamic Time-Aware Golden Subset Diffusion (GoldDiff), a training-free framework that decouples inference complexity from dataset size. Instead of static retrieval, GoldDiff uses a coarse-to-fine mechanism to dynamically pinpoint the ''Golden Subset'' for inference. Theoretically, we derive rigorous bounds guaranteeing that our sparse approximation converges to the exact score. Empirically, GoldDiff achieves a $\\bf 71 \\times$ speedup on AFHQ while matching or achieving even better performance than full-scan baselines. Most notably, we demonstrate the first successful scaling of analytical diffusion to ImageNet-1K, unlocking a scalable, training-free paradigm for large-scale generative modeling.",
        "authors": "Xinyi Shang, Peng Sun, Jingyu Lin, Zhiqiang Shen",
        "url": "http://arxiv.org/abs/2602.16498v1",
        "pdf_url": "https://arxiv.org/pdf/2602.16498v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "该论文首次系统性地解决了分析扩散模型（Analytical Diffusion Models）的扩展性瓶颈。它发现了“后验渐进集中”现象，并基于此提出了GoldDiff框架，将推理复杂度从数据集大小中解耦。这不仅在理论上具有高度创新性，还通过实现71倍加速和首次将分析扩散扩展到ImageNet-1K，显著提升了生成模型的效率和可扩展性。"
    },
    {
        "id": "http://arxiv.org/abs/2602.16363v1",
        "title": "Improved Bounds for Reward-Agnostic and Reward-Free Exploration",
        "summary": "We study reward-free and reward-agnostic exploration in episodic finite-horizon Markov decision processes (MDPs), where an agent explores an unknown environment without observing external rewards. Reward-free exploration aims to enable $ε$-optimal policies for any reward revealed after exploration, while reward-agnostic exploration targets $ε$-optimality for rewards drawn from a small finite class. In the reward-agnostic setting, Li, Yan, Chen, and Fan achieve minimax sample complexity, but only for restrictively small accuracy parameter $ε$. We propose a new algorithm that significantly relaxes the requirement on $ε$. Our approach is novel and of technical interest by itself. Our algorithm employs an online learning procedure with carefully designed rewards to construct an exploration policy, which is used to gather data sufficient for accurate dynamics estimation and subsequent computation of an $ε$-optimal policy once the reward is revealed. Finally, we establish a tight lower bound for reward-free exploration, closing the gap between known upper and lower bounds.",
        "authors": "Oran Ridel, Alon Cohen",
        "url": "http://arxiv.org/abs/2602.16363v1",
        "pdf_url": "https://arxiv.org/pdf/2602.16363v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "这篇论文在强化学习理论方面取得了重要进展，为奖励无关（reward-agnostic）和奖励自由（reward-free）探索提供了改进的界限。它提出了一种新算法，显著放宽了对准确性参数的要求，并为奖励自由探索建立了紧致的下界。这对于理解和设计更高效、更鲁棒的RL探索算法具有基础性理论价值。"
    },
    {
        "id": "http://arxiv.org/abs/2602.16274v1",
        "title": "Regret and Sample Complexity of Online Q-Learning via Concentration of Stochastic Approximation with Time-Inhomogeneous Markov Chains",
        "summary": "We present the first high-probability regret bound for classical online Q-learning in infinite-horizon discounted Markov decision processes, without relying on optimism or bonus terms. We first analyze Boltzmann Q-learning with decaying temperature and show that its regret depends critically on the suboptimality gap of the MDP: for sufficiently large gaps, the regret is sublinear, while for small gaps it deteriorates and can approach linear growth. To address this limitation, we study a Smoothed $ε_n$-Greedy exploration scheme that combines $ε_n$-greedy and Boltzmann exploration, for which we prove a gap-robust regret bound of near-$\\tilde{O}(N^{9/10})$. To analyze these algorithms, we develop a high-probability concentration bound for contractive Markovian stochastic approximation with iterate- and time-dependent transition dynamics. This bound may be of independent interest as the contraction factor in our bound is governed by the mixing time and is allowed to converge to one asymptotically.",
        "authors": "Rahul Singh, Siddharth Chandak, Eric Moulines, Vivek S. Borkar, Nicholas Bambos",
        "url": "http://arxiv.org/abs/2602.16274v1",
        "pdf_url": "https://arxiv.org/pdf/2602.16274v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文首次为无限视野折扣马尔可夫决策过程中的经典在线Q-学习提供了高概率后悔界限，且不依赖于乐观或奖励项。它还引入了Smoothed εn-Greedy探索方案，并为此证明了对子最优间隙鲁棒的后悔界限。这在强化学习理论中具有极高的严谨性和基础性贡献，对于Q-学习的理论理解和算法改进至关重要。"
    },
    {
        "id": "http://arxiv.org/abs/2602.16256v1",
        "title": "Color-based Emotion Representation for Speech Emotion Recognition",
        "summary": "Speech emotion recognition (SER) has traditionally relied on categorical or dimensional labels. However, this technique is limited in representing both the diversity and interpretability of emotions. To overcome this limitation, we focus on color attributes, such as hue, saturation, and value, to represent emotions as continuous and interpretable scores. We annotated an emotional speech corpus with color attributes via crowdsourcing and analyzed them. Moreover, we built regression models for color attributes in SER using machine learning and deep learning, and explored the multitask learning of color attribute regression and emotion classification. As a result, we demonstrated the relationship between color attributes and emotions in speech, and successfully developed color attribute regression models for SER. We also showed that multitask learning improved the performance of each task.",
        "authors": "Ryotaro Nagase, Ryoichi Takashima, Yoichi Yamashita",
        "url": "http://arxiv.org/abs/2602.16256v1",
        "pdf_url": "https://arxiv.org/pdf/2602.16256v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "这篇论文对Wasserstein和Gromov-Wasserstein最优传输计划的稀疏性、极值结构和单调性属性进行了深入的理论探讨。最优传输是机器学习中的一个基本数学工具，该研究加深了对GW距离的理解，对于您作为数理统计博士生在理论层面的探索非常有价值，尽管其直接实践影响力可能不如其他LLM应用论文。"
    },
    {
        "id": "http://arxiv.org/abs/2602.16218v1",
        "title": "Bayesian Quadrature: Gaussian Processes for Integration",
        "summary": "Bayesian quadrature is a probabilistic, model-based approach to numerical integration, the estimation of intractable integrals, or expectations. Although Bayesian quadrature was popularised already in the 1980s, no systematic and comprehensive treatment has been published. The purpose of this survey is to fill this gap. We review the mathematical foundations of Bayesian quadrature from different points of view; present a systematic taxonomy for classifying different Bayesian quadrature methods along the three axes of modelling, inference, and sampling; collect general theoretical guarantees; and provide a controlled numerical study that explores and illustrates the effect of different choices along the axes of the taxonomy. We also provide a realistic assessment of practical challenges and limitations to application of Bayesian quadrature methods and include an up-to-date and nearly exhaustive bibliography that covers not only machine learning and statistics literature but all areas of mathematics and engineering in which Bayesian quadrature or equivalent methods have seen use.",
        "authors": "Maren Mahsereci, Toni Karvonen",
        "url": "http://arxiv.org/abs/2602.16218v1",
        "pdf_url": "https://arxiv.org/pdf/2602.16218v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "虽然是一篇综述论文，但它对贝叶斯积分（Bayesian Quadrature）进行了系统而全面的处理，填补了该领域缺乏系统性论述的空白。它回顾了数学基础，提出了分类法，收集了理论保证，并进行了数值研究。贝叶斯积分是机器学习和统计学中估计难处理积分的关键概率方法，其理论严谨性和对领域知识的整合，使其成为一篇极具价值的理论基础性论文。"
    },
    {
        "id": "http://arxiv.org/abs/2602.16196v1",
        "title": "Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning",
        "summary": "Coordinating large populations of interacting agents is a central challenge in multi-agent reinforcement learning (MARL), where the size of the joint state-action space scales exponentially with the number of agents. Mean-field methods alleviate this burden by aggregating agent interactions, but these approaches assume homogeneous interactions. Recent graphon-based frameworks capture heterogeneity, but are computationally expensive as the number of agents grows. Therefore, we introduce $\\texttt{GMFS}$, a $\\textbf{G}$raphon $\\textbf{M}$ean-$\\textbf{F}$ield $\\textbf{S}$ubsampling framework for scalable cooperative MARL with heterogeneous agent interactions. By subsampling $κ$ agents according to interaction strength, we approximate the graphon-weighted mean-field and learn a policy with sample complexity $\\mathrm{poly}(κ)$ and optimality gap $O(1/\\sqrtκ)$. We verify our theory with numerical simulations in robotic coordination, showing that $\\texttt{GMFS}$ achieves near-optimal performance.",
        "authors": "Emile Anand, Richard Hoffmann, Sarah Liaw, Adam Wierman",
        "url": "http://arxiv.org/abs/2602.16196v1",
        "pdf_url": "https://arxiv.org/pdf/2602.16196v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文提出了GMFS框架，通过图核平均场子采样（Graphon Mean-Field Subsampling）实现异构多智能体强化学习（MARL）的可扩展合作。它提供了严格的理论保证（样本复杂度和最优性差距），并解决了大规模异构智能体群体协调的根本性挑战。这在MARL理论和算法上具有高度创新性和严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2602.16198v1",
        "title": "Training-Free Adaptation of Diffusion Models via Doob's $h$-Transform",
        "summary": "Adaptation methods have been a workhorse for unlocking the transformative power of pre-trained diffusion models in diverse applications. Existing approaches often abstract adaptation objectives as a reward function and steer diffusion models to generate high-reward samples. However, these approaches can incur high computational overhead due to additional training, or rely on stringent assumptions on the reward such as differentiability. Moreover, despite their empirical success, theoretical justification and guarantees are seldom established. In this paper, we propose DOIT (Doob-Oriented Inference-time Transformation), a training-free and computationally efficient adaptation method that applies to generic, non-differentiable rewards. The key framework underlying our method is a measure transport formulation that seeks to transport the pre-trained generative distribution to a high-reward target distribution. We leverage Doob's $h$-transform to realize this transport, which induces a dynamic correction to the diffusion sampling process and enables efficient simulation-based computation without modifying the pre-trained model. Theoretically, we establish a high probability convergence guarantee to the target high-reward distribution via characterizing the approximation error in the dynamic Doob's correction. Empirically, on D4RL offline RL benchmarks, our method consistently outperforms state-of-the-art baselines while preserving sampling efficiency.",
        "authors": "Qijie Zhu, Zeqi Ye, Han Liu, Zhaoran Wang, Minshuo Chen",
        "url": "http://arxiv.org/abs/2602.16198v1",
        "pdf_url": "https://arxiv.org/pdf/2602.16198v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "这篇论文提出了DOIT（Doob-Oriented Inference-time Transformation），一种无训练、计算高效的扩散模型适应方法，利用Doob的h-变换实现。它基于测度传输（measure transport）理论，并提供了高概率收敛保证。这在理论上非常严谨，并且通过实现对通用、不可微奖励的适应，解决了扩散模型适应的计算开销瓶颈，具有重要的实践意义。"
    },
    {
        "id": "http://arxiv.org/abs/2602.16183v1",
        "title": "Multi-Agent Combinatorial-Multi-Armed-Bandit framework for the Submodular Welfare Problem under Bandit Feedback",
        "summary": "We study the \\emph{Submodular Welfare Problem} (SWP), where items are partitioned among agents with monotone submodular utilities to maximize the total welfare under \\emph{bandit feedback}. Classical SWP assumes full value-oracle access, achieving $(1-1/e)$ approximations via continuous-greedy algorithms. We extend this to a \\emph{multi-agent combinatorial bandit} framework (\\textsc{MA-CMAB}), where actions are partitions under full-bandit feedback with non-communicating agents. Unlike prior single-agent or separable multi-agent CMAB models, our setting couples agents through shared allocation constraints. We propose an explore-then-commit strategy with randomized assignments, achieving $\\tilde{\\mathcal{O}}(T^{2/3})$ regret against a $(1-1/e)$ benchmark, the first such guarantee for partition-based submodular welfare problem under bandit feedback.",
        "authors": "Subham Pokhriyal, Shweta Jain, Vaneet Aggarwal",
        "url": "http://arxiv.org/abs/2602.16183v1",
        "pdf_url": "https://arxiv.org/pdf/2602.16183v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文将子模福利问题（Submodular Welfare Problem）扩展到多智能体组合多臂老虎机（MA-CMAB）框架，其中智能体之间通过共享分配约束耦合。它提出了一种带有随机分配的探索-提交策略，并首次为这种设置提供了后悔界限。这在多智能体决策和资源分配理论中具有高度的创新性和严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2602.16165v1",
        "title": "HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents",
        "summary": "Training LLMs as interactive agents for multi-turn decision-making remains challenging, particularly in long-horizon tasks with sparse and delayed rewards, where agents must execute extended sequences of actions before receiving meaningful feedback. Most existing reinforcement learning (RL) approaches model LLM agents as flat policies operating at a single time scale, selecting one action at each turn. In sparse-reward settings, such flat policies must propagate credit across the entire trajectory without explicit temporal abstraction, which often leads to unstable optimization and inefficient credit assignment.   We propose HiPER, a novel Hierarchical Plan-Execute RL framework that explicitly separates high-level planning from low-level execution. HiPER factorizes the policy into a high-level planner that proposes subgoals and a low-level executor that carries them out over multiple action steps. To align optimization with this structure, we introduce a key technique called hierarchical advantage estimation (HAE), which carefully assigns credit at both the planning and execution levels. By aggregating returns over the execution of each subgoal and coordinating updates across the two levels, HAE provides an unbiased gradient estimator and provably reduces variance compared to flat generalized advantage estimation.   Empirically, HiPER achieves state-of-the-art performance on challenging interactive benchmarks, reaching 97.4\\% success on ALFWorld and 83.3\\% on WebShop with Qwen2.5-7B-Instruct (+6.6\\% and +8.3\\% over the best prior method), with especially large gains on long-horizon tasks requiring multiple dependent subtasks. These results highlight the importance of explicit hierarchical decomposition for scalable RL training of multi-turn LLM agents.",
        "authors": "Jiangweizhi Peng, Yuanxin Liu, Ruida Zhou, Charles Fleming, Zhaoran Wang, Alfredo Garcia, Mingyi Hong",
        "url": "http://arxiv.org/abs/2602.16165v1",
        "pdf_url": "https://arxiv.org/pdf/2602.16165v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 4,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "这篇论文提出了HiPER，一个新颖的分层规划-执行强化学习框架，用于LLM智能体。它将策略分解为高级规划器和低级执行器，并引入了分层优势估计（HAE）来解决稀疏和延迟奖励下的信用分配问题。这在LLM智能体架构和RL算法上具有显著创新，并显著提高了LLM智能体在长程任务中的性能和训练效率，直接解决了LLM应用瓶颈。"
    }
]