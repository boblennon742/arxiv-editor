[
    {
        "id": "http://arxiv.org/abs/2602.23152v1",
        "title": "The Trinity of Consistency as a Defining Principle for General World Models",
        "summary": "The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.",
        "authors": "Jingxuan Wei, Siyuan Li, Yuhang Xu, Zheng Sun, Junjie Jiang, Hexuan Jin, Caijun Jia, Honghao He, Xinglong Xu, Xi bai, Chang Yu, Yumou Liu, Junnan Zhu, Xuanhe Zhou, Jintao Chen, Xiaobin Hu, Shancheng Pang, Bihui Yu, Ran He, Zhen Lei, Stan Z. Li, Conghui He, Shuicheng Yan, Cheng Tan",
        "url": "http://arxiv.org/abs/2602.23152v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23152v1",
        "type": "前沿深度",
        "scores": {
            "Novelty": 5,
            "Rigor": 4,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "提出通用世界模型的理论框架“一致性三位一体”，指引AGI和多模态学习的未来方向。",
        "reason_zh": "这篇论文深入探讨了构建通用世界模型（AGI的核心挑战）所需的理论框架，提出了“一致性三位一体”原则。它不仅提供了高层次的理论指导，还能帮助您理解LLM理论机制的未来演进方向，视野开阔，指引性强。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20934v1",
        "title": "Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence",
        "summary": "The paradigm of Large Language Models is undergoing a fundamental transition from static inference engines to dynamic autonomous cognitive systems.While current research primarily focuses on scaling context windows or optimizing prompt engineering the theoretical bridge between micro scale token processing and macro scale systemic intelligence remains fragmented.This paper proposes AgentOS,a holistic conceptual framework that redefines the LLM as a \"Reasoning Kernel\" governed by structured operating system logic.Central to this architecture is Deep Context Management which conceptualizes the context window as an Addressable Semantic Space rather than a passive buffer.We systematically deconstruct the transition from discrete sequences to coherent cognitive states introducing mechanisms for Semantic Slicing and Temporal Alignment to mitigate cognitive drift in multi-agent orchestration.By mapping classical OS abstractions such as memory paging interrupt handling and process scheduling onto LLM native constructs, this review provides a rigorous roadmap for architecting resilient scalable and self-evolving cognitive environments.Our analysis asserts that the next frontier of AGI development lies in the architectural efficiency of system-level coordination.",
        "authors": "ChengYou Li, XiaoDong Liu, XiangBao Meng, XinYu Zhao",
        "url": "http://arxiv.org/abs/2602.20934v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20934v1",
        "type": "前沿深度",
        "scores": {
            "Novelty": 5,
            "Rigor": 4,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "提出AgentOS概念框架，从操作系统视角重构LLM，连接微观处理与宏观系统智能，深化LLM理论机制。",
        "reason_zh": "该文将LLM视为“推理内核”，并以操作系统逻辑构建AgentOS，探讨了深度上下文管理、语义切片和时间对齐等机制，以实现系统级智能。这为理解LLM的理论机制和未来Agent架构提供了深刻而新颖的视角，具有极高的研究指导价值。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20571v1",
        "title": "CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation",
        "summary": "Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems.",
        "authors": "Ayush Sawarni, Jiyuan Tan, Vasilis Syrgkanis",
        "url": "http://arxiv.org/abs/2602.20571v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20571v1",
        "type": "前沿深度",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "针对自动化因果推断提出解耦评估基准，区分识别与估计，推动因果推断与LLM结合的严谨性。",
        "reason_zh": "这篇论文提出了一个创新的因果推断基准CausalReasoningBenchmark，旨在解耦评估因果识别和估计，这对于理解和改进LLM在因果推断中的应用至关重要。它直接回应了您对“因果推断”前沿研究的关注，并强调了评估的严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2602.19837v1",
        "title": "Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent",
        "summary": "Humans are highly effective at utilizing prior knowledge to adapt to novel tasks, a capability that standard machine learning models struggle to replicate due to their reliance on task-specific training. Meta-learning overcomes this limitation by allowing models to acquire transferable knowledge from various tasks, enabling rapid adaptation to new challenges with minimal data. This survey provides a rigorous, task-based formalization of meta-learning and meta-reinforcement learning and uses that paradigm to chronicle the landmark algorithms that paved the way for DeepMind's Adaptive Agent, consolidating the essential concepts needed to understand the Adaptive Agent and other generalist approaches.",
        "authors": "Björn Hoppmann, Christoph Scholz",
        "url": "http://arxiv.org/abs/2602.19837v1",
        "pdf_url": "https://arxiv.org/pdf/2602.19837v1",
        "type": "前沿深度",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "严谨综述元学习和元强化学习，系统梳理其发展路径，为理解高级自适应智能体奠定基础。",
        "reason_zh": "这是一篇对元学习和元强化学习的严谨综述，系统地追溯了通向DeepMind自适应智能体的路径。它直接契合您对“Offline RL”等前沿领域的需求，提供了全面的理论基础和发展脉络，能有效指引您未来的研究方向。"
    },
    {
        "id": "http://arxiv.org/abs/2602.19115v1",
        "title": "How Do LLMs Encode Scientific Quality? An Empirical Study Using Monosemantic Features from Sparse Autoencoders",
        "summary": "In recent years, there has been a growing use of generative AI, and large language models (LLMs) in particular, to support both the assessment and generation of scientific work. Although some studies have shown that LLMs can, to a certain extent, evaluate research according to perceived quality, our understanding of the internal mechanisms that enable this capability remains limited. This paper presents the first study that investigates how LLMs encode the concept of scientific quality through relevant monosemantic features extracted using sparse autoencoders. We derive such features under different experimental settings and assess their ability to serve as predictors across three tasks related to research quality: predicting citation count, journal SJR, and journal h-index. The results indicate that LLMs encode features associated with multiple dimensions of scientific quality. In particular, we identify four recurring types of features that capture key aspects of how research quality is represented: 1) features reflecting research methodologies; 2) features related to publication type, with literature reviews typically exhibiting higher impact; 3) features associated with high-impact research fields and technologies; and 4) features corresponding to specific scientific jargons. These findings represent an important step toward understanding how LLMs encapsulate concepts related to research quality.",
        "authors": "Michael McCoubrey, Angelo Salatino, Francesco Osborne, Enrico Motta",
        "url": "http://arxiv.org/abs/2602.19115v1",
        "pdf_url": "https://arxiv.org/pdf/2602.19115v1",
        "type": "前沿深度",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "通过稀疏自编码器提取单义特征，实证研究LLM内部机制如何编码科学质量，提升LLM可解释性。",
        "reason_zh": "该研究深入探究了LLM如何编码科学质量这一抽象概念的内部机制，利用稀疏自编码器进行可解释性分析。这直接满足了您对“LLM理论机制”的关注，提供了理解LLM深层工作原理的实证方法和洞察。"
    },
    {
        "id": "http://arxiv.org/abs/2602.21501v1",
        "title": "A Researcher's Guide to Empirical Risk Minimization",
        "summary": "This guide develops high-probability regret bounds for empirical risk minimization (ERM). The presentation is modular: we state broadly applicable guarantees under high-level conditions and give tools for verifying them for specific losses and function classes. We emphasize that many ERM rate derivations can be organized around a three-step recipe -- a basic inequality, a uniform local concentration bound, and a fixed-point argument -- which yields regret bounds in terms of a critical radius, defined via localized Rademacher complexity, under a mild Bernstein-type variance--risk condition. To make these bounds concrete, we upper bound the critical radius using local maximal inequalities and metric-entropy integrals, recovering familiar rates for VC-subgraph, Sobolev/Hölder, and bounded-variation classes.   We also review ERM with nuisance components -- including weighted ERM and Neyman-orthogonal losses -- as they arise in causal inference, missing data, and domain adaptation. Following the orthogonal learning framework, we highlight that these problems often admit regret-transfer bounds linking regret under an estimated loss to population regret under the target loss. These bounds typically decompose regret into (i) statistical error under the estimated (optimized) loss and (ii) approximation error due to nuisance estimation. Under sample splitting or cross-fitting, the first term can be controlled using standard fixed-loss ERM regret bounds, while the second term depends only on nuisance-estimation accuracy. We also treat the in-sample regime, where nuisances and the ERM are fit on the same data, deriving regret bounds and giving sufficient conditions for fast rates.",
        "authors": "Lars van der Laan",
        "url": "http://arxiv.org/abs/2602.21501v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21501v1",
        "type": "基础核心",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Clarity": 5,
            "Utility": 5
        },
        "core_value_zh": "作为研究者指南，严谨阐述经验风险最小化（ERM）理论，涵盖高概率悔恨界、Rademacher复杂度及与因果推断的结合。",
        "reason_zh": "这篇论文明确标注为“研究者指南”，系统而严谨地讲解了经验风险最小化（ERM）这一统计学习理论的基石。它涵盖了高概率悔恨界、局部Rademacher复杂度等核心概念，并触及因果推断等前沿应用，是补全您博士数学/CS基础的硬核资料，数学严谨性极高，逻辑清晰。"
    },
    {
        "id": "http://arxiv.org/abs/2602.19272v1",
        "title": "Time-iteration methods for controllability",
        "summary": "These notes are based on a short course delivered at the Summer School EUR MINT 2025 \"Control, Inverse Problems and Spectral Theory\", held in June 2025 in Toulouse, France. The course presents three important strategies in control theory, formulated as time-iteration methods, where each time step brings the state of the system closer to the desired target.   For linear PDEs, we survey the classical Lebeau-Robbiano method and its more recent developments. This method combines spectral inequalities and dissipation estimates to prove null controllability of a dissipative linear system.   For nonlinear PDEs, we reinterpret the Liu-Takahashi-Tucsnak method, which establishes local controllability of a nonlinear system by analyzing the control cost of its linearization. We provide an easily applicable black-box formulation of their method.   Finally, for nonlinear ODEs, we present the tangent vectors method, which establishes local exact controllability starting from approximately reachable directions.",
        "authors": "Frédéric Marbach",
        "url": "http://arxiv.org/abs/2602.19272v1",
        "pdf_url": "https://arxiv.org/pdf/2602.19272v1",
        "type": "基础核心",
        "scores": {
            "Novelty": 3,
            "Rigor": 5,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "基于短课程讲义，深入讲解控制理论中的时间迭代方法，涵盖线性/非线性PDEs/ODEs的严谨控制理论。",
        "reason_zh": "这组笔记基于一个短课程，系统介绍了控制理论中的时间迭代方法，特别是针对线性/非线性偏微分方程（PDEs）和常微分方程（ODEs）的精确可控性。其数学严谨性极高，内容硬核，非常适合作为您“高等概率论/随机过程”或一般高级数学基础的补充，以补全博士数学拼图。"
    },
    {
        "id": "http://arxiv.org/abs/2602.21572v1",
        "title": "Goodness-of-Fit Tests for Latent Class Models with Ordinal Categorical Data",
        "summary": "Ordinal categorical data are widely collected in psychology, education, and other social sciences, appearing commonly in questionnaires, assessments, and surveys. Latent class models provide a flexible framework for uncovering unobserved heterogeneity by grouping individuals into homogeneous classes based on their response patterns. A fundamental challenge in applying these models is determining the number of latent classes, which is unknown and must be inferred from data. In this paper, we propose one test statistic for this problem. The test statistic centers the largest singular value of a normalized residual matrix by a simple sample-size adjustment. Under the null hypothesis that the candidate number of latent classes is correct, its upper bound converges to zero in probability. Under an under-fitted alternative, the statistic itself exceeds a fixed positive constant with probability approaching one. This sharp dichotomous behavior of the test statistic yields two sequential testing algorithms that consistently estimate the true number of latent classes. Extensive experimental studies confirm the theoretical findings and demonstrate their accuracy and reliability in determining the number of latent classes.",
        "authors": "Huan Qing",
        "url": "http://arxiv.org/abs/2602.21572v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21572v1",
        "type": "基础核心",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "提出基于归一化残差矩阵最大奇异值的高维统计检验方法，解决潜在类别模型中的核心问题。",
        "reason_zh": "该论文针对潜在类别模型中确定类别数的关键问题，提出了一种基于归一化残差矩阵最大奇异值的严谨检验统计量。这与您对“高维统计”和“数学严谨性”的要求高度契合，是统计学研究生深入理解高维数据建模和推断的优质资料。"
    },
    {
        "id": "http://arxiv.org/abs/2602.22585v1",
        "title": "Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach",
        "summary": "Human evaluations play a central role in training and assessing AI models, yet these data are rarely treated as measurements subject to systematic error. This paper integrates psychometric rater models into the AI pipeline to improve the reliability and validity of conclusions drawn from human judgments. The paper reviews common rater effects, severity and centrality, that distort observed ratings, and demonstrates how item response theory rater models, particularly the multi-faceted Rasch model, can separate true output quality from rater behavior. Using the OpenAI summarization dataset as an empirical example, we show how adjusting for rater severity produces corrected estimates of summary quality and provides diagnostic insight into rater performance. Incorporating psychometric modeling into human-in-the-loop evaluation offers more principled and transparent use of human data, enabling developers to make decisions based on adjusted scores rather than raw, error-prone ratings. This perspective highlights a path toward more robust, interpretable, and construct-aligned practices for AI development and evaluation.",
        "authors": "Jodi M. Casabianca, Maggie Beiting-Parrish",
        "url": "http://arxiv.org/abs/2602.22585v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22585v1",
        "type": "基础核心",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Clarity": 4,
            "Utility": 4
        },
        "core_value_zh": "将心理测量学中的项目反应理论（IRT）应用于AI评估，提供严谨的测量误差校正方法。",
        "reason_zh": "虽然应用场景是AI评估，但其核心是应用多面Rasch模型等项目反应理论（IRT）来校正人类评分中的系统误差。IRT是统计学中一个高度数学严谨的测量理论框架，对于统计学博士生理解和应用高级统计模型、尤其是处理测量误差问题，具有重要的基础性价值。"
    },
    {
        "id": "http://arxiv.org/abs/2602.21268v1",
        "title": "A Dynamic Survey of Soft Set Theory and Its Extensions",
        "summary": "Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets-and has been connected to diverse areas such as topology and matroid theory. In this book, we present a survey-style overview of soft sets and their major extensions, highlighting core definitions, representative constructions, and key directions of current development.",
        "authors": "Takaaki Fujita, Florentin Smarandache",
        "url": "http://arxiv.org/abs/2602.21268v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21268v1",
        "type": "基础核心",
        "scores": {
            "Novelty": 3,
            "Rigor": 4,
            "Clarity": 4,
            "Utility": 3
        },
        "core_value_zh": "全面综述软集理论及其扩展，提供处理不确定性的数学基础框架。",
        "reason_zh": "鉴于列表中缺乏更多直接对应核心数学课程的教程，这篇对软集理论及其扩展的综述提供了一个处理不确定性的数学框架。尽管软集理论可能不是统计学博士生的主流核心课程，但其作为一种数学基础理论，具有一定的严谨性，可以作为您“博士数学拼图”中关于不确定性建模的补充资料。"
    }
]