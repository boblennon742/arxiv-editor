[
    {
        "id": "http://arxiv.org/abs/2512.10183v1",
        "title": "Topology Identification and Inference over Graphs",
        "summary": "Topology identification and inference of processes evolving over graphs arise in timely applications involving brain, transportation, financial, power, as well as social and information networks. This chapter provides an overview of graph topology identification and statistical inference methods for multidimensional relational data. Approaches for undirected links connecting graph nodes are outlined, going all the way from correlation metrics to covariance selection, and revealing ties with smooth signal priors. To account for directional (possibly causal) relations among nodal variables and address the limitations of linear time-invariant models in handling dynamic as well as nonlinear dependencies, a principled framework is surveyed to capture these complexities through judiciously selected kernels from a prescribed dictionary. Generalizations are also described via structural equations and vector autoregressions that can exploit attributes such as low rank, sparsity, acyclicity, and smoothness to model dynamic processes over possibly time-evolving topologies. It is argued that this approach supports both batch and online learning algorithms with convergence rate guarantees, is amenable to tensor (that is, multi-way array) formulations as well as decompositions that are well-suited for multidimensional network data, and can seamlessly leverage high-order statistical information.",
        "authors": "Gonzalo Mateos, Yanning Shen, Georgios B. Giannakis, Ananthram Swami",
        "url": "http://arxiv.org/abs/2512.10183v1",
        "pdf_url": "https://arxiv.org/pdf/2512.10183v1",
        "type": "前沿深度",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "全面综述图上拓扑识别与统计推断方法，为高维关系数据分析提供理论框架。",
        "reason_zh": "这篇综述深入探讨了图上拓扑识别和统计推断方法，涵盖了从相关性度量到协方差选择，以及利用核函数处理动态非线性依赖等前沿内容。对于关注“高维统计”和“因果推断”（通过方向性关系）的博士生而言，它提供了广阔的视野和严谨的理论基础，能有效指引未来在复杂网络数据分析方向的研究。作为一篇概述性文章，其实用性极高，有助于构建高维图数据分析的知识体系。"
    },
    {
        "id": "http://arxiv.org/abs/2512.08444v1",
        "title": "Learned iterative networks: An operator learning perspective",
        "summary": "Learned image reconstruction has become a pillar in computational imaging and inverse problems. Among the most successful approaches are learned iterative networks, which are formulated by unrolling classical iterative optimisation algorithms for solving variational problems. While the underlying algorithm is usually formulated in the functional analytic setting, learned approaches are often viewed as purely discrete. In this chapter we present a unified operator view for learned iterative networks. Specifically, we formulate a learned reconstruction operator, defining how to compute, and separately the learning problem, which defines what to compute. In this setting we present common approaches and show that many approaches are closely related in their core. We review linear as well as nonlinear inverse problems in this framework and present a short numerical study to conclude.",
        "authors": "Andreas Hauptmann, Ozan Öktem",
        "url": "http://arxiv.org/abs/2512.08444v1",
        "pdf_url": "https://arxiv.org/pdf/2512.08444v1",
        "type": "前沿深度",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Clarity": 4,
            "Utility": 4
        },
        "core_value_zh": "从算子学习的统一视角审视学习迭代网络，揭示计算成像与逆问题中的核心原理。",
        "reason_zh": "这篇章节（chapter）以算子学习的视角，对计算成像和逆问题中成功的学习迭代网络进行了统一的理论阐述。它将离散的学习方法提升到泛函分析的层面，揭示了不同方法间的深层联系。对于寻求“高维统计”和“LLM理论机制”背后更深层数学原理的博士生，这种对学习算法的泛函分析和算子学习的视角，能提供理解复杂模型（如LLMs中的迭代优化）的理论工具和未来研究方向，具有较高的理论深度。"
    },
    {
        "id": "http://arxiv.org/abs/2512.08290v1",
        "title": "Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem",
        "summary": "The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the \"USB-C for Agentic AI.\" While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial security threats (e.g., indirect prompt injection, tool poisoning) and epistemic safety hazards (e.g., alignment failures in distributed tool delegation). We analyze the structural vulnerabilities of MCP primitives, specifically Resources, Prompts, and Tools, and demonstrate how \"context\" can be weaponized to trigger unauthorized operations in multi-agent environments. Furthermore, we survey state-of-the-art defenses, ranging from cryptographic provenance (ETDI) to runtime intent verification, and conclude with a roadmap for securing the transition from conversational chatbots to autonomous agentic operating systems.",
        "authors": "Shiva Gaire, Srijan Gyawali, Saroj Mishra, Suman Niroula, Dilip Thakur, Umesh Yadav",
        "url": "http://arxiv.org/abs/2512.08290v1",
        "pdf_url": "https://arxiv.org/pdf/2512.08290v1",
        "type": "前沿深度",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "深入剖析LLM智能体交互中的安全与信任挑战，构建统一的风险分析模型并指明研究方向。",
        "reason_zh": "这篇关于LLM智能体生态系统安全与安全性的SoK（知识系统化）文章，提出了“信任-授权不匹配”的风险分析模型。它超越了具体的攻击手段，从理论层面系统性地分类和分析了LLM代理交互中的结构性漏洞和威胁。对于研究“LLM理论机制”的博士生，这篇综述不仅提供了对当前安全挑战的全面理解，更重要的是，它提供了一个统一的理论框架和未来研究路线图，具有极高的前瞻性和指导意义。"
    },
    {
        "id": "http://arxiv.org/abs/2512.08179v1",
        "title": "Distributional Random Forests for Complex Survey Designs on Reproducing Kernel Hilbert Spaces",
        "summary": "We study estimation of the conditional law $P(Y|X=\\mathbf{x})$ and continuous functionals $Ψ(P(Y|X=\\mathbf{x}))$ when $Y$ takes values in a locally compact Polish space, $X \\in \\mathbb{R}^p$, and the observations arise from a complex survey design. We propose a survey-calibrated distributional random forest (SDRF) that incorporates complex-design features via a pseudo-population bootstrap, PSU-level honesty, and a Maximum Mean Discrepancy (MMD) split criterion computed from kernel mean embeddings of Hájek-type (design-weighted) node distributions. We provide a framework for analyzing forest-style estimators under survey designs; establish design consistency for the finite-population target and model consistency for the super-population target under explicit conditions on the design, kernel, resampling multipliers, and tree partitions. As far as we are aware, these are the first results on model-free estimation of conditional distributions under survey designs. Simulations under a stratified two-stage cluster design provide finite sample performance and demonstrate the statistical error price of ignoring the survey design. The broad applicability of SDRF is demonstrated using NHANES: We estimate the tolerance regions of the conditional joint distribution of two diabetes biomarkers, illustrating how distributional heterogeneity can support subgroup-specific risk profiling for diabetes mellitus in the U.S. population.",
        "authors": "Yating Zou, Marcos Matabuena, Michael R. Kosorok",
        "url": "http://arxiv.org/abs/2512.08179v1",
        "pdf_url": "https://arxiv.org/pdf/2512.08179v1",
        "type": "前沿深度",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "提出针对复杂调查设计的分布随机森林，实现高维数据下条件分布的无模型估计。",
        "reason_zh": "这篇论文提出了一种创新的、针对复杂调查设计的分布随机森林（SDRF）方法，用于估计条件分布和连续泛函。它结合了伪群体bootstrap、PSU级诚实性和MMD分裂准则，并在再生核希尔伯特空间（RKHS）上操作。这对于“高维统计”和“因果推断”（复杂调查设计常涉及因果推断挑战）领域的博士生而言，提供了前沿的统计建模工具和理论分析框架，特别是在处理非i.i.d.数据时的严谨性，具有很高的研究价值。"
    },
    {
        "id": "http://arxiv.org/abs/2512.10449v1",
        "title": "When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection",
        "summary": "The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the \"Lazy Reviewer\" hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these \"LLM-as-a-Judge\" systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping \"Reject\" decisions to \"Accept,\" for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like \"Maximum Mark Magyk\" successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic.",
        "authors": "Devanshu Sahoo, Manish Prasad, Vasudev Majhi, Jahnvi Singh, Vinay Chamola, Yash Sinha, Murari Mandal, Dhruv Kumar",
        "url": "http://arxiv.org/abs/2512.10449v1",
        "pdf_url": "https://arxiv.org/pdf/2512.10449v1",
        "type": "前沿深度",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Clarity": 5,
            "Utility": 5
        },
        "core_value_zh": "量化LLM作为审稿人时对间接提示注入的脆弱性，揭示其在科学评估中的潜在风险。",
        "reason_zh": "这篇研究深入探讨了LLM在科学评审中作为“裁判”时，如何通过对抗性PDF操纵（间接提示注入）被利用，从而改变评审结果。它提出了WAVS（加权对抗性脆弱性分数）这一新颖评估指标，并进行了广泛的模型测试。对于研究“LLM理论机制”的博士生，这篇论文提供了一个具体且具有现实意义的案例，揭示了LLM在关键决策场景中的深层安全漏洞和局限性，有助于理解其鲁棒性边界和未来改进方向，具有很强的警示和指导作用。"
    },
    {
        "id": "http://arxiv.org/abs/2512.10204v1",
        "title": "Variational-hemivariational inequalities: A brief survey on mathematical theory and numerical analysis",
        "summary": "Variational-hemivariational inequalities are an area full of interesting and challenging mathematical problems. The area can be viewed as a natural extension of that of variational inequalities. Variational-hemivariational inequalities are valuable for application problems from physical sciences and engineering that involve non-smooth and even set-valued relations, monotone or non-monotone, among physical quantities. In the recent years, there has been substantial growth of research interest in modeling, well-posedness analysis, development of numerical methods and numerical algorithms of variational-hemivariational inequalities. This survey paper is devoted to a brief account of well-posedness and numerical analysis results for variational-hemivariational inequalities. The theoretical results are presented for a family of abstract stationary variational-hemivariational inequalities and the main idea is explained for an accessible proof of existence and uniqueness. To better appreciate the distinguished feature of variational-hemivariational inequalities, for comparison, three mechanical problems are introduced leading to a variational equation, a variational inequality, and a variational-hemivariational inequality, respectively. The paper also comments on mixed variational-hemivariational inequalities, with examples from applications in fluid mechanics, and on results concerning the numerical solution of other types (nonstationary, history dependent) of variational-hemivariational inequalities.",
        "authors": "Weimin Han",
        "url": "http://arxiv.org/abs/2512.10204v1",
        "pdf_url": "https://arxiv.org/pdf/2512.10204v1",
        "type": "基础核心",
        "scores": {
            "Novelty": 3,
            "Rigor": 5,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "概述变分-半变分不等式的数学理论与数值分析，为涉及非光滑、集值关系的物理工程问题提供严谨基础。",
        "reason_zh": "这篇简要综述聚焦于变分-半变分不等式，这是一个在数学理论和数值分析上都具有挑战性的领域，尤其适用于处理涉及非光滑和集值关系的物理与工程问题。对于需要补全“凸优化”和高等分析数学拼图的博士生，它提供了严谨的理论基础，包括存在性、唯一性证明的核心思想，以及与变分方程和变分不等式的对比，是深入理解相关数学工具的硬核资料，能有效提升数学严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2512.08364v1",
        "title": "Generalized Discrepancy of Random Points",
        "summary": "We study the $L_p$-discrepancy of random point sets in high dimensions, with emphasis on small values of $p$. Although the classical $L_p$-discrepancy suffers from the curse of dimensionality for all $p \\in (1,\\infty)$, the gap between known upper and lower bounds remains substantial, in particular for small $p \\ge 1$. To clarify this picture, we review the existing results for i.i.d.\\ uniformly distributed points and derive new upper bounds for \\emph{generalized} $L_p$-discrepancies, obtained by allowing non-uniform sampling densities and corresponding non-negative quadrature weights.   Using the probabilistic method, we show that random points drawn from optimally chosen product densities lead to significantly improved upper bounds. For $p=2$ these bounds are explicit and optimal; for general $p \\in [1,\\infty)$ we obtain sharp asymptotic estimates. The improvement can be interpreted as a form of importance sampling for the underlying Sobolev space $F_{d,q}$.   Our results also reveal that, even with optimal densities, the curse of dimensionality persists for random points when $p\\ge 1$, and it becomes most pronounced for small $p$. This suggests that the curse should also hold for the classical $L_1$-discrepancy for deterministic point sets.",
        "authors": "Erich Novak, Friedrich Pillichshammer",
        "url": "http://arxiv.org/abs/2512.08364v1",
        "pdf_url": "https://arxiv.org/pdf/2512.08364v1",
        "type": "基础核心",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "深入研究高维随机点集的广义$L_p$差异性，揭示维度灾难在随机点集中的表现。",
        "reason_zh": "这篇论文探讨了高维随机点集的广义$L_p$差异性，特别关注小$p$值的情况，并讨论了维度灾难。它回顾了i.i.d.均匀分布点的现有结果，并推导了非均匀采样密度下的新上界。对于“高等概率论/随机过程”和“高维统计”的博士生，这篇论文提供了数学严谨的理论分析，有助于理解高维空间中随机点集的性质，是补全数学基础的重要资料，尤其适合对理论细节有高要求的学习者。"
    },
    {
        "id": "http://arxiv.org/abs/2512.08052v2",
        "title": "An Introduction to Deep Reinforcement and Imitation Learning",
        "summary": "Embodied agents, such as robots and virtual characters, must continuously select actions to execute tasks effectively, solving complex sequential decision-making problems. Given the difficulty of designing such controllers manually, learning-based approaches have emerged as promising alternatives, most notably Deep Reinforcement Learning (DRL) and Deep Imitation Learning (DIL). DRL leverages reward signals to optimize behavior, while DIL uses expert demonstrations to guide learning. This document introduces DRL and DIL in the context of embodied agents, adopting a concise, depth-first approach to the literature. It is self-contained, presenting all necessary mathematical and machine learning concepts as they are needed. It is not intended as a survey of the field; rather, it focuses on a small set of foundational algorithms and techniques, prioritizing in-depth understanding over broad coverage. The material ranges from Markov Decision Processes to REINFORCE and Proximal Policy Optimization (PPO) for DRL, and from Behavioral Cloning to Dataset Aggregation (DAgger) and Generative Adversarial Imitation Learning (GAIL) for DIL.",
        "authors": "Pedro Santana",
        "url": "http://arxiv.org/abs/2512.08052v2",
        "pdf_url": "https://arxiv.org/pdf/2512.08052v2",
        "type": "基础核心",
        "scores": {
            "Novelty": 3,
            "Rigor": 5,
            "Clarity": 5,
            "Utility": 5
        },
        "core_value_zh": "自包含地介绍深度强化学习与模仿学习的数学和机器学习基础概念。",
        "reason_zh": "这篇教程以简洁而深入的方式介绍了深度强化学习（DRL）和深度模仿学习（DIL），并自包含地提供了所有必要的数学和机器学习概念。它聚焦于马尔可夫决策过程、REINFORCE、PPO等基础算法，以及行为克隆、DAgger、GAIL等DIL技术。对于需要系统性构建“Offline RL”等前沿研究的“核心基础”的博士生，这篇教程提供了极高的数学严谨性和清晰度，是理解RL底层原理的绝佳资料，非常适合作为入门但硬核的教材。"
    },
    {
        "id": "http://arxiv.org/abs/2512.07201v1",
        "title": "Understanding Diffusion Models via Code Execution",
        "summary": "Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.",
        "authors": "Cheng Yu",
        "url": "http://arxiv.org/abs/2512.07201v1",
        "pdf_url": "https://arxiv.org/pdf/2512.07201v1",
        "type": "基础核心",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Clarity": 5,
            "Utility": 5
        },
        "core_value_zh": "通过精简代码实现深入理解扩散模型的理论机制与实际操作。",
        "reason_zh": "这篇技术报告通过一个约300行的精简代码实现，从代码执行的角度解释了扩散模型的核心组件（前向扩散、逆向采样、噪声预测网络、训练循环），并桥接了数学公式与实际代码之间的鸿沟。对于希望深入理解“LLM理论机制”中生成模型（如扩散模型）的博士生，这篇教程提供了极高的清晰度和实用性，能够帮助他们快速建立理论与实践相结合的扎实基础，是理解现代生成AI的必备资料。"
    },
    {
        "id": "http://arxiv.org/abs/2512.06797v1",
        "title": "Optimal and Diffusion Transports in Machine Learning",
        "summary": "Several problems in machine learning are naturally expressed as the design and analysis of time-evolving probability distributions. This includes sampling via diffusion methods, optimizing the weights of neural networks, and analyzing the evolution of token distributions across layers of large language models. While the targeted applications differ (samples, weights, tokens), their mathematical descriptions share a common structure. A key idea is to switch from the Eulerian representation of densities to their Lagrangian counterpart through vector fields that advect particles. This dual view introduces challenges, notably the non-uniqueness of Lagrangian vector fields, but also opportunities to craft density evolutions and flows with favorable properties in terms of regularity, stability, and computational tractability. This survey presents an overview of these methods, with emphasis on two complementary approaches: diffusion methods, which rely on stochastic interpolation processes and underpin modern generative AI, and optimal transport, which defines interpolation by minimizing displacement cost. We illustrate how both approaches appear in applications ranging from sampling, neural network optimization, to modeling the dynamics of transformers for large language models.",
        "authors": "Gabriel Peyré",
        "url": "http://arxiv.org/abs/2512.06797v1",
        "pdf_url": "https://arxiv.org/pdf/2512.06797v1",
        "type": "基础核心",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "综述机器学习中最优传输与扩散方法，从概率分布演化视角统一理解采样、优化与LLM机制。",
        "reason_zh": "这篇综述从时间演化概率分布的角度，统一了最优传输和扩散方法在机器学习中的应用，包括采样、神经网络优化和LLM中的token分布动力学。它强调了从欧拉表示到拉格朗日表示的转换，并探讨了如何设计具有良好性质的密度演化和流。对于需要补全“高等概率论/随机过程”和“信息论”基础，并希望从数学深层理解现代生成模型（如LLM）机制的博士生，这篇综述提供了高度数学严谨性和前瞻性的视角，是构建高级数学基础的理想选择。"
    }
]