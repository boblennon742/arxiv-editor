[
    {
        "id": "http://arxiv.org/abs/2602.10045v1",
        "title": "Conformal Prediction Sets for Instance Segmentation",
        "summary": "Current instance segmentation models achieve high performance on average predictions, but lack principled uncertainty quantification: their outputs are not calibrated, and there is no guarantee that a predicted mask is close to the ground truth. To address this limitation, we introduce a conformal prediction algorithm to generate adaptive confidence sets for instance segmentation. Given an image and a pixel coordinate query, our algorithm generates a confidence set of instance predictions for that pixel, with a provable guarantee for the probability that at least one of the predictions has high Intersection-Over-Union (IoU) with the true object instance mask. We apply our algorithm to instance segmentation examples in agricultural field delineation, cell segmentation, and vehicle detection. Empirically, we find that our prediction sets vary in size based on query difficulty and attain the target coverage, outperforming existing baselines such as Learn Then Test, Conformal Risk Control, and morphological dilation-based methods. We provide versions of the algorithm with asymptotic and finite sample guarantees.",
        "authors": "Kerri Lu, Dan M. Kluger, Stephen Bates, Sherrie Wang",
        "url": "http://arxiv.org/abs/2602.10045v1",
        "pdf_url": "https://arxiv.org/pdf/2602.10045v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文将共形预测（一种提供有限样本覆盖保证的统计方法）应用于实例分割这一现代AI任务，明确提出了“可证明的保证”以及“渐近和有限样本保证”。这完美契合了您对“强大的理论基础”和“统计保证”的要求，是理论与实践结合的优秀范例。"
    },
    {
        "id": "http://arxiv.org/abs/2602.10006v1",
        "title": "Answer First, Reason Later: Aligning Search Relevance via Mode-Balanced Reinforcement Learning",
        "summary": "Building a search relevance model that achieves both low latency and high performance is a long-standing challenge in the search industry. To satisfy the millisecond-level response requirements of online systems while retaining the interpretable reasoning traces of Large Language Models (LLMs), we propose a novel \\textbf{Answer-First, Reason Later (AFRL)} paradigm. This paradigm requires the model to output the definitive relevance score in the very first token, followed by a structured logical explanation. Inspired by the success of reasoning models, we adopt a \"Supervised Fine-Tuning (SFT) + Reinforcement Learning (RL)\" pipeline to achieve AFRL. However, directly applying existing RL training often leads to \\textbf{mode collapse} in the search relevance task, where the model forgets complex long-tail rules in pursuit of high rewards. From an information theory perspective: RL inherently minimizes the \\textbf{Reverse KL divergence}, which tends to seek probability peaks (mode-seeking) and is prone to \"reward hacking.\" On the other hand, SFT minimizes the \\textbf{Forward KL divergence}, forcing the model to cover the data distribution (mode-covering) and effectively anchoring expert rules. Based on this insight, we propose a \\textbf{Mode-Balanced Optimization} strategy, incorporating an SFT auxiliary loss into Stepwise-GRPO training to balance these two properties. Furthermore, we construct an automated instruction evolution system and a multi-stage curriculum to ensure expert-level data quality. Extensive experiments demonstrate that our 32B teacher model achieves state-of-the-art performance. Moreover, the AFRL architecture enables efficient knowledge distillation, successfully transferring expert-level logic to a 0.6B model, thereby reconciling reasoning depth with deployment latency.",
        "authors": "Shijie Zhang, Xiang Guo, Rujun Guo, Shaoyu Liu, Xiaozhao Wang, Guanjun Jiang, Kevin Zhang",
        "url": "http://arxiv.org/abs/2602.10006v1",
        "pdf_url": "https://arxiv.org/pdf/2602.10006v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "针对大型语言模型（LLM）的搜索相关性任务，该论文深入分析了强化学习中常见的“模式崩溃”问题，并从信息论角度（逆KL散度与前向KL散度）提出了“模式平衡优化”策略。其理论动机清晰，并提供了数学推导来指导优化过程，对理解和改进LLM训练具有重要价值。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09963v1",
        "title": "Drug Release Modeling using Physics-Informed Neural Networks",
        "summary": "Accurate modeling of drug release is essential for designing and developing controlled-release systems. Classical models (Fick, Higuchi, Peppas) rely on simplifying assumptions that limit their accuracy in complex geometries and release mechanisms. Here, we propose a novel approach using Physics-Informed Neural Networks (PINNs) and Bayesian PINNs (BPINNs) for predicting release from planar, 1D-wrinkled, and 2D-crumpled films. This approach uniquely integrates Fick's diffusion law with limited experimental data to enable accurate long-term predictions from short-term measurements, and is systematically benchmarked against classical drug release models. We embedded Fick's second law into PINN as loss with 10,000 Latin-hypercube collocation points and utilized previously published experimental datasets to assess drug release performance through mean absolute error (MAE) and root mean square error (RMSE), considering noisy conditions and limited-data scenarios. Our approach reduced mean error by up to 40% relative to classical baselines across all film types. The PINN formulation achieved RMSE <0.05 utilizing only the first 6% of the release time data (reducing 94% of release time required for the experiments) for the planar film. For wrinkled and crumpled films, the PINN reached RMSE <0.05 in 33% of the release time data. BPINNs provide tighter and more reliable uncertainty quantification under noise. By combining physical laws with experimental data, the proposed framework yields highly accurate long-term release predictions from short-term measurements, offering a practical route for accelerated characterization and more efficient early-stage drug release system formulation.",
        "authors": "Daanish Aleem Qureshi, Khemraj Shukla, Vikas Srivastava",
        "url": "http://arxiv.org/abs/2602.09963v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09963v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该研究利用物理信息神经网络（PINNs）将Fick扩散定律等严谨的物理方程嵌入到AI模型的损失函数中，实现药物释放建模，并结合贝叶斯PINNs提供不确定性量化。这是将“严谨的数学逻辑应用于现代AI系统”的典范，展现了物理学原理与深度学习的深度融合。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09911v1",
        "title": "Doubly Robust Machine Learning for Population Size Estimation with Missing Covariates: Application to Gaza Conflict Mortality",
        "summary": "Population size estimation from capture-recapture data is central for studying hard-to-reach populations, incorporating auxiliary covariates to account for heterogeneous capture probabilities and recapture dependencies. However, missing attributes pose a critical methodological challenge due to reluctance to share sensitive information, data collection limitations, and imperfect record linkage. Existing approaches either ignore missingness or rely on a priori imputation, potentially introducing substantial bias. In this work, we develop a novel nonparametric estimation framework using a Missing at Random assumption to identify capture probabilities under missing covariates. Using semiparametric efficiency theory, we construct one-step estimators that combine efficiency, robustness, and finite-sample validity: they approximately achieve the nonparametric efficiency bound, accommodate flexible machine learning methods through a doubly robust structure, and provide approximately valid inference for any sample size. Simulations demonstrate substantial improvements over naive imputation approaches, with our doubly robust ML estimators maintaining valid inference even at high missingness rates where competing methods fail. We apply our methodology to re-estimate mortality in the Gaza Strip from October 7, 2023, to June 30, 2024, using three-list capture-recapture data with missing demographic information. Our approach yields more conservative yet precise estimates compared to previous methods, indicating the true death toll exceeds official statistics by approximately 26%. Our framework provides practitioners with principled tools for handling incomplete data in conflict settings and other applications with hard-to-reach populations.",
        "authors": "Mateo Dulce Rubio, Edward H. Kennedy, Nicholas P. Jewell",
        "url": "http://arxiv.org/abs/2602.09911v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09911v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文提出了处理缺失协变量下人口规模估计的非参数框架，基于“半参数效率理论”构建“双重鲁棒”的“一步估计量”，并提供了“有限样本有效性”和“渐近效率界”的保证。这是统计理论和因果推断的杰出应用，具有高度的理论严谨性和实际影响力。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09848v1",
        "title": "Robust Processing and Learning: Principles, Methods, and Wireless Applications",
        "summary": "This tutorial-style overview article examines the fundamental principles and methods of robustness, using wireless sensing and communication (WSC) as the narrative and exemplifying framework. First, we formalize the conceptual and mathematical foundations of robustness, highlighting the interpretations and relations across robust statistics, optimization, and machine learning. Key techniques, such as robust estimation and testing, distributionally robust optimization, and regularized and adversary training, are investigated. Together, the costs of robustness in system design, for example, the compromised nominal performances and the extra computational burdens, are discussed. Second, we review recent robust signal processing solutions for WSC that address model mismatch, data scarcity, adversarial perturbation, and distributional shift. Specific applications include robust ranging-based localization, modality sensing, channel estimation, receive combining, waveform design, and federated learning. Through this effort, we aim to introduce the classical developments and recent advances in robustness theory to the general signal processing community, exemplifying how robust statistical, optimization, and machine learning approaches can address the uncertainties inherent in WSC systems.",
        "authors": "Shixiong Wang, Wei Dai, Li-Chun Wang, Geoffrey Ye Li",
        "url": "http://arxiv.org/abs/2602.09848v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09848v1",
        "scores": {
            "Novelty": 3,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "作为一篇教程式综述，它系统地阐述了鲁棒性在统计学、优化和机器学习中的“概念和数学基础”，深入探讨了鲁棒估计、分布鲁棒优化等关键技术。尽管是综述，但其对理论基石的清晰梳理和形式化表达，对于理解AI系统的鲁棒性原理至关重要。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09794v1",
        "title": "GHS-TDA: A Synergistic Reasoning Framework Integrating Global Hypothesis Space with Topological Data Analysis",
        "summary": "Chain-of-Thought (CoT) has been shown to significantly improve the reasoning accuracy of large language models (LLMs) on complex tasks. However, due to the autoregressive, step-by-step generation paradigm, existing CoT methods suffer from two fundamental limitations. First, the reasoning process is highly sensitive to early decisions: once an initial error is introduced, it tends to propagate and amplify through subsequent steps, while the lack of a global coordination and revision mechanism makes such errors difficult to correct, ultimately leading to distorted reasoning chains. Second, current CoT approaches lack structured analysis techniques for filtering redundant reasoning and extracting key reasoning features, resulting in unstable reasoning processes and limited interpretability. To address these issues, we propose GHS-TDA. GHS-TDA first constructs a semantically enriched global hypothesis graph to aggregate, align, and coordinate multiple candidate reasoning paths, thereby providing alternative global correction routes when local reasoning fails. It then applies topological data analysis based on persistent homology to capture stable multi-scale structures, remove redundancy and inconsistencies, and extract a more reliable reasoning skeleton. By jointly leveraging reasoning diversity and topological stability, GHS-TDA achieves self-adaptive convergence, produces high-confidence and interpretable reasoning paths, and consistently outperforms strong baselines in terms of both accuracy and robustness across multiple reasoning benchmarks.",
        "authors": "Jiaquan Zhang, Chaoning Zhang, Shuxu Chen, Xudong Wang, Zhenzhen Huang, Pengcheng Zheng, Shuai Yuan, Sheng Zheng, Qigan Sun, Jie Zou, Lik-Hang Lee, Yang Yang",
        "url": "http://arxiv.org/abs/2602.09794v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09794v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文将具有强大数学基础的“拓扑数据分析（TDA）”和“持久同调”应用于LLM的思维链推理，旨在通过结构化分析过滤冗余并提取更可靠的推理骨架。这是理论工具与现代AI系统（LLM推理）结合的优秀范例，为提升LLM的可解释性和鲁棒性提供了新视角。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09783v1",
        "title": "Why Linear Interpretability Works: Invariant Subspaces as a Result of Architectural Constraints",
        "summary": "Linear probes and sparse autoencoders consistently recover meaningful structure from transformer representations -- yet why should such simple methods succeed in deep, nonlinear systems? We show this is not merely an empirical regularity but a consequence of architectural necessity: transformers communicate information through linear interfaces (attention OV circuits, unembedding matrices), and any semantic feature decoded through such an interface must occupy a context-invariant linear subspace. We formalize this as the \\emph{Invariant Subspace Necessity} theorem and derive the \\emph{Self-Reference Property}: tokens directly provide the geometric direction for their associated features, enabling zero-shot identification of semantic structure without labeled data or learned probes. Empirical validation in eight classification tasks and four model families confirms the alignment between class tokens and semantically related instances. Our framework provides \\textbf{a principled architectural explanation} for why linear interpretability methods work, unifying linear probes and sparse autoencoders.",
        "authors": "Andres Saurez, Yousung Lee, Dongsoo Har",
        "url": "http://arxiv.org/abs/2602.09783v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09783v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "这篇论文提出了一个“有原则的架构解释”，通过“不变子空间必要性定理”和“自引用属性”来解释Transformer中线性可解释性方法为何有效。它提供了对现代AI系统（Transformer）深层工作机制的“强大理论基础”和“清晰数学推导”，是可解释AI领域的开创性工作。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09764v1",
        "title": "Self-Supervised Learning as Discrete Communication",
        "summary": "Most self-supervised learning (SSL) methods learn continuous visual representations by aligning different views of the same input, offering limited control over how information is structured across representation dimensions. In this work, we frame visual self-supervised learning as a discrete communication process between a teacher and a student network, where semantic information is transmitted through a fixed-capacity binary channel. Rather than aligning continuous features, the student predicts multi-label binary messages produced by the teacher. Discrete agreement is enforced through an element-wise binary cross-entropy objective, while a coding-rate regularization term encourages effective utilization of the constrained channel, promoting structured representations. We further show that periodically reinitializing the projection head strengthens this effect by encouraging embeddings that remain predictive across multiple discrete encodings. Extensive experiments demonstrate consistent improvements over continuous agreement baselines on image classification, retrieval, and dense visual prediction tasks, as well as under domain shift through self-supervised adaptation. Beyond backbone representations, we analyze the learned binary codes and show that they form a compact and informative discrete language, capturing semantic factors reusable across classes.",
        "authors": "Kawtar Zaher, Ilyass Moummad, Olivier Buisson, Alexis Joly",
        "url": "http://arxiv.org/abs/2602.09764v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09764v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该研究从“离散通信过程”和“信息论”的角度重新审视自监督学习，引入“编码率正则化”以促进结构化表示。这种信息论的视角为理解和设计自监督学习提供了新的理论框架，与您对“严谨的数学逻辑”的偏好高度一致。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09595v1",
        "title": "Sharp Bounds for Treatment Effect Generalization under Outcome Distribution Shift",
        "summary": "Generalizing treatment effects from a randomized trial to a target population requires the assumption that potential outcome distributions are invariant across populations after conditioning on observed covariates. This assumption fails when unmeasured effect modifiers are distributed differently between trial participants and the target population. We develop a sensitivity analysis framework that bounds how much conclusions can change when this transportability assumption is violated. Our approach constrains the likelihood ratio between target and trial outcome densities by a scalar parameter $Λ\\geq 1$, with $Λ= 1$ recovering standard transportability. For each $Λ$, we derive sharp bounds on the target average treatment effect -- the tightest interval guaranteed to contain the true effect under all data-generating processes compatible with the observed data and the sensitivity model. We show that the optimal likelihood ratios have a simple threshold structure, leading to a closed-form greedy algorithm that requires only sorting trial outcomes and redistributing probability mass. The resulting estimator runs in $O(n \\log n)$ time and is consistent under standard regularity conditions. Simulations demonstrate that our bounds achieve nominal coverage when the true outcome shift falls within the specified $Λ$, provide substantially tighter intervals than worst-case bounds, and remain informative across a range of realistic violations of transportability.",
        "authors": "Amir Asiaee, Samhita Pal, Cole Beck, Jared D. Huling",
        "url": "http://arxiv.org/abs/2602.09595v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09595v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "针对结果分布偏移下的治疗效果泛化问题，该论文开发了“敏感性分析框架”，推导了“尖锐界限”，并提供了具有“一致性估计量”和“名义覆盖率”的“封闭形式贪婪算法”。这是因果推断领域高度严谨的理论工作，具有清晰的数学推导和统计保证。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09569v1",
        "title": "Training deep physical neural networks with local physical information bottleneck",
        "summary": "Deep learning has revolutionized modern society but faces growing energy and latency constraints. Deep physical neural networks (PNNs) are interconnected computing systems that directly exploit analog dynamics for energy-efficient, ultrafast AI execution. Realizing this potential, however, requires universal training methods tailored to physical intricacies. Here, we present the Physical Information Bottleneck (PIB), a general and efficient framework that integrates information theory and local learning, enabling deep PNNs to learn under arbitrary physical dynamics. By allocating matrix-based information bottlenecks to each unit, we demonstrate supervised, unsupervised, and reinforcement learning across electronic memristive chips and optical computing platforms. PIB also adapts to severe hardware faults and allows for parallel training via geographically distributed resources. Bypassing auxiliary digital models and contrastive measurements, PIB recasts PNN training as an intrinsic, scalable information-theoretic process compatible with diverse physical substrates.",
        "authors": "Hao Wang, Ziao Wang, Xiangpeng Liang, Han Zhao, Jianqi Hu, Junjie Jiang, Xing Fu, Jianshi Tang, Huaqiang Wu, Sylvain Gigan, Qiang Liu",
        "url": "http://arxiv.org/abs/2602.09569v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09569v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文提出了“物理信息瓶颈（PIB）”框架，将“信息论”与局部学习相结合，用于训练深度物理神经网络。它为新型AI硬件的训练提供了理论上严谨且通用的方法，是信息论在AI系统设计中应用的典范。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09537v1",
        "title": "A joint QoL-Survival framework with debiased estimation under truncation by death",
        "summary": "Evaluating quality-of-life (QoL) outcomes in populations with high mortality risk is complicated by truncation by death, since QoL is undefined for individuals who do not survive to the planned measurement time. We propose a framework that jointly models the distribution of QoL and survival without extrapolating QoL beyond death. Inspired by multistate formulations, we extend the joint characterization of binary health states and mortality to continuous QoL outcomes. Because treatment effects cannot be meaningfully summarized in a single one-dimensional estimand without strong assumptions, our approach simultaneously considers both survival and the joint distribution of QoL and survival with the latter conveniently displayed in a simplex. We develop assumption-lean, semiparametric estimators based on efficient influence functions, yielding flexible, root-n consistent estimators that accommodate machine-learning methods while making transparent the conditions these must satisfy. The proposed method is illustrated through simulation studies and two real-data applications.",
        "authors": "Torben Martinussen, Klaus K. Holst, Christian Bressen Pipper, Per Kragh Andersen",
        "url": "http://arxiv.org/abs/2602.09537v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09537v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文提出了一个联合QoL-生存框架，解决了死亡截断问题，并开发了基于“有效影响函数”的“半参数估计量”，具有“根n一致性”。这是生存分析和半参数理论的深度应用，提供了严谨的统计保证和清晰的数学推导。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09507v1",
        "title": "Towards Uniformity and Alignment for Multimodal Representation Learning",
        "summary": "Multimodal representation learning aims to construct a shared embedding space in which heterogeneous modalities are semantically aligned. Despite strong empirical results, InfoNCE-based objectives introduce inherent conflicts that yield distribution gaps across modalities. In this work, we identify two conflicts in the multimodal regime, both exacerbated as the number of modalities increases: (i) an alignment-uniformity conflict, whereby the repulsion of uniformity undermines pairwise alignment, and (ii) an intra-alignment conflict, where aligning multiple modalities induces competing alignment directions. To address these issues, we propose a principled decoupling of alignment and uniformity for multimodal representations, providing a conflict-free recipe for multimodal learning that simultaneously supports discriminative and generative use cases without task-specific modules. We then provide a theoretical guarantee that our method acts as an efficient proxy for a global Hölder divergence over multiple modality distributions, and thus reduces the distribution gap among modalities. Extensive experiments on retrieval and UnCLIP-style generation demonstrate consistent gains.",
        "authors": "Wenzhe Yin, Pan Zhou, Zehao Xiao, Jie Liu, Shujian Yu, Jan-Jakob Sonke, Efstratios Gavves",
        "url": "http://arxiv.org/abs/2602.09507v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09507v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该研究深入分析了多模态表示学习中InfoNCE目标的内在冲突，并提出了“对齐与均匀性的解耦”原则，提供了“全局Hölder散度”的理论保证。这为多模态AI的表示学习奠定了坚实的理论基础，解决了核心的数学问题。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09318v1",
        "title": "GAFR-Net: A Graph Attention and Fuzzy-Rule Network for Interpretable Breast Cancer Image Classification",
        "summary": "Accurate classification of breast cancer histopathology images is pivotal for early oncological diagnosis and therapeutic intervention.However, conventional deep learning architectures often encounter performance degradation under limited annotations and suffer from a \"blackbox\" nature, hindering their clinical integration. To mitigate these limitations, we propose GAFRNet, a robust and interpretable Graph Attention and FuzzyRule Network specifically engineered for histopathology image classification with scarce supervision. GAFRNet constructs a similarity-driven graph representation to model intersample relationships and employs a multihead graph attention mechanism to capture complex relational features across heterogeneous tissue structures.Concurrently, a differentiable fuzzy-rule module encodes intrinsic topological descriptorsincluding node degree, clustering coefficient, and label consistencyinto explicit, human-understandable diagnostic logic. This design establishes transparent \"IF-THEN\" mappings that mimic the heuristic deduction process of medical experts, providing clear reasoning behind each prediction without relying on post-hoc attribution methods. Extensive evaluations on three benchmark datasets (BreakHis, Mini-DDSM, and ICIAR2018) demonstrate that GAFR-Net consistently outperforms various state-of-the-art methods across multiple magnifications and classification tasks. These results validate the superior generalization and practical utility of GAFR-Net as a reliable decision-support tool for weakly supervised medical image analysis.",
        "authors": "Lin-Guo Gao, Suxing Liu",
        "url": "http://arxiv.org/abs/2602.09318v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09318v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文提出了GAFRNet，一个结合图注意力与模糊规则的网络，用于可解释的乳腺癌图像分类。它通过“可微分模糊规则模块”将“内在拓扑描述符”编码为“人类可理解的诊断逻辑”（“IF-THEN”映射），旨在实现“白盒”可解释性，具有清晰的逻辑和数学基础。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09314v1",
        "title": "Clarifying Shampoo: Adapting Spectral Descent to Stochasticity and the Parameter Trajectory",
        "summary": "Optimizers leveraging the matrix structure in neural networks, such as Shampoo and Muon, are more data-efficient than element-wise algorithms like Adam and Signum. While in specific settings, Shampoo and Muon reduce to spectral descent analogous to how Adam and Signum reduce to sign descent, their general relationship and relative data efficiency under controlled settings remain unclear. Through extensive experiments on language models, we demonstrate that Shampoo achieves higher token efficiency than Muon, mirroring Adam's advantage over Signum. We show that Shampoo's update applied to weight matrices can be decomposed into an adapted Muon update. Consistent with this, Shampoo's benefits can be exclusively attributed to its application to weight matrices, challenging interpretations agnostic to parameter shapes. This admits a new perspective that also avoids shortcomings of related interpretations based on variance adaptation and whitening: rather than enforcing semi-orthogonality as in spectral descent, Shampoo's updates are time-averaged semi-orthogonal in expectation.",
        "authors": "Runa Eschenhagen, Anna Cai, Tsung-Hsien Lee, Hao-Jun Michael Shi",
        "url": "http://arxiv.org/abs/2602.09314v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09314v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "这篇论文对深度学习优化器（如Shampoo）进行了理论分析，将其更新分解并归因于“期望中的时间平均半正交性”，揭示了优化算法的深层数学性质。这对于理解和改进现代AI系统的训练过程至关重要，展现了极高的数学严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09306v1",
        "title": "Empowering Contrastive Federated Sequential Recommendation with LLMs",
        "summary": "Federated sequential recommendation (FedSeqRec) aims to perform next-item prediction while keeping user data decentralised, yet model quality is frequently constrained by fragmented, noisy, and homogeneous interaction logs stored on individual devices. Many existing approaches attempt to compensate through manual data augmentation or additional server-side constraints, but these strategies either introduce limited semantic diversity or increase system overhead. To overcome these challenges, we propose \\textbf{LUMOS}, a parameter-isolated FedSeqRec architecture that integrates large language models (LLMs) as \\emph{local semantic generators}. Instead of sharing gradients or auxiliary parameters, LUMOS privately invokes an on-device LLM to construct three complementary sequence variants from each user history: (i) \\emph{future-oriented} trajectories that infer plausible behavioural continuations, (ii) \\emph{semantically equivalent rephrasings} that retain user intent while diversifying interaction patterns, and (iii) \\emph{preference-inconsistent counterfactuals} that serve as informative negatives. These synthesized sequences are jointly encoded within the federated backbone through a tri-view contrastive optimisation scheme, enabling richer representation learning without exposing sensitive information. Experimental results across three public benchmarks show that LUMOS achieves consistent gains over competitive centralised and federated baselines on HR@20 and NDCG@20. In addition, the use of semantically grounded positive signals and counterfactual negatives improves robustness under noisy and adversarial environments, even without dedicated server-side protection modules. Overall, this work demonstrates the potential of LLM-driven semantic generation as a new paradigm for advancing privacy-preserving federated recommendation.",
        "authors": "Thi Minh Chau Nguyen, Minh Hieu Nguyen, Duc Anh Nguyen, Xuan Huong Tran, Thanh Trung Huynh, Quoc Viet Hung Nguyen",
        "url": "http://arxiv.org/abs/2602.09306v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09306v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文引入了基于“拓扑数据分析（TDA）”和“持久景观（PLs）”的框架来量化数据集多样性，超越了传统熵度量。它从几何角度提供了“理论基础”且“可解释”的多样性衡量工具，是TDA在数据分析中严谨应用的体现。"
    }
]