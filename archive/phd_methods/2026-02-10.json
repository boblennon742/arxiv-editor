[
    {
        "id": "http://arxiv.org/abs/2602.09373v1",
        "title": "AfriNLLB: Efficient Translation Models for African Languages",
        "summary": "In this work, we present AfriNLLB, a series of lightweight models for efficient translation from and into African languages. AfriNLLB supports 15 language pairs (30 translation directions), including Swahili, Hausa, Yoruba, Amharic, Somali, Zulu, Lingala, Afrikaans, Wolof, and Egyptian Arabic, as well as other African Union official languages such as Arabic (MSA), French, Portuguese, and Spanish. Our training data covers bidirectional translation between English and 13 languages, and between French and two languages (Lingala and Wolof).   AfriNLLB models are based on NLLB-200 600M, which we compress using iterative layer pruning and quantization. We fine-tune the pruned models on parallel corpora we curated for African languages, employing knowledge distillation from a larger teacher model. Our work aims at enabling efficient deployment of translation models for African languages in resource-constrained settings.   Our evaluation results demonstrate that AfriNLLB models achieve performance comparable to the baseline while being significantly faster. We release two versions of the AfriNLLB models, a Transformers version that allows further fine-tuning and a CTranslate2 version for efficient inference. Moreover, we release all the training data that we used for fine-tuning the baseline and pruned models to facilitate further research.",
        "authors": "Yasmin Moslem, Aman Kassahun Wassie, Amanuel Gizachew Abebe",
        "url": "http://arxiv.org/abs/2602.09373v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09373v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "这篇论文从信息论角度理论和实证地论证了自进化AI社会中安全对齐的不可避免的衰退，提出了一个关于自进化AI系统基本限制的深刻见解。其信息论框架和对内在动态风险的分析，而非症状驱动的修补，与您对前沿算法和架构的理论创新性偏好高度契合，对AI安全领域具有里程碑式的意义。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09781v1",
        "title": "Explainability in Generative Medical Diffusion Models: A Faithfulness-Based Analysis on MRI Synthesis",
        "summary": "This study investigates the explainability of generative diffusion models in the context of medical imaging, focusing on Magnetic resonance imaging (MRI) synthesis. Although diffusion models have shown strong performance in generating realistic medical images, their internal decision making process remains largely opaque. We present a faithfulness-based explainability framework that analyzes how prototype-based explainability methods like ProtoPNet (PPNet), Enhanced ProtoPNet (EPPNet), and ProtoPool can link the relationship between generated and training features. Our study focuses on understanding the reasoning behind image formation through denoising trajectory of diffusion model and subsequently prototype explainability with faithfulness analysis. Experimental analysis shows that EPPNet achieves the highest faithfulness (with score 0.1534), offering more reliable insights, and explainability into the generative process. The results highlight that diffusion models can be made more transparent and trustworthy through faithfulness-based explanations, contributing to safer and more interpretable applications of generative AI in healthcare.",
        "authors": "Surjo Dey, Pallabi Saikia",
        "url": "http://arxiv.org/abs/2602.09781v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09781v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "该论文提出了GHS-TDA框架，通过结合全局假设空间和拓扑数据分析（TDA）来增强LLM的CoT推理。利用持久同调（persistent homology）捕捉稳定的多尺度结构，解决了现有CoT方法中错误传播和缺乏结构化分析的根本限制。这种将TDA引入LLM推理的理论创新性极高，且能显著提升推理的准确性和可解释性，完美符合您对理论创新和解决实际应用瓶颈的要求。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09708v1",
        "title": "Physics-informed diffusion models in spectral space",
        "summary": "We propose a methodology that combines generative latent diffusion models with physics-informed machine learning to generate solutions of parametric partial differential equations (PDEs) conditioned on partial observations, which includes, in particular, forward and inverse PDE problems. We learn the joint distribution of PDE parameters and solutions via a diffusion process in a latent space of scaled spectral representations, where Gaussian noise corresponds to functions with controlled regularity. This spectral formulation enables significant dimensionality reduction compared to grid-based diffusion models and ensures that the induced process in function space remains within a class of functions for which the PDE operators are well defined. Building on diffusion posterior sampling, we enforce physics-informed constraints and measurement conditions during inference, applying Adam-based updates at each diffusion step. We evaluate the proposed approach on Poisson, Helmholtz, and incompressible Navier--Stokes equations, demonstrating improved accuracy and computational efficiency compared with existing diffusion-based PDE solvers, which are state of the art for sparse observations. Code is available at https://github.com/deeplearningmethods/PISD.",
        "authors": "Davide Gallon, Philippe von Wurstemberger, Patrick Cheridito, Arnulf Jentzen",
        "url": "http://arxiv.org/abs/2602.09708v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09708v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "这篇论文提出了一种将生成式潜在扩散模型与物理信息机器学习相结合的方法，用于在谱空间中生成参数偏微分方程（PDEs）的解。其谱表示确保了函数空间的正则性，并在推理过程中强制执行物理信息约束。这种将物理信息与扩散模型在谱空间中融合的理论创新性极强，为科学计算和ML for Science领域解决了计算效率和准确性的瓶颈。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09541v1",
        "title": "Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination",
        "summary": "Rapid progress in large vision-language models (LVLMs) has achieved unprecedented performance in vision-language tasks. However, due to the strong prior of large language models (LLMs) and misaligned attention across modalities, LVLMs often generate outputs inconsistent with visual content - termed hallucination. To address this, we propose \\textbf{Scalpel}, a method that reduces hallucination by refining attention activation distributions toward more credible regions. Scalpel predicts trusted attention directions for each head in Transformer layers during inference and adjusts activations accordingly. It employs a Gaussian mixture model to capture multi-peak distributions of attention in trust and hallucination manifolds, and uses entropic optimal transport (equivalent to Schrödinger bridge problem) to map Gaussian components precisely. During mitigation, Scalpel dynamically adjusts intervention strength and direction based on component membership and mapping relationships between hallucination and trust activations. Extensive experiments across multiple datasets and benchmarks demonstrate that Scalpel effectively mitigates hallucinations, outperforming previous methods and achieving state-of-the-art performance. Moreover, Scalpel is model- and data-agnostic, requiring no additional computation, only a single decoding step.",
        "authors": "Ziqiang Shi, Rujie Liu, Shanshan Yu, Satoshi Munakata, Koichi Shirahata",
        "url": "http://arxiv.org/abs/2602.09541v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09541v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "Scalpel通过混合高斯桥（Mixture Gaussian Bridges）和熵最优传输（等同于Schrödinger桥问题）来精细对齐注意力激活流形，从而缓解多模态幻觉。这种将高级数学理论（最优传输、Schrödinger桥）应用于LLM/VLM幻觉缓解的理论深度和创新性极高，且能有效提升模型可靠性，是您会非常感兴趣的前沿算法研究。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09424v1",
        "title": "Reward-Guided Discrete Diffusion via Clean-Sample Markov Chain for Molecule and Biological Sequence Design",
        "summary": "Discrete diffusion models have recently emerged as a powerful class of generative models for chemistry and biology data. In these fields, the goal is to generate various samples with high rewards (e.g., drug-likeness in molecules), making reward-based guidance crucial. Most existing methods are based on guiding the diffusion model using intermediate rewards but tend to underperform since intermediate rewards are noisy due to the non-smooth nature of reward functions used in scientific domains. To address this, we propose Clean-Sample Markov Chain (CSMC) Sampler, a method that performs effective test-time reward-guided sampling for discrete diffusion models, enabling local search without relying on intermediate rewards. CSMC constructs a Markov chain of clean samples using the Metropolis-Hastings algorithm such that its stationary distribution is the target distribution. We design a proposal distribution by sequentially applying the forward and backward diffusion processes, making the acceptance probability tractable. Experiments on molecule and biological sequence generation with various reward functions demonstrate that our method consistently outperforms prior approaches that rely on intermediate rewards.",
        "authors": "Prin Phunyaphibarn, Minhyuk Sung",
        "url": "http://arxiv.org/abs/2602.09424v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09424v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "该论文提出了一个新颖的指纹识别框架，利用安全对齐引起的行为模式（拒绝向量）进行LLM溯源追踪。其理论框架能够将私有指纹转化为可公开验证、保护隐私的工件，具有极高的理论严谨性和创新性。它解决了LLM知识产权保护的关键挑战，对AI安全和治理具有深远影响。"
    },
    {
        "id": "http://arxiv.org/abs/2602.10095v1",
        "title": "Causality in Video Diffusers is Separable from Denoising",
        "summary": "Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering. Motivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame-wise rendering, via a lightweight diffusion decoder. Extensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.",
        "authors": "Xingjian Bai, Guande He, Zhengqi Li, Eli Shechtman, Xun Huang, Zongze Wu",
        "url": "http://arxiv.org/abs/2602.10095v1",
        "pdf_url": "https://arxiv.org/pdf/2602.10095v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 4,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "这篇论文提出了一个关键洞察：视频扩散模型中的因果推理可以与迭代去噪过程分离。基于此，提出了Separable Causal Diffusion (SCD) 架构，显著提高了吞吐量和帧延迟，同时保持或超越了生成质量。这种对扩散模型内部机制的理论性解耦和架构创新，直接解决了视频生成效率的瓶颈，非常符合您的研究方向。"
    },
    {
        "id": "http://arxiv.org/abs/2602.10031v1",
        "title": "Position: Message-passing and spectral GNNs are two sides of the same coin",
        "summary": "Graph neural networks (GNNs) are commonly divided into message-passing neural networks (MPNNs) and spectral graph neural networks, reflecting two largely separate research traditions in machine learning and signal processing. This paper argues that this divide is mostly artificial, hindering progress in the field. We propose a viewpoint in which both MPNNs and spectral GNNs are understood as different parametrizations of permutation-equivariant operators acting on graph signals. From this perspective, many popular architectures are equivalent in expressive power, while genuine gaps arise only in specific regimes. We further argue that MPNNs and spectral GNNs offer complementary strengths. That is, MPNNs provide a natural language for discrete structure and expressivity analysis using tools from logic and graph isomorphism research, while the spectral perspective provides principled tools for understanding smoothing, bottlenecks, stability, and community structure. Overall, we posit that progress in graph learning will be accelerated by clearly understanding the key similarities and differences between these two types of GNNs, and by working towards unifying these perspectives within a common theoretical and conceptual framework rather than treating them as competing paradigms.",
        "authors": "Antonis Vasileiou, Juan Cervino, Pascal Frossard, Charilaos I. Kanatsoulis, Christopher Morris, Michael T. Schaub, Pierre Vandergheynst, Zhiyang Wang, Guy Wolf, Ron Levie",
        "url": "http://arxiv.org/abs/2602.10031v1",
        "pdf_url": "https://arxiv.org/pdf/2602.10031v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "这篇Position Paper提出了一个统一的观点，认为消息传递神经网络（MPNNs）和谱图神经网络（spectral GNNs）是同一枚硬币的两面。它从理论上分析了它们的表达能力、平滑性、瓶颈和稳定性，旨在弥合两个研究传统之间的鸿沟。这种对GNN架构的深刻理论洞察和统一性思考，对推动图学习领域的发展具有重要意义，是您作为博士生会高度重视的理论创新。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09651v1",
        "title": "The Entropic Signature of Class Speciation in Diffusion Models",
        "summary": "Diffusion models do not recover semantic structure uniformly over time. Instead, samples transition from semantic ambiguity to class commitment within a narrow regime. Recent theoretical work attributes this transition to dynamical instabilities along class-separating directions, but practical methods to detect and exploit these windows in trained models are still limited. We show that tracking the class-conditional entropy of a latent semantic variable given the noisy state provides a reliable signature of these transition regimes. By restricting the entropy to semantic partitions, the entropy can furthermore resolve semantic decisions at different levels of abstraction. We analyze this behavior in high-dimensional Gaussian mixture models and show that the entropy rate concentrates on the same logarithmic time scale as the speciation symmetry-breaking instability previously identified in variance-preserving diffusion. We validate our method on EDM2-XS and Stable Diffusion 1.5, where class-conditional entropy consistently isolates the noise regimes critical for semantic structure formation. Finally, we use our framework to quantify how guidance redistributes semantic information over time. Together, these results connect information-theoretic and statistical physics perspectives on diffusion and provide a principled basis for time-localized control.",
        "authors": "Florian Handke, Dejan Stančević, Felix Koulischer, Thomas Demeester, Luca Ambrogioni",
        "url": "http://arxiv.org/abs/2602.09651v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09651v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "该论文揭示了扩散模型中类特化（class speciation）的熵特征，表明通过追踪潜在语义变量的类条件熵，可以可靠地识别扩散模型中的语义转换区域。它将信息论和统计物理学的视角联系起来，为扩散模型的时间局部控制提供了原理性基础。这种对扩散模型内部机制的深层理论理解，具有极高的创新性和严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09639v1",
        "title": "Blind denoising diffusion models and the blessings of dimensionality",
        "summary": "We analyze, theoretically and empirically, the performance of generative diffusion models based on \\emph{blind denoisers}, in which the denoiser is not given the noise amplitude in either the training or sampling processes. Assuming that the data distribution has low intrinsic dimensionality, we prove that blind denoising diffusion models (BDDMs), despite not having access to the noise amplitude, \\emph{automatically} track a particular \\emph{implicit} noise schedule along the reverse process. Our analysis shows that BDDMs can accurately sample from the data distribution in polynomially many steps as a function of the intrinsic dimension. Empirical results corroborate these mathematical findings on both synthetic and image data, demonstrating that the noise variance is accurately estimated from the noisy image. Remarkably, we observe that schedule-free BDDMs produce samples of higher quality compared to their non-blind counterparts. We provide evidence that this performance gain arises because BDDMs correct the mismatch between the true residual noise (of the image) and the noise assumed by the schedule used in non-blind diffusion models.",
        "authors": "Zahra Kadkhodaie, Aram-Alexandre Pooladian, Sinho Chewi, Eero Simoncelli",
        "url": "http://arxiv.org/abs/2602.09639v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09639v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "这篇论文理论和实证地分析了“盲去噪扩散模型”（BDDMs）的性能，证明了BDDMs在不访问噪声幅度的情况下，能够自动追踪一个特定的隐式噪声调度，并在多项式步数内准确采样。这一发现挑战了传统观念，并表明BDDMs在某些情况下甚至优于非盲模型，为扩散模型的设计提供了新的理论基础和简化方向。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09456v1",
        "title": "Taming the Monster Every Context: Complexity Measure and Unified Framework for Offline-Oracle Efficient Contextual Bandits",
        "summary": "We propose an algorithmic framework, Offline Estimation to Decisions (OE2D), that reduces contextual bandit learning with general reward function approximation to offline regression. The framework allows near-optimal regret for contextual bandits with large action spaces with $O(log(T))$ calls to an offline regression oracle over $T$ rounds, and makes $O(loglog(T))$ calls when $T$ is known. The design of OE2D algorithm generalizes Falcon~\\citep{simchi2022bypassing} and its linear reward version~\\citep[][Section 4]{xu2020upper} in that it chooses an action distribution that we term ``exploitative F-design'' that simultaneously guarantees low regret and good coverage that trades off exploration and exploitation. Central to our regret analysis is a new complexity measure, the Decision-Offline Estimation Coefficient (DOEC), which we show is bounded in bounded Eluder dimension per-context and smoothed regret settings. We also establish a relationship between DOEC and Decision Estimation Coefficient (DEC)~\\citep{foster2021statistical}, bridging the design principles of offline- and online-oracle efficient contextual bandit algorithms for the first time.",
        "authors": "Hao Qin, Chicheng Zhang",
        "url": "http://arxiv.org/abs/2602.09456v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09456v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "该论文提出了OE2D框架，将上下文强盗学习（contextual bandit learning）简化为离线回归问题，实现了近乎最优的遗憾值。引入了新的复杂性度量——决策-离线估计系数（DOEC），并首次建立了离线和在线预言机高效上下文强盗算法设计原则之间的桥梁。这是强化学习/强盗问题领域的深层理论创新，具有极高的严谨性和概念影响力。"
    },
    {
        "id": "http://arxiv.org/abs/2602.10014v1",
        "title": "A Task-Centric Theory for Iterative Self-Improvement with Easy-to-Hard Curricula",
        "summary": "Iterative self-improvement fine-tunes an autoregressive large language model (LLM) on reward-verified outputs generated by the LLM itself. In contrast to the empirical success of self-improvement, the theoretical foundation of this generative, iterative procedure in a practical, finite-sample setting remains limited. We make progress toward this goal by modeling each round of self-improvement as maximum-likelihood fine-tuning on a reward-filtered distribution and deriving finite-sample guarantees for the expected reward. Our analysis reveals an explicit feedback loop where better models accept more data per iteration, supporting sustained self-improvement while explaining eventual saturation of such improvement. Adopting a task-centric view by considering reasoning tasks with multiple difficulty levels, we further prove quantifiable conditions on model initialization, task difficulty, and sample budget where easy-to-hard curricula provably achieve better guarantees than training on fixed mixtures of tasks. Our analyses are validated via Monte-Carlo simulations and controlled experiments on graph-based reasoning tasks.",
        "authors": "Chenruo Liu, Yijun Dong, Yiqiu Shen, Qi Lei",
        "url": "http://arxiv.org/abs/2602.10014v1",
        "pdf_url": "https://arxiv.org/pdf/2602.10014v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文为LLM的迭代自我改进（iterative self-improvement）提供了任务中心理论基础，推导了预期奖励的有限样本保证，并揭示了模型改进的反馈循环和饱和机制。进一步证明了从易到难的课程设置在可量化条件下能提供更好的保证。这种对LLM学习过程的深层理论分析和数学严谨性，对指导未来LLM训练策略具有重要价值。"
    },
    {
        "id": "http://arxiv.org/abs/2602.10044v1",
        "title": "Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning",
        "summary": "Efficient exploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward environments. We introduce Optimistic World Models (OWMs), a principled and scalable framework for optimistic exploration that brings classical reward-biased maximum likelihood estimation (RBMLE) from adaptive control into deep RL. In contrast to upper confidence bound (UCB)-style exploration methods, OWMs incorporate optimism directly into model learning by augmentation with an optimistic dynamics loss that biases imagined transitions toward higher-reward outcomes. This fully gradient-based loss requires neither uncertainty estimates nor constrained optimization. Our approach is plug-and-play with existing world model frameworks, preserving scalability while requiring only minimal modifications to standard training procedures. We instantiate OWMs within two state-of-the-art world model architectures, leading to Optimistic DreamerV3 and Optimistic STORM, which demonstrate significant improvements in sample efficiency and cumulative return compared to their baseline counterparts.",
        "authors": "Akshay Mete, Shahid Aamir Sheikh, Tzu-Hsiang Lin, Dileep Kalathil, P. R. Kumar",
        "url": "http://arxiv.org/abs/2602.10044v1",
        "pdf_url": "https://arxiv.org/pdf/2602.10044v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "Optimistic World Models (OWMs) 框架将经典的奖励偏置最大似然估计（RBMLE）引入深度强化学习，通过乐观动态损失直接将乐观性融入模型学习，从而实现高效探索。这种将经典自适应控制理论与深度RL结合的原理性方法，解决了稀疏奖励环境中高效探索的挑战，提升了样本效率，具有理论创新性。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09395v1",
        "title": "Sparse Layer Sharpness-Aware Minimization for Efficient Fine-Tuning",
        "summary": "Sharpness-aware minimization (SAM) seeks the minima with a flat loss landscape to improve the generalization performance in machine learning tasks, including fine-tuning. However, its extra parameter perturbation step doubles the computation cost, which becomes the bottleneck of SAM in the practical implementation. In this work, we propose an approach SL-SAM to break this bottleneck by introducing the sparse technique to layers. Our key innovation is to frame the dynamic selection of layers for both the gradient ascent (perturbation) and descent (update) steps as a multi-armed bandit problem. At the beginning of each iteration, SL-SAM samples a part of the layers of the model according to the gradient norm to participate in the backpropagation of the following parameter perturbation and update steps, thereby reducing the computation complexity. We then provide the analysis to guarantee the convergence of SL-SAM. In the experiments of fine-tuning models in several tasks, SL-SAM achieves the performances comparable to the state-of-the-art baselines, including a \\#1 rank on LLM fine-tuning. Meanwhile, SL-SAM significantly reduces the ratio of active parameters in backpropagation compared to vanilla SAM (SL-SAM activates 47\\%, 22\\% and 21\\% parameters on the vision, moderate and large language model respectively while vanilla SAM always activates 100\\%), verifying the efficiency of our proposed algorithm.",
        "authors": "Yifei Cheng, Xianglin Yang, Guoxia Wang, Chao Huang, Fei Ma, Dianhai Yu, Xiaochun Cao, Li Shen",
        "url": "http://arxiv.org/abs/2602.09395v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09395v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "SL-SAM通过将动态选择参与反向传播的层视为多臂老虎机问题，引入稀疏技术来优化Sharpness-Aware Minimization (SAM) 的计算成本。它显著减少了反向传播中的活跃参数比例，同时保持了与SOTA相当的性能。这种对优化算法的理论创新，直接解决了大模型微调的效率瓶颈，具有很高的实践价值。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09475v1",
        "title": "ArtifactLens: Hundreds of Labels Are Enough for Artifact Detection with VLMs",
        "summary": "Modern image generators produce strikingly realistic images, where only artifacts like distorted hands or warped objects reveal their synthetic origin. Detecting these artifacts is essential: without detection, we cannot benchmark generators or train reward models to improve them. Current detectors fine-tune VLMs on tens of thousands of labeled images, but this is expensive to repeat whenever generators evolve or new artifact types emerge. We show that pretrained VLMs already encode the knowledge needed to detect artifacts - with the right scaffolding, this capability can be unlocked using only a few hundred labeled examples per artifact category. Our system, ArtifactLens, achieves state-of-the-art on five human artifact benchmarks (the first evaluation across multiple datasets) while requiring orders of magnitude less labeled data. The scaffolding consists of a multi-component architecture with in-context learning and text instruction optimization, with novel improvements to each. Our methods generalize to other artifact types - object morphology, animal anatomy, and entity interactions - and to the distinct task of AIGC detection.",
        "authors": "James Burgess, Rameen Abdal, Dan Stoddart, Sergey Tulyakov, Serena Yeung-Levy, Kuan-Chieh Jackson Wang",
        "url": "http://arxiv.org/abs/2602.09475v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09475v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "Align-TI是一个新颖的知识蒸馏框架，通过关注视觉-指令令牌交互和响应内令牌交互来压缩多模态大语言模型（MLLMs）。它解决了现有KD方法仅依赖静态下一令牌对齐的局限性，实现了参数高效的MLLM训练，甚至超越了更大的基线模型。这种从令牌交互角度进行KD的理论创新，对MLLM压缩和效率提升具有重要意义。"
    },
    {
        "id": "http://arxiv.org/abs/2602.09316v1",
        "title": "Effective MoE-based LLM Compression by Exploiting Heterogeneous Inter-Group Experts Routing Frequency and Information Density",
        "summary": "Mixture-of-Experts (MoE) based Large Language Models (LLMs) have achieved superior performance, yet the massive memory overhead caused by storing multiple expert networks severely hinders their practical deployment. Singular Value Decomposition (SVD)-based compression has emerged as a promising post-training technique; however, most existing methods apply uniform rank allocation or rely solely on static weight properties. This overlooks the substantial heterogeneity in expert utilization observed in MoE models, where frequent routing patterns and intrinsic information density vary significantly across experts. In this work, we propose RFID-MoE, an effective framework for MoE compression by exploiting heterogeneous Routing Frequency and Information Density. We first introduce a fused metric that combines expert activation frequency with effective rank to measure expert importance, adaptively allocating higher ranks to critical expert groups under a fixed budget. Moreover, instead of discarding compression residuals, we reconstruct them via a parameter-efficient sparse projection mechanism to recover lost information with minimal parameter overhead. Extensive experiments on representative MoE LLMs (e.g., Qwen3, DeepSeekMoE) across multiple compression ratios demonstrate that RFID-MoE consistently outperforms state-of-the-art methods like MoBE and D2-MoE. Notably, RFID-MoE achieves a perplexity of 16.92 on PTB with the Qwen3-30B model at a 60% compression ratio, reducing perplexity by over 8.0 compared to baselines, and improves zero-shot accuracy on HellaSwag by approximately 8%.",
        "authors": "Zhendong Mi, Yixiao Chen, Pu Zhao, Xiaodong Yu, Hao Wang, Yanzhi Wang, Shaoyi Huang",
        "url": "http://arxiv.org/abs/2602.09316v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09316v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "RFID-MoE框架通过利用MoE模型中专家路由频率和信息密度的异质性，实现了高效的MoE模型压缩。它引入了一个结合专家激活频率和有效秩的融合度量，并采用参数高效的稀疏投影机制重建压缩残差。这种基于理论洞察的压缩策略，显著解决了MoE LLM巨大的内存开销瓶颈，具有极高的实践影响力。"
    }
]