[
    {
        "id": "http://arxiv.org/abs/2601.09465v1",
        "title": "EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines",
        "summary": "While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but unconstrained optimization often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the DeepSearch benchmark. Additional results on interactive decision-making tasks further validate its generalization.",
        "authors": "Shuo Zhang, Chaofa Yuan, Ryan Guo, Xiaomin Yu, Rui Xu, Zhangquan Chen, Zinuo Li, Zhi Yang, Shuhao Guan, Zhenheng Tang, Sen Hu, Liwen Zhang, Ronghao Chen, Huacan Wang",
        "url": "http://arxiv.org/abs/2601.09465v1",
        "pdf_url": "https://arxiv.org/pdf/2601.09465v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "这篇论文提出了EvoFSM框架，通过演化有限状态机（FSM）实现LLM智能体的可控自进化，将宏观流程与微观技能解耦。其理论创新性在于FSM的结构化演化机制和自进化记忆，有效解决了LLM智能体在开放式查询中的适应性和稳定性问题，具有显著的实践影响力，完美契合您对AI前沿算法和架构的偏好。"
    },
    {
        "id": "http://arxiv.org/abs/2601.09269v1",
        "title": "RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering",
        "summary": "Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.",
        "authors": "Wencheng Ye, Liang Peng, Xiaoyang Yuan, Yi Bin, Pengpeng Zeng, Hengyu Jin, Heng Tao Shen",
        "url": "http://arxiv.org/abs/2601.09269v1",
        "pdf_url": "https://arxiv.org/pdf/2601.09269v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "RISER框架通过强化学习优化的轻量级路由器，在激活空间自适应地引导LLM推理，实现了参数高效的推理增强。其理论创新性在于动态组合推理向量以激活潜在认知原语，为LLM的精细化控制和效率提升提供了新视角，对LLM算法架构有重要意义。"
    },
    {
        "id": "http://arxiv.org/abs/2601.09260v1",
        "title": "Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models",
        "summary": "High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.",
        "authors": "Yan Liu, Feng Zhang, Zhanyu Ma, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He, Han Liu, Yangdong Deng",
        "url": "http://arxiv.org/abs/2601.09260v1",
        "pdf_url": "https://arxiv.org/pdf/2601.09260v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "CoT-Flow将LLM的离散推理步骤重构为连续概率流，量化每一步的信息增益，从而实现流引导解码和无验证器的密集奖励强化学习。该方法理论严谨性高，解决了CoT推理效率低下和优化困难的瓶颈，对LLM推理算法的效率和性能提升具有深远影响。"
    },
    {
        "id": "http://arxiv.org/abs/2601.09233v1",
        "title": "GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization",
        "summary": "The prevailing post-training paradigm for Large Reasoning Models (LRMs)--Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL)--suffers from an intrinsic optimization mismatch: the rigid supervision inherent in SFT induces distributional collapse, thereby exhausting the exploration space necessary for subsequent RL. In this paper, we reformulate SFT within a unified post-training framework and propose Gibbs Initialization with Finite Temperature (GIFT). We characterize standard SFT as a degenerate zero-temperature limit that suppresses base priors. Conversely, GIFT incorporates supervision as a finite-temperature energy potential, establishing a distributional bridge that ensures objective consistency throughout the post-training pipeline. Our experiments demonstrate that GIFT significantly outperforms standard SFT and other competitive baselines when utilized for RL initialization, providing a mathematically principled pathway toward achieving global optimality in post-training. Our code is available at https://github.com/zzy1127/GIFT.",
        "authors": "Zhengyang Zhao, Lu Ma, Yizhen Jiang, Xiaochen Ma, Zimo Meng, Chengyu Shen, Lexiang Tang, Haoze Sun, Peng Pei, Wentao Zhang",
        "url": "http://arxiv.org/abs/2601.09233v1",
        "pdf_url": "https://arxiv.org/pdf/2601.09233v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "GIFT通过引入有限温度吉布斯初始化，解决了SFT（监督微调）与RL（强化学习）之间固有的优化不匹配问题，建立了分布桥梁。其理论创新性极高，将SFT表征为零温度极限，为LLM的后训练范式提供了基础性的数学原理和优化路径。"
    },
    {
        "id": "http://arxiv.org/abs/2601.09151v1",
        "title": "Interpretable Probability Estimation with LLMs via Shapley Reconstruction",
        "summary": "Large Language Models (LLMs) demonstrate potential to estimate the probability of uncertain events, by leveraging their extensive knowledge and reasoning capabilities. This ability can be applied to support intelligent decision-making across diverse fields, such as financial forecasting and preventive healthcare. However, directly prompting LLMs for probability estimation faces significant challenges: their outputs are often noisy, and the underlying predicting process is opaque. In this paper, we propose PRISM: Probability Reconstruction via Shapley Measures, a framework that brings transparency and precision to LLM-based probability estimation. PRISM decomposes an LLM's prediction by quantifying the marginal contribution of each input factor using Shapley values. These factor-level contributions are then aggregated to reconstruct a calibrated final estimate. In our experiments, we demonstrate PRISM improves predictive accuracy over direct prompting and other baselines, across multiple domains including finance, healthcare, and agriculture. Beyond performance, PRISM provides a transparent prediction pipeline: our case studies visualize how individual factors shape the final estimate, helping build trust in LLM-based decision support systems.",
        "authors": "Yang Nan, Qihao Wen, Jiahao Wang, Pengfei He, Ravi Tandon, Yong Ge, Han Xu",
        "url": "http://arxiv.org/abs/2601.09151v1",
        "pdf_url": "https://arxiv.org/pdf/2601.09151v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "PRISM框架利用Shapley值量化每个输入因素对LLM预测的边际贡献，实现了LLM概率估计的可解释性与精确性。该方法理论严谨性高，解决了LLM决策过程不透明的实际瓶颈，有助于在金融、医疗等高风险领域建立对LLM的信任。"
    },
    {
        "id": "http://arxiv.org/abs/2601.09694v1",
        "title": "LLMs can Compress LLMs: Adaptive Pruning by Agents",
        "summary": "As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratios. Moreover, recent work has shown that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering capabilities. We introduce agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select which layers to prune at each iteration while preserving critical knowledge pathways. Our method constructs layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics with gradient importance scores, normalized as z-scores for model-agnostic comparison. These statistics are processed by an LLM agent equipped with self-reflection capabilities, enabling it to learn from previous pruning outcomes and iteratively refine its strategy. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. We evaluate our approach on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, demonstrating substantial improvements over structured pruning baselines: 56% relative improvement in MMLU accuracy, 19x better factual knowledge retention on FreebaseQA, and 69% lower perplexity degradation. Notably, our framework requires no retraining, operates in a model-agnostic manner, and exhibits effective self-correction with only 2-4 rollbacks across 21-40 iterations, demonstrating that foundation models can effectively guide the compression of other foundation models.",
        "authors": "Sai Varun Kodathala, Rakesh Vunnam",
        "url": "http://arxiv.org/abs/2601.09694v1",
        "pdf_url": "https://arxiv.org/pdf/2601.09694v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 4,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "这篇论文提出了一种创新的LLM智能体自适应剪枝LLM的方法，通过结合权重-激活指标和梯度重要性分数构建层级敏感度，并利用LLM智能体进行自反思迭代优化。其创新性在于利用LLM本身来指导另一个LLM的压缩，有效解决了LLM模型压缩中的知识退化问题，具有极高的实践价值。"
    },
    {
        "id": "http://arxiv.org/abs/2601.09684v1",
        "title": "Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection",
        "summary": "Multi-Task Learning (MTL) combined with Low-Rank Adaptation (LoRA) has emerged as a promising direction for parameter-efficient deployment of Large Language Models (LLMs). By sharing a single adapter across multiple tasks, one can significantly reduce storage overhead. However, this approach suffers from negative transfer, where conflicting gradient updates from distinct tasks degrade the performance of individual tasks compared to single-task fine-tuning. This problem is exacerbated in LoRA due to the low-rank constraint, which limits the optimization landscape's capacity to accommodate diverse task requirements. In this paper, we propose Ortho-LoRA, a gradient projection method specifically tailored for the bipartite structure of LoRA. Ortho-LoRA dynamically projects conflicting task gradients onto the orthogonal complement of each other within the intrinsic LoRA subspace. Extensive experiments on the GLUE benchmark demonstrate that Ortho-LoRA effectively mitigates task interference, outperforming standard joint training and recovering 95\\% of the performance gap between multi-task and single-task baselines with negligible computational overhead.",
        "authors": "Ziyu Yang, Guibin Chen, Yuxin Yang, Aoxiong Zeng, Xiangquan Yang",
        "url": "http://arxiv.org/abs/2601.09684v1",
        "pdf_url": "https://arxiv.org/pdf/2601.09684v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "Ortho-LoRA针对LoRA在多任务学习中梯度冲突导致的负迁移问题，提出了正交梯度投影方法，将冲突梯度投影到LoRA子空间的正交补集。该方法理论严谨性高，有效提升了LLM参数效率和多任务性能，是LLM架构优化和效率提升的优秀范例。"
    },
    {
        "id": "http://arxiv.org/abs/2601.09667v1",
        "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
        "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.",
        "authors": "Zhiyuan Hu, Yunhai Hu, Juncheng Liu, Shuyue Stella Li, Yucheng Wang, Zhen Xu, See-Kiong Ng, Anh Tuan Luu, Xinxing Xu, Bryan Hooi, Cynthia Breazeal, Hae Won Park",
        "url": "http://arxiv.org/abs/2601.09667v1",
        "pdf_url": "https://arxiv.org/pdf/2601.09667v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "MATTRL框架在推理时将结构化文本经验注入多智能体协作推理中，解决了传统多智能体强化学习训练资源密集和不稳定的问题。其理论创新性在于测试时强化学习的引入，对多智能体系统和LLM推理的鲁棒性和效率有重要贡献。"
    },
    {
        "id": "http://arxiv.org/abs/2601.09398v1",
        "title": "Ability Transfer and Recovery via Modularized Parameters Localization",
        "summary": "Large language models can be continually pre-trained or fine-tuned to improve performance in specific domains, languages, or skills, but this specialization often degrades other capabilities and may cause catastrophic forgetting. We investigate how abilities are distributed within LLM parameters by analyzing module activations under domain- and language-specific inputs for closely related models. Across layers and modules, we find that ability-related activations are highly concentrated in a small set of channels (typically <5\\%), and these channels are largely disentangled with good sufficiency and stability. Building on these observations, we propose ACT (Activation-Guided Channel-wise Ability Transfer), which localizes ability-relevant channels via activation differences and selectively transfers only the corresponding parameters, followed by lightweight fine-tuning for compatibility. Experiments on multilingual mathematical and scientific reasoning show that ACT can recover forgotten abilities while preserving retained skills. It can also merge multiple specialized models to integrate several abilities into a single model with minimal interference. Our code and data will be publicly released.",
        "authors": "Songyao Jin, Kun Zhou, Wenqi Li, Peng Wang, Biwei Huang",
        "url": "http://arxiv.org/abs/2601.09398v1",
        "pdf_url": "https://arxiv.org/pdf/2601.09398v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "ACT方法通过激活差异定位LLM中与能力相关的通道，并选择性地转移相应参数，以实现灾难性遗忘的恢复和多模型能力融合。该研究深入分析了LLM内部机制，理论创新性强，对LLM架构的模块化和持续学习具有重要意义。"
    },
    {
        "id": "http://arxiv.org/abs/2601.09306v1",
        "title": "On-Device Large Language Models for Sequential Recommendation",
        "summary": "On-device recommendation is critical for a number of real-world applications, especially in scenarios that have agreements on execution latency, user privacy, and robust functionality when internet connectivity is unstable or even impossible. While large language models (LLMs) can now provide exceptional capabilities that model user behavior for sequential recommendation tasks, their substantial memory footprint and computational overhead make the deployment on resource-constrained devices a high risk proposition. In this paper, we propose OD-LLM, the first task-adaptive compression framework explicitly designed to provide efficient and accurate on-device deployment of LLMs for sequential recommendation tasks. OD-LLM uniquely integrates two complementary compression strategies: a low-rank structural compression algorithm which uses Singular Value Decomposition (SVD) to significantly reduce parameter redundancy in the model, and a novel tokenization normalization technique that better complements the low-rank decomposition process being used. Additionally, to minimize any potential performance degradation when using higher compression ratios, a novel progressive alignment algorithm is used to iteratively refine the parameters required layerwise in the target model. Empirical evaluations conducted on sequential recommendation benchmarks show that OD-LLM exhibits no loss in effectiveness when compared to the original recommendation model, when the deployed model size is halved. These promising results demonstrate the efficacy and scalability of OD-LLM, making this novel solution a practical alternative for real-time, on-device solutions wishing to replace expensive, remotely executed LLMs.",
        "authors": "Xin Xia, Hongzhi Yin, Shane Culpepper",
        "url": "http://arxiv.org/abs/2601.09306v1",
        "pdf_url": "https://arxiv.org/pdf/2601.09306v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "OD-LLM是首个针对序列推荐任务的LLM设备端压缩框架，结合了低秩结构压缩（SVD）、tokenization归一化和渐进对齐算法。其理论创新性在于对LLM压缩算法的系统性整合，解决了LLM在资源受限设备上的部署瓶颈，具有极高的实践价值。"
    },
    {
        "id": "http://arxiv.org/abs/2601.09285v1",
        "title": "Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction",
        "summary": "Metal-organic frameworks (MOFs) are porous crystalline materials with broad applications such as carbon capture and drug delivery, yet accurately predicting their 3D structures remains a significant challenge. While Large Language Models (LLMs) have shown promise in generating crystals, their application to MOFs is hindered by MOFs' high atomic complexity. Inspired by the success of block-wise paradigms in deep generative models, we pioneer the use of LLMs in this domain by introducing MOF-LLM, the first LLM framework specifically adapted for block-level MOF structure prediction. To effectively harness LLMs for this modular assembly task, our training paradigm integrates spatial-aware continual pre-training (CPT), structural supervised fine-tuning (SFT), and matching-driven reinforcement learning (RL). By incorporating explicit spatial priors and optimizing structural stability via Soft Adaptive Policy Optimization (SAPO), our approach substantially enhances the spatial reasoning capability of a Qwen-3 8B model for accurate MOF structure prediction. Comprehensive experiments demonstrate that MOF-LLM outperforms state-of-the-art denoising-based and LLM-based methods while exhibiting superior sampling efficiency.",
        "authors": "Mianzhi Pan, JianFei Li, Peishuo Liu, Botian Wang, Yawen Ouyang, Yiming Rong, Hao Zhou, Jianbing Zhang",
        "url": "http://arxiv.org/abs/2601.09285v1",
        "pdf_url": "https://arxiv.org/pdf/2601.09285v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "MOF-LLM是首个用于金属有机框架（MOF）结构预测的LLM框架，通过空间感知持续预训练、结构SFT和匹配驱动RL增强空间推理能力。该研究将LLM应用于复杂材料科学领域，理论创新性强，对LLM的跨领域应用和空间推理能力提升有突破性意义。"
    },
    {
        "id": "http://arxiv.org/abs/2601.09281v1",
        "title": "STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models",
        "summary": "Large Reasoning Models (LRMs) have advanced automated multi-step reasoning, but their ability to generate complex Chain-of-Thought (CoT) trajectories introduces severe privacy risks, as sensitive information may be deeply embedded throughout the reasoning process. Existing Large Language Models (LLMs) unlearning approaches that typically focus on modifying only final answers are insufficient for LRMs, as they fail to remove sensitive content from intermediate steps, leading to persistent privacy leakage and degraded security. To address these challenges, we propose Sensitive Trajectory Regulation (STaR), a parameter-free, inference-time unlearning framework that achieves robust privacy protection throughout the reasoning process. Specifically, we first identify sensitive content via semantic-aware detection. Then, we inject global safety constraints through secure prompt prefix. Next, we perform trajectory-aware suppression to dynamically block sensitive content across the entire reasoning chain. Finally, we apply token-level adaptive filtering to prevent both exact and paraphrased sensitive tokens during generation. Furthermore, to overcome the inadequacies of existing evaluation protocols, we introduce two metrics: Multi-Decoding Consistency Assessment (MCS), which measures the consistency of unlearning across diverse decoding strategies, and Multi-Granularity Membership Inference Attack (MIA) Evaluation, which quantifies privacy protection at both answer and reasoning-chain levels. Experiments on the R-TOFU benchmark demonstrate that STaR achieves comprehensive and stable unlearning with minimal utility loss, setting a new standard for privacy-preserving reasoning in LRMs.",
        "authors": "Jingjing Zhou, Gaoxiang Cong, Li Su, Liang Li",
        "url": "http://arxiv.org/abs/2601.09281v1",
        "pdf_url": "https://arxiv.org/pdf/2601.09281v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "STaR框架提出了一种参数无关、推理时的大型推理模型（LRM）遗忘方法，通过敏感内容识别、安全提示注入、轨迹感知抑制和token级自适应过滤，实现推理过程中的隐私保护。其理论创新性强，解决了LRM隐私泄露的实际瓶颈，对LLM的安全应用至关重要。"
    },
    {
        "id": "http://arxiv.org/abs/2601.09176v1",
        "title": "$D^2Prune$: Sparsifying Large Language Models via Dual Taylor Expansion and Attention Distribution Awareness",
        "summary": "Large language models (LLMs) face significant deployment challenges due to their massive computational demands. % While pruning offers a promising compression solution, existing methods suffer from two critical limitations: (1) They neglect activation distribution shifts between calibration data and test data, resulting in inaccurate error estimations; (2) They overlook the long-tail distribution characteristics of activations in the attention module. To address these limitations, this paper proposes a novel pruning method, $D^2Prune$. First, we propose a dual Taylor expansion-based method that jointly models weight and activation perturbations for precise error estimation, leading to precise pruning mask selection and weight updating and facilitating error minimization during pruning. % Second, we propose an attention-aware dynamic update strategy that preserves the long-tail attention pattern by jointly minimizing the KL divergence of attention distributions and the reconstruction error. Extensive experiments show that $D^2Prune$ consistently outperforms SOTA methods across various LLMs (e.g., OPT-125M, LLaMA2/3, and Qwen3). Moreover, the dynamic attention update mechanism also generalizes well to ViT-based vision models like DeiT, achieving superior accuracy on ImageNet-1K.",
        "authors": "Lang Xiong, Ning Liu, Ao Ren, Yuheng Bai, Haining Fang, BinYan Zhang, Zhe Jiang, Yujuan Tan, Duo Liu",
        "url": "http://arxiv.org/abs/2601.09176v1",
        "pdf_url": "https://arxiv.org/pdf/2601.09176v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "D2Prune剪枝方法通过双泰勒展开联合建模权重和激活扰动，并结合注意力分布感知动态更新策略，解决了现有剪枝方法中激活分布偏移和注意力长尾分布问题。该方法理论严谨性高，对LLM模型压缩的精度和效率有重要贡献。"
    },
    {
        "id": "http://arxiv.org/abs/2601.09172v1",
        "title": "BalDRO: A Distributionally Robust Optimization based Framework for Large Language Model Unlearning",
        "summary": "As Large Language Models (LLMs) increasingly shape online content, removing targeted information from well-trained LLMs (also known as LLM unlearning) has become critical for web governance. A key challenge lies in sample-wise imbalance within the forget set: different samples exhibit widely varying unlearning difficulty, leading to asynchronous forgetting where some knowledge remains insufficiently erased while others become over-forgotten. To address this, we propose BalDRO, a novel and efficient framework for balanced LLM unlearning. BalDRO formulates unlearning as a min-sup process: an inner step identifies a worst-case data distribution that emphasizes hard-to-unlearn samples, while an outer step updates model parameters under this distribution. We instantiate BalDRO via two efficient variants: BalDRO-G, a discrete GroupDRO-based approximation focusing on high-loss subsets, and BalDRO-DV, a continuous Donsker-Varadhan dual method enabling smooth adaptive weighting within standard training pipelines. Experiments on TOFU and MUSE show that BalDRO significantly improves both forgetting quality and model utility over existing methods, and we release code for reproducibility.",
        "authors": "Pengyang Shao, Naixin Zhai, Lei Chen, Yonghui Yang, Fengbin Zhu, Xun Yang, Meng Wang",
        "url": "http://arxiv.org/abs/2601.09172v1",
        "pdf_url": "https://arxiv.org/pdf/2601.09172v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "BalDRO框架将LLM遗忘问题公式化为min-sup过程，通过分布鲁棒优化（DRO）解决遗忘集中样本不平衡问题，实现平衡的LLM遗忘。该方法理论严谨性高，对LLM治理和隐私保护提供了新的理论工具和实践路径。"
    },
    {
        "id": "http://arxiv.org/abs/2601.09185v1",
        "title": "OrthoGeoLoRA: Geometric Parameter-Efficient Fine-Tuning for Structured Social Science Concept Retrieval on theWeb",
        "summary": "Large language models and text encoders increasingly power web-based information systems in the social sciences, including digital libraries, data catalogues, and search interfaces used by researchers, policymakers, and civil society. Full fine-tuning is often computationally and energy intensive, which can be prohibitive for smaller institutions and non-profit organizations in the Web4Good ecosystem. Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation (LoRA), reduces this cost by updating only a small number of parameters. We show that the standard LoRA update $ΔW = BA^\\top$ has geometric drawbacks: gauge freedom, scale ambiguity, and a tendency toward rank collapse. We introduce OrthoGeoLoRA, which enforces an SVD-like form $ΔW = BΣA^\\top$ by constraining the low-rank factors to be orthogonal (Stiefel manifold). A geometric reparameterization implements this constraint while remaining compatible with standard optimizers such as Adam and existing fine-tuning pipelines. We also propose a benchmark for hierarchical concept retrieval over the European Language Social Science Thesaurus (ELSST), widely used to organize social science resources in digital repositories. Experiments with a multilingual sentence encoder show that OrthoGeoLoRA outperforms standard LoRA and several strong PEFT variants on ranking metrics under the same low-rank budget, offering a more compute- and parameter-efficient path to adapt foundation models in resource-constrained settings.",
        "authors": "Zeqiang Wang, Xinyue Wu, Chenxi Li, Zixi Chen, Nishanth Sastry, Jon Johnson, Suparna De",
        "url": "http://arxiv.org/abs/2601.09185v1",
        "pdf_url": "https://arxiv.org/pdf/2601.09185v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "OrthoGeoLoRA通过约束低秩因子正交来增强LoRA的几何特性，解决了标准LoRA的几何缺陷（如规范自由度、尺度模糊和秩崩溃）。该方法理论严谨性高，为参数高效微调提供了更高效和鲁棒的途径，对LLM效率和架构优化有重要贡献。"
    }
]