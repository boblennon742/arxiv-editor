{
    "id": "http://arxiv.org/abs/2511.06719v1",
    "title": "MobileLLM-Pro Technical Report",
    "summary": "Efficient on-device language models around 1 billion parameters are essential for powering low-latency AI applications on mobile and wearable devices. However, achieving strong performance in this model class, while supporting long context windows and practical deployment remains a significant challenge. We introduce MobileLLM-Pro, a 1-billion-parameter language model optimized for on-device deployment. MobileLLM-Pro achieves state-of-the-art results across 11 standard benchmarks, significantly outperforming both Gemma 3-1B and Llama 3.2-1B, while supporting context windows of up to 128,000 tokens and showing only minor performance regressions at 4-bit quantization. These improvements are enabled by four core innovations: (1) implicit positional distillation, a novel technique that effectively instills long-context capabilities through knowledge distillation; (2) a specialist model merging framework that fuses multiple domain experts into a compact model without parameter growth; (3) simulation-driven data mixing using utility estimation; and (4) 4-bit quantization-aware training with self-distillation. We release our model weights and code to support future research in efficient on-device language models.",
    "authors": "Patrick Huber, Ernie Chang, Wei Wen, Igor Fedorov, Tarek Elgamal, Hanxian Huang, Naveen Suda, Chinnadhurai Sankar, Vish Vogeti, Yanghan Wang, Alex Gladkov, Kai Sheng Tai, Abdelrahman Elogeel, Tarek Hefny, Vikas Chandra, Ahmed Aly, Anuj Kumar, Raghuraman Krishnamoorthi, Adithya Sagar",
    "url": "http://arxiv.org/abs/2511.06719v1",
    "pdf_url": "http://arxiv.org/pdf/2511.06719v1",
    "reason_zh": "这篇论文完美契合您的偏好/任务。它专注于为移动和可穿戴设备部署高效的片上（on-device）语言模型，这直接解决了“模型部署和效率问题”中的“Edge AI”部分。论文明确提到了您感兴趣的几种效率提升技术：1) **量化 (Quantization)**：它通过“4-bit quantization-aware training with self-distillation”实现了在4位量化下性能的微小下降，显著提高了效率。2) **知识蒸馏 (Knowledge Distillation)**：论文采用了“implicit positional distillation”和“self-distillation”来提升模型能力和效率。3) **模型剪枝 (Model Pruning) 的理念**：通过“specialist model merging framework”将多个专家模型融合为一个紧凑模型，在不增加参数的情况下实现模型精简，这与剪枝在减小模型体积和计算量上的目标一致。该方法旨在“实际落地并显著提高效率”，并通过发布模型权重和代码来支持未来的研究，进一步证明了其实用性。其在LLM领域的应用也使其在当前研究中具有高度的相关性和实用价值。",
    "reason_en": "This paper perfectly aligns with your preferences and task. It focuses on deploying efficient on-device language models for mobile and wearable devices, directly addressing the 'Edge AI' aspect of 'model deployment and efficiency problems.' The paper explicitly mentions several efficiency enhancement techniques you are interested in: 1) **Quantization**: It achieves only minor performance regressions at 4-bit quantization through '4-bit quantization-aware training with self-distillation,' significantly boosting efficiency. 2) **Knowledge Distillation**: The paper utilizes 'implicit positional distillation' and 'self-distillation' to enhance model capabilities and efficiency. 3) **The concept of Model Pruning**: Through a 'specialist model merging framework,' it fuses multiple domain experts into a compact model without parameter growth, which aligns with pruning's goal of reducing model size and computational load. The method aims for 'actual implementability and significant efficiency improvement,' further demonstrated by releasing model weights and code to support future research, proving its practicality. Its application in the LLM domain also gives it high relevance and practical value in current research."
}