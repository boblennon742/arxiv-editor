[
    {
        "id": "http://arxiv.org/abs/2512.16857v1",
        "title": "Identification and efficient estimation of compliance and network causal effects in cluster-randomized trials",
        "summary": "Treatment noncompliance is pervasive in infectious disease cluster-randomized trials. Although all individuals within a cluster are assigned the same treatment condition, the treatment uptake status may vary across individuals due to noncompliance. We propose a semiparametric framework to evaluate the individual compliance effect and network assignment effect within principal stratum exhibiting different patterns of noncompliance. The individual compliance effect captures the portion of the treatment effect attributable to changes in treatment receipt, while the network assignment effect reflects the pure impact of treatment assignment and spillover among individuals within the same cluster. Unlike prior efforts which either empirically identify or interval identify these estimands, we characterize new structural assumptions for nonparametric point identification. We then develop semiparametrically efficient estimators that combine data-adaptive machine learning methods with efficient influence functions to enable more robust inference. Additionally, we introduce sensitivity analysis methods to study the impact under assumption violations, and apply the proposed methods to reanalyze a cluster-randomized trial in Kenya that evaluated the impact of school-based mass deworming on disease transmission.",
        "authors": "Chao Cheng, Georgia Papadogeorgou, Fan Li",
        "url": "http://arxiv.org/abs/2512.16857v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16857v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文提出了一个半参数框架，用于在聚类随机试验中评估个体依从性效应和网络分配效应，并表征了非参数点识别的新结构假设。其核心贡献在于开发了结合数据自适应机器学习方法和高效影响函数的半参数高效估计器，并引入了敏感性分析方法。这与您对统计保证、因果逻辑和严谨数学推导的偏好高度契合。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16733v1",
        "title": "Discovering and Learning Probabilistic Models of Black-Box AI Capabilities",
        "summary": "Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.",
        "authors": "Daniel Bramblett, Rushang Karia, Adrian Ciotinga, Ruthvick Suresh, Pulkit Verma, YooJung Choi, Siddharth Srivastava",
        "url": "http://arxiv.org/abs/2512.16733v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16733v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文关注黑盒AI系统的能力建模，并提出了使用PDDL风格表示来学习和建模其规划能力的方法。摘要明确指出“理论结果展示了学习模型的健全性、完备性和收敛性”，这直接满足了您对优化收敛性和理论保证的严格要求。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16515v1",
        "title": "The Universe Learning Itself: On the Evolution of Dynamics from the Big Bang to Machine Intelligence",
        "summary": "We develop a unified, dynamical-systems narrative of the universe that traces a continuous chain of structure formation from the Big Bang to contemporary human societies and their artificial learning systems. Rather than treating cosmology, astrophysics, geophysics, biology, cognition, and machine intelligence as disjoint domains, we view each as successive regimes of dynamics on ever-richer state spaces, stitched together by phase transitions, symmetry-breaking events, and emergent attractors. Starting from inflationary field dynamics and the growth of primordial perturbations, we describe how gravitational instability sculpts the cosmic web, how dissipative collapse in baryonic matter yields stars and planets, and how planetary-scale geochemical cycles define long-lived nonequilibrium attractors. Within these attractors, we frame the origin of life as the emergence of self-maintaining reaction networks, evolutionary biology as flow on high-dimensional genotype-phenotype-environment manifolds, and brains as adaptive dynamical systems operating near critical surfaces. Human culture and technology-including modern machine learning and artificial intelligence-are then interpreted as symbolic and institutional dynamics that implement and refine engineered learning flows which recursively reshape their own phase space. Throughout, we emphasize recurring mathematical motifs-instability, bifurcation, multiscale coupling, and constrained flows on measure-zero subsets of the accessible state space. Our aim is not to present any new cosmological or biological model, but a cross-scale, theoretical perspective: a way of reading the universe's history as the evolution of dynamics itself, culminating (so far) in biological and artificial systems capable of modeling, predicting, and deliberately perturbing their own future trajectories.",
        "authors": "Pradeep Singh, Mudasani Rushikesh, Bezawada Sri Sai Anurag, Balasubramanian Raman",
        "url": "http://arxiv.org/abs/2512.16515v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16515v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 4,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "这篇论文从动力系统角度，将宇宙从大爆炸到机器智能的演化视为连续的结构形成链条。尽管并非直接的统计学论文，但其强调“贯穿始终的数学主题——不稳定、分岔、多尺度耦合以及可达状态空间零测度子集上的受限流”，提供了一个跨尺度的理论视角，对理解复杂系统和AI的动态演化具有深刻的数学和哲学严谨性，且清晰度极高，非常适合您对严谨数学逻辑的追求。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16452v1",
        "title": "Smart Data Portfolios: A Quantitative Framework for Input Governance in AI",
        "summary": "Growing concerns about fairness, privacy, robustness, and transparency have made it a central expectation of AI governance that automated decisions be explainable by institutions and intelligible to affected parties. We introduce the Smart Data Portfolio (SDP) framework, which treats data categories as productive but risk-bearing assets, formalizing input governance as an information-risk trade-off. Within this framework, we define two portfolio-level quantities, Informational Return and Governance-Adjusted Risk, whose interaction characterizes data mixtures and generates a Governance-Efficient Frontier. Regulators shape this frontier through risk caps, admissible categories, and weight bands that translate fairness, privacy, robustness, and provenance requirements into measurable constraints on data allocation while preserving model flexibility. A telecommunications illustration shows how different AI services require distinct portfolios within a common governance structure. The framework offers a familiar portfolio logic as an input-level explanation layer suited to the large-scale deployment of AI systems.",
        "authors": "A. Talha Yalta, A. Yasemin Yalta",
        "url": "http://arxiv.org/abs/2512.16452v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16452v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "该论文引入了“智能数据组合 (Smart Data Portfolio)”框架，将数据类别视为生产性但有风险的资产，将AI输入治理形式化为信息-风险权衡问题。它定义了组合层面的“信息回报”和“治理调整风险”等量化指标，并生成“治理效率前沿”。这种将治理问题量化和形式化的方法，体现了强大的理论基础和数学严谨性，与您的研究方向高度相关。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16430v1",
        "title": "Multi-Fidelity Delayed Acceptance: hierarchical MCMC sampling for Bayesian inverse problems combining multiple solvers through deep neural networks",
        "summary": "Inverse uncertainty quantification (UQ) tasks such as parameter estimation are computationally demanding whenever dealing with physics-based models, and typically require repeated evaluations of complex numerical solvers. When partial differential equations are involved, full-order models such as those based on the Finite Element Method can make traditional sampling approaches like Markov Chain Monte Carlo (MCMC) computationally infeasible. Although data-driven surrogate models may help reduce evaluation costs, their utility is often limited by the expense of generating high-fidelity data. In contrast, low-fidelity data can be produced more efficiently, although relying on them alone may degrade the accuracy of the inverse UQ solution.   To address these challenges, we propose a Multi-Fidelity Delayed Acceptance scheme for Bayesian inverse problems. Extending the Multi-Level Delayed Acceptance framework, the method introduces multi-fidelity neural networks that combine the predictions of solvers of varying fidelity, with high fidelity evaluations restricted to an offline training stage. During the online phase, likelihood evaluations are obtained by evaluating the coarse solvers and passing their outputs to the trained neural networks, thereby avoiding additional high-fidelity simulations.   This construction allows heterogeneous coarse solvers to be incorporated consistently within the hierarchy, providing greater flexibility than standard Multi-Level Delayed Acceptance. The proposed approach improves the approximation accuracy of the low fidelity solvers, leading to longer sub-chain lengths, better mixing, and accelerated posterior inference. The effectiveness of the strategy is demonstrated on two benchmark inverse problems involving (i) steady isotropic groundwater flow, (ii) an unsteady reaction-diffusion system, for which substantial computational savings are obtained.",
        "authors": "Filippo Zacchei, Paolo Conti, Attilio Alberto Frangi, Andrea Manzoni",
        "url": "http://arxiv.org/abs/2512.16430v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16430v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文提出了一个多保真度延迟接受方案，用于贝叶斯逆问题中的分层MCMC采样。它通过多保真度神经网络结合不同保真度求解器的预测，旨在加速后验推断并提高近似精度。其对MCMC采样、贝叶斯逆问题和不确定性量化的关注，以及对计算效率和精度提升的理论探讨，非常符合您对统计保证和优化收敛性的要求。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16386v1",
        "title": "Quantitative Verification of Fairness in Tree Ensembles",
        "summary": "This work focuses on quantitative verification of fairness in tree ensembles. Unlike traditional verification approaches that merely return a single counterexample when the fairness is violated, quantitative verification estimates the ratio of all counterexamples and characterizes the regions where they occur, which is important information for diagnosing and mitigating bias. To date, quantitative verification has been explored almost exclusively for deep neural networks (DNNs). Representative methods, such as DeepGemini and FairQuant, all build on the core idea of Counterexample-Guided Abstraction Refinement, a generic framework that could be adapted to other model classes. We extended the framework into a model-agnostic form, but discovered two limitations: (i) it can provide only lower bounds, and (ii) its performance scales poorly. Exploiting the discrete structure of tree ensembles, our work proposes an efficient quantification technique that delivers any-time upper and lower bounds. Experiments on five widely used datasets demonstrate its effectiveness and efficiency. When applied to fairness testing, our quantification method significantly outperforms state-of-the-art testing techniques.",
        "authors": "Zhenjiang Zhao, Takahisa Toda, Takashi Kitamura",
        "url": "http://arxiv.org/abs/2512.16386v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16386v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文专注于树集成模型中公平性的定量验证，旨在估计反例的比例并表征其发生区域，而非仅仅返回单个反例。它提出了一个高效的量化技术，能够提供任意时间的上界和下界。这种对模型公平性进行严格量化和提供理论界限的方法，体现了强大的数学严谨性，对AI系统的可信赖性至关重要。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16383v1",
        "title": "Multivariate Uncertainty Quantification with Tomographic Quantile Forests",
        "summary": "Quantifying predictive uncertainty is essential for safe and trustworthy real-world AI deployment. Yet, fully nonparametric estimation of conditional distributions remains challenging for multivariate targets. We propose Tomographic Quantile Forests (TQF), a nonparametric, uncertainty-aware, tree-based regression model for multivariate targets. TQF learns conditional quantiles of directional projections $\\mathbf{n}^{\\top}\\mathbf{y}$ as functions of the input $\\mathbf{x}$ and the unit direction $\\mathbf{n}$. At inference, it aggregates quantiles across many directions and reconstructs the multivariate conditional distribution by minimizing the sliced Wasserstein distance via an efficient alternating scheme with convex subproblems. Unlike classical directional-quantile approaches that typically produce only convex quantile regions and require training separate models for different directions, TQF covers all directions with a single model without imposing convexity restrictions. We evaluate TQF on synthetic and real-world datasets, and release the source code on GitHub.",
        "authors": "Takuya Kanazawa",
        "url": "http://arxiv.org/abs/2512.16383v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16383v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文提出了“层析分位数森林 (Tomographic Quantile Forests, TQF)”，一个用于多元目标变量的非参数、不确定性感知、基于树的回归模型。它通过学习方向投影的条件分位数，并最小化切片Wasserstein距离来重建多元条件分布。这种对非参数估计、不确定性量化和优化算法的深入研究，具有极高的统计学和数学严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16363v1",
        "title": "Empirical Likelihood Meets Prediction-Powered Inference",
        "summary": "We study inference with a small labeled sample, a large unlabeled sample, and high-quality predictions from an external model. We link prediction-powered inference with empirical likelihood by stacking supervised estimating equations based on labeled outcomes with auxiliary moment conditions built from predictions, and then optimizing empirical likelihood under these joint constraints. The resulting empirical likelihood-based prediction-powered inference (EPI) estimator is asymptotically normal, has asymptotic variance no larger than the fully supervised estimator, and attains the semiparametric efficiency bound when the auxiliary functions span the predictable component of the supervised score. For hypothesis testing and confidence sets, empirical likelihood ratio statistics admit chi-squared-type limiting distributions. As a by-product, the empirical likelihood weights induce a calibrated empirical distribution that integrates supervised and prediction-based information, enabling estimation and uncertainty quantification for general functionals beyond parameters defined by estimating equations. We present two practical implementations: one based on basis expansions in the predictions and covariates, and one that learns an approximately optimal auxiliary function by cross-fitting. In simulations and applications, EPI reduces mean squared error and shortens confidence intervals while maintaining nominal coverage.",
        "authors": "Guanghui Wang, Mengtao Wen, Changliang Zou",
        "url": "http://arxiv.org/abs/2512.16363v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16363v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "该论文将预测驱动推断与经验似然法相结合，提出了一个基于经验似然的预测驱动推断 (EPI) 估计器。摘要明确指出该估计器具有渐近正态性，渐近方差不大于完全监督估计器，并在辅助函数跨越监督得分的可预测分量时达到半参数效率界限。这些都是非常强大的统计保证和理论推导，完美符合您的偏好。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16344v1",
        "title": "AI Needs Physics More Than Physics Needs AI",
        "summary": "Artificial intelligence (AI) is commonly depicted as transformative. Yet, after more than a decade of hype, its measurable impact remains modest outside a few high-profile scientific and commercial successes. The 2024 Nobel Prizes in Chemistry and Physics recognized AI's potential, but broader assessments indicate the impact to date is often more promotional than technical. We argue that while current AI may influence physics, physics has significantly more to offer this generation of AI. Current architectures - large language models, reasoning models, and agentic AI - can depend on trillions of meaningless parameters, suffer from distributional bias, lack uncertainty quantification, provide no mechanistic insights, and fail to capture even elementary scientific laws. We review critiques of these limits, highlight opportunities in quantum AI and analogue computing, and lay down a roadmap for the adoption of 'Big AI': a synthesis of theory-based rigour with the flexibility of machine learning.",
        "authors": "Peter Coveney, Roger Highfield",
        "url": "http://arxiv.org/abs/2512.16344v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16344v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "这篇论文以批判性视角探讨了AI的局限性，并主张物理学能为AI带来“基于理论的严谨性”。它强调了当前AI缺乏不确定性量化、机制洞察和捕捉基本科学定律的能力，并提出了“大AI”的路线图，即理论严谨性与机器学习灵活性的结合。其对AI基础理论的深刻反思和对严谨性的呼唤，与您的研究理念高度一致，且行文清晰有力。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16336v1",
        "title": "Hazard-based distributional regression via ordinary differential equations",
        "summary": "The hazard function is central to the formulation of commonly used survival regression models such as the proportional hazards and accelerated failure time models. However, these models rely on a shared baseline hazard, which, when specified parametrically, can only capture limited shapes. To overcome this limitation, we propose a general class of parametric survival regression models obtained by modelling the hazard function using autonomous systems of ordinary differential equations (ODEs). Covariate information is incorporated via transformed linear predictors on the parameters of the ODE system. Our framework capitalises on the interpretability of parameters in common ODE systems, enabling the identification of covariate values that produce qualitatively distinct hazard shapes associated with different attractors of the system of ODEs. This provides deeper insights into how covariates influence survival dynamics. We develop efficient Bayesian computational tools, including parallelised evaluation of the log-posterior, which facilitates integration with general-purpose Markov Chain Monte Carlo samplers. We also derive conditions for posterior asymptotic normality, enabling fast approximations of the posterior. A central contribution of our work lies in the case studies. We demonstrate the methodology using clinical trial data with crossing survival curves, and a study of cancer recurrence times where our approach reveals how the efficacy of interventions (treatments) on hazard and survival are influenced by patient characteristics.",
        "authors": "J. A. Christen, F. J. Rubio",
        "url": "http://arxiv.org/abs/2512.16336v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16336v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文提出了一类新的参数生存回归模型，通过使用常微分方程 (ODEs) 建模风险函数。它利用ODE系统参数的可解释性，实现了对协变量如何影响生存动态的更深层次洞察，并推导了后验渐近正态性的条件。这种将ODE系统引入生存分析，并提供贝叶斯计算工具和渐近理论的方法，具有极高的数学和统计严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16231v1",
        "title": "An Efficient Framework for Robust Sample Size Determination",
        "summary": "In many settings, robust data analysis involves computational methods for uncertainty quantification and statistical inference. To design frequentist studies that leverage robust analysis methods, suitable sample sizes to achieve desired power are often found by estimating sampling distributions of p-values via intensive simulation. Moreover, most sample size recommendations rely heavily on assumptions about a single data-generating process. Consequently, robustness in data analysis does not by itself imply robustness in study design, as examining sample size sensitivity to data-generating assumptions typically requires further simulations. We propose an economical alternative for determining sample sizes that are robust to multiple data-generating mechanisms. Applying our theoretical results that model p-values as a function of the sample size, we assess power across the sample size space using simulations conducted at only two sample sizes for each data-generating mechanism. We demonstrate the broad applicability of our methodology to study design based on M-estimators in both experimental and observational settings through a varied set of clinical examples.",
        "authors": "Luke Hagar, Andrew J. Martin",
        "url": "http://arxiv.org/abs/2512.16231v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16231v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "这篇论文提出了一个高效的框架，用于稳健的样本量确定。它利用理论结果将p值建模为样本量的函数，从而在仅两个样本量下进行模拟即可评估不同数据生成机制下的功效。其对统计推断、不确定性量化和M-估计器的应用，以及对理论结果的强调，是您会非常感兴趣的严谨统计学工作。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16227v1",
        "title": "An Information-Theoretic Framework for Robust Large Language Model Editing",
        "summary": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.",
        "authors": "Qizhou Chen, Chengyu Wang, Taolin Zhang, Xiaofeng He",
        "url": "http://arxiv.org/abs/2512.16227v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16227v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 4
        },
        "reason_zh": "该论文引入了一个新颖的LLM编辑框架，其“根植于信息瓶颈理论”。这种方法旨在精确压缩和隔离知识修正所需的基本信息，同时最大限度地减少对无关模型行为的干扰。摘要中明确提及“信息论框架”和“信息瓶颈理论”，表明了其强大的理论基础和数学推导，对LLM的可信赖性具有重要意义。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16185v1",
        "title": "Weighted K-Harmonic Means Clustering: Convergence Analysis and Applications to Wireless Communications",
        "summary": "We propose the \\emph{weighted K-harmonic means} (WKHM) clustering algorithm, a regularized variant of K-harmonic means designed to ensure numerical stability while enabling soft assignments through inverse-distance weighting. Unlike classical K-means and constrained K-means, WKHM admits a direct interpretation in wireless networks: its weights are exactly equivalent to fractional user association based on received signal strength. We establish rigorous convergence guarantees under both deterministic and stochastic settings, addressing key technical challenges arising from non-convexity and random initialization. Specifically, we prove monotone descent to a local minimum under fixed initialization, convergence in probability under Binomial Point Process (BPP) initialization, and almost sure convergence under mild decay conditions. These results provide the first stochastic convergence guarantees for harmonic-mean-based clustering. Finally, through extensive simulations with diverse user distributions, we show that WKHM achieves a superior tradeoff between minimum signal strength and load fairness compared to classical and modern clustering baselines, making it a principled tool for joint radio node placement and user association in wireless networks.",
        "authors": "Gourab Ghatak",
        "url": "http://arxiv.org/abs/2512.16185v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16185v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文提出了“加权K-调和均值 (WKHM)”聚类算法，并建立了在确定性和随机设置下的“严格收敛性保证”，包括单调下降到局部最小值、概率收敛和几乎必然收敛。这些是聚类算法中非常重要的理论保证，直接符合您对优化收敛性和数学严谨性的要求。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16141v1",
        "title": "Existence of Solutions for Non-monotone VIs and Implications for Games",
        "summary": "In this paper, we study the existence of solutions in non-monotone variational inequalities (VIs) through the normal mapping properties. In particular, we show that when the normal mapping $F_K^{\\rm nor}(\\cdot)$ is norm coercive over a set $K$, and the generalized Jacobian of the normal mapping has a full rank at points $x$ where $F_K^{\\rm nor}(x)\\ne0$, then the VI$(K,F)$ has a solution. We then investigate conditions on the mapping $F(\\cdot)$ and its Jacobian that imply the full rank condition for the generalized Jacobian, such as the uniform P-function and the uniform P-matrix condition. Subsequently, we focus on VIs arising from games and interpret our main result in a game setting. Based on the P$_Υ$-matrix condition, we provide a sufficient condition for a game to have a Nash equilibrium. Additionally, through examples we show that our sufficient conditions can be used to assert the existence of a solution to a VI, or a quasi-Nash in a game, while the existing results relying on the uniform P-function property or the P$_Υ$-matrix condition cannot be employed.",
        "authors": "Sina Arefizadeh, Angelia Nedić",
        "url": "http://arxiv.org/abs/2512.16141v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16141v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "该论文通过法线映射性质研究非单调变分不等式 (VIs) 解的存在性，并将其结果应用于博弈论，为博弈存在纳什均衡提供了充分条件。这是一篇纯粹的数学理论论文，专注于存在性证明和严格的数学推导，完美符合您对严谨数学逻辑的偏好。"
    },
    {
        "id": "http://arxiv.org/abs/2512.16046v1",
        "title": "CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting",
        "summary": "Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.",
        "authors": "Shu Wan, Reepal Shah, John Sabo, Huan Liu, K. Selçuk Candan",
        "url": "http://arxiv.org/abs/2512.16046v1",
        "pdf_url": "https://arxiv.org/pdf/2512.16046v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文提出了一个用于因果时空径流预测的统一框架 (CauSTream)。它共同学习气象强迫之间的径流因果图和站点间的动态依赖路由图，并“在非参数设置下建立了这些因果结构的可识别性条件”。这种对因果学习、可识别性条件和非参数设置的深入研究，具有极高的统计学和因果逻辑严谨性，是您会非常感兴趣的论文。"
    }
]