[
    {
        "id": "http://arxiv.org/abs/2601.08574v1",
        "title": "Intersectional Data and the Social Cost of Digital Extraction: A Pigouvian Surcharge",
        "summary": "Contemporary digital capitalism relies on the large-scale extraction and commodification of personal data. Far from revealing isolated attributes, such data increasingly exposes intersectional social identities formed by combinations of race, gender, disability and others. This process generates a structural privacy externality: while firms appropriate economic value through profiling, prediction, and personalization, individuals and social groups bear diffuse costs in the form of heightened social risk, discrimination, and vulnerability. This paper develops a formal political economic framework to internalize these externalities by linking data valuation to information-theoretic measures. We propose a pricing rule based on mutual information that assigns monetary value to the entropy reduction induced by individual data points over joint intersectional identity distributions. Interpreted as a Pigouvian-style surcharge on data extraction, this mechanism functions as an institutional constraint on the asymmetric accumulation of informational power. A key advantage of the approach is its model-agnostic character: the valuation rule operates independently of the statistical structure used to estimate intersectional attributes, whether parametric, nonparametric, or machine-learned, and can be approximated through discretization of joint distributions. We argue that regulators can calibrate this surcharge to reflect contested social values, thereby embedding normative judgments directly into market design. By formalizing the social cost of intersectional data extraction, the proposed mechanism offers both a corrective to market failure and a redistributive institutional shield for vulnerable groups under conditions of digital asymmetry.",
        "authors": "Eduardo C. Garrido-Merchán",
        "url": "http://arxiv.org/abs/2601.08574v1",
        "pdf_url": "https://arxiv.org/pdf/2601.08574v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "本文提出了一个新颖的政治经济学框架，通过信息论度量（特别是互信息）来量化交叉性数据的社会成本，并引入皮古税式附加费来内化隐私外部性。其“形式化的政治经济学框架”、“信息论度量”和“基于互信息的定价规则”等内容，完美契合了您对强大理论基础和清晰数学推导的偏好。"
    },
    {
        "id": "http://arxiv.org/abs/2601.08149v1",
        "title": "Dynamic Graph Structure Learning via Resistance Curvature Flow",
        "summary": "Geometric Representation Learning (GRL) aims to approximate the non-Euclidean topology of high-dimensional data through discrete graph structures, grounded in the manifold hypothesis. However, traditional static graph construction methods based on Euclidean distance often fail to capture the intrinsic curvature characteristics of the data manifold. Although Ollivier-Ricci Curvature Flow (OCF) has proven to be a powerful tool for dynamic topological optimization, its core reliance on Optimal Transport (Wasserstein distance) leads to prohibitive computational complexity, severely limiting its application in large-scale datasets and deep learning frameworks. To break this bottleneck, this paper proposes a novel geometric evolution framework: Resistance Curvature Flow (RCF). Leveraging the concept of effective resistance from circuit physics, RCF transforms expensive curvature optimization into efficient matrix operations. This approach achieves over 100x computational acceleration while maintaining geometric optimization capabilities comparable to OCF. We provide an in-depth exploration of the theoretical foundations and dynamical principles of RCF, elucidating how it guides the redistribution of edge weights via curvature gradients to eliminate topological noise and strengthen local cluster structures. Furthermore, we provide a mechanistic explanation of RCF's role in manifold enhancement and noise suppression, as well as its compatibility with deep learning models. We design a graph optimization algorithm, DGSL-RCF, based on this framework. Experimental results across deep metric learning, manifold learning, and graph structure learning demonstrate that DGSL-RCF significantly improves representation quality and downstream task performance.",
        "authors": "Chaoqun Fei, Huanjiang Liu, Tinglve Zhou, Yangyang Li, Tianyong Hao",
        "url": "http://arxiv.org/abs/2601.08149v1",
        "pdf_url": "https://arxiv.org/pdf/2601.08149v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文提出了电阻曲率流（RCF）这一新颖的几何演化框架，用于动态图结构学习。它利用电路物理中的有效电阻概念，将昂贵的曲率优化转化为高效的矩阵运算，并提供了RCF理论基础和动力学原理的深入探讨。其对“非欧几何拓扑”、“曲率优化”、“理论基础和动力学原理”的数学严谨分析，与您的研究兴趣高度吻合。"
    },
    {
        "id": "http://arxiv.org/abs/2601.08705v1",
        "title": "RMBRec: Robust Multi-Behavior Recommendation towards Target Behaviors",
        "summary": "Multi-behavior recommendation faces a critical challenge in practice: auxiliary behaviors (e.g., clicks, carts) are often noisy, weakly correlated, or semantically misaligned with the target behavior (e.g., purchase), which leads to biased preference learning and suboptimal performance. While existing methods attempt to fuse these heterogeneous signals, they inherently lack a principled mechanism to ensure robustness against such behavioral inconsistency.   In this work, we propose Robust Multi-Behavior Recommendation towards Target Behaviors (RMBRec), a robust multi-behavior recommendation framework grounded in an information-theoretic robustness principle. We interpret robustness as a joint process of maximizing predictive information while minimizing its variance across heterogeneous behavioral environments. Under this perspective, the Representation Robustness Module (RRM) enhances local semantic consistency by maximizing the mutual information between users' auxiliary and target representations, whereas the Optimization Robustness Module (ORM) enforces global stability by minimizing the variance of predictive risks across behaviors, which is an efficient approximation to invariant risk minimization. This local-global collaboration bridges representation purification and optimization invariance in a theoretically coherent way. Extensive experiments on three real-world datasets demonstrate that RMBRec not only outperforms state-of-the-art methods in accuracy but also maintains remarkable stability under various noise perturbations. For reproducibility, our code is available at https://github.com/miaomiao-cai2/RMBRec/.",
        "authors": "Miaomiao Cai, Zhijie Zhang, Junfeng Fang, Zhiyong Cheng, Xiang Wang, Meng Wang",
        "url": "http://arxiv.org/abs/2601.08705v1",
        "pdf_url": "https://arxiv.org/pdf/2601.08705v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "本文提出了一个基于信息论鲁棒性原则的鲁棒多行为推荐框架RMBRec。它通过最大化用户辅助行为与目标行为表示之间的互信息，并最小化预测风险的方差来确保鲁棒性。其“信息论鲁棒性原则”、“互信息最大化”、“不变风险最小化”等理论基础，展现了极高的数学和统计严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.08643v1",
        "title": "Automatic debiased machine learning and sensitivity analysis for sample selection models",
        "summary": "In this paper, we extend the Riesz representation framework to causal inference under sample selection, where both treatment assignment and outcome observability are non-random. Formulating the problem in terms of a Riesz representer enables stable estimation and a transparent decomposition of omitted variable bias into three interpretable components: a data-identified scale factor, outcome confounding strength, and selection confounding strength. For estimation, we employ the ForestRiesz estimator, which accounts for selective outcome observability while avoiding the instability associated with direct propensity score inversion. We assess finite-sample performance through a simulation study and show that conventional double machine learning approaches can be highly sensitive to tuning parameters due to their reliance on inverse probability weighting, whereas the ForestRiesz estimator delivers more stable performance by leveraging automatic debiased machine learning. In an empirical application to the gender wage gap in the U.S., we find that our ForestRiesz approach yields larger treatment effect estimates than a standard double machine learning approach, suggesting that ignoring sample selection leads to an underestimation of the gender wage gap. Sensitivity analysis indicates that implausibly strong unobserved confounding would be required to overturn our results. Overall, our approach provides a unified, robust, and computationally attractive framework for causal inference under sample selection.",
        "authors": "Jakob Bjelac, Victor Chernozhukov, Phil-Adrian Klotz, Jannis Kueck, Theresa M. A. Schmitz",
        "url": "http://arxiv.org/abs/2601.08643v1",
        "pdf_url": "https://arxiv.org/pdf/2601.08643v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文将Riesz表示框架扩展到样本选择下的因果推断问题，并提出了ForestRiesz估计器。它透明地分解了遗漏变量偏差，并进行了敏感性分析。其“Riesz表示框架”、“因果推断”、“去偏机器学习”和“敏感性分析”等内容，提供了强大的统计保证和清晰的数学推导。"
    },
    {
        "id": "http://arxiv.org/abs/2601.08421v1",
        "title": "Coverage Improvement and Fast Convergence of On-policy Preference Learning",
        "summary": "Online on-policy preference learning algorithms for language model alignment such as online direct policy optimization (DPO) can significantly outperform their offline counterparts. We provide a theoretical explanation for this phenomenon by analyzing how the sampling policy's coverage evolves throughout on-policy training. We propose and rigorously justify the \\emph{coverage improvement principle}: with sufficient batch size, each update moves into a region around the target where coverage is uniformly better, making subsequent data increasingly informative and enabling rapid convergence. In the contextual bandit setting with Bradley-Terry preferences and linear softmax policy class, we show that on-policy DPO converges exponentially in the number of iterations for batch size exceeding a generalized coverage threshold. In contrast, any learner restricted to offline samples from the initial policy suffers a slower minimax rate, leading to a sharp separation in total sample complexity. Motivated by this analysis, we further propose a simple hybrid sampler based on a novel \\emph{preferential} G-optimal design, which removes dependence on coverage and guarantees convergence in just two rounds. Finally, we develop principled on-policy schemes for reward distillation in the general function class setting, and show faster noiseless rates under an alternative deviation-based notion of coverage. Experimentally, we confirm that on-policy DPO and our proposed reward distillation algorithms outperform their off-policy counterparts and enjoy stable, monotonic performance gains across iterations.",
        "authors": "Juno Kim, Jihun Yun, Jason D. Lee, Kwang-Sung Jun",
        "url": "http://arxiv.org/abs/2601.08421v1",
        "pdf_url": "https://arxiv.org/pdf/2601.08421v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "本文为在线策略偏好学习算法提供了理论解释，提出了“覆盖改进原则”，并严格证明了在特定条件下策略DPO的指数收敛性，以及与离线方法的样本复杂度差异。其“理论解释”、“指数收敛”、“最小最大率”和“样本复杂度分析”等，是您所寻求的严谨数学逻辑和优化收敛性的典范。"
    },
    {
        "id": "http://arxiv.org/abs/2601.08122v1",
        "title": "Generalization Analysis and Method for Domain Generalization for a Family of Recurrent Neural Networks",
        "summary": "Deep learning (DL) has driven broad advances across scientific and engineering domains. Despite its success, DL models often exhibit limited interpretability and generalization, which can undermine trust, especially in safety-critical deployments. As a result, there is growing interest in (i) analyzing interpretability and generalization and (ii) developing models that perform robustly under data distributions different from those seen during training (i.e. domain generalization). However, the theoretical analysis of DL remains incomplete. For example, many generalization analyses assume independent samples, which is violated in sequential data with temporal correlations. Motivated by these limitations, this paper proposes a method to analyze interpretability and out-of-domain (OOD) generalization for a family of recurrent neural networks (RNNs). Specifically, the evolution of a trained RNN's states is modeled as an unknown, discrete-time, nonlinear closed-loop feedback system. Using Koopman operator theory, these nonlinear dynamics are approximated with a linear operator, enabling interpretability. Spectral analysis is then used to quantify the worst-case impact of domain shifts on the generalization error. Building on this analysis, a domain generalization method is proposed that reduces the OOD generalization error and improves the robustness to distribution shifts. Finally, the proposed analysis and domain generalization approach are validated on practical temporal pattern-learning tasks.",
        "authors": "Atefeh Termehchi, Ekram Hossain, Isaac Woungang",
        "url": "http://arxiv.org/abs/2601.08122v1",
        "pdf_url": "https://arxiv.org/pdf/2601.08122v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "该论文利用Koopman算子理论分析了循环神经网络（RNNs）的泛化行为和域泛化问题，并通过谱分析量化了域偏移的最坏情况影响。其“Koopman算子理论”、“谱分析”、“量化最坏情况影响”等，提供了对深度学习模型泛化能力的深刻理论分析和数学推导。"
    },
    {
        "id": "http://arxiv.org/abs/2601.08116v1",
        "title": "Learning a Stochastic Differential Equation Model of Tropical Cyclone Intensification from Reanalysis and Observational Data",
        "summary": "Tropical cyclones are dangerous natural hazards, but their hazard is challenging to quantify directly from historical datasets due to limited dataset size and quality. Models of cyclone intensification fill this data gap by simulating huge ensembles of synthetic hurricanes based on estimates of the storm's large scale environment. Both physics-based and statistical/ML intensification models have been developed to tackle this problem, but an open question is: can a physically reasonable and simple physics-style differential equation model of intensification be learned from data? In this paper, we answer this question in the affirmative by presenting a 10-term cubic stochastic differential equation model of Tropical Cyclone intensification. The model depends on a well-vetted suite of engineered environmental features known to drive intensification and is trained using a high quality dataset of hurricane intensity (IBTrACS) with estimates of the cyclone's large scale environment from a data-assimilated simulation (ERA5 reanalysis), restricted to the Northern Hemisphere. The model generates synthetic intensity series which capture many aspects of historical intensification statistics and hazard estimates in the Northern Hemisphere. Our results show promise that interpretable, physics style models of complex earth system dynamics can be learned using automated system identification techniques.",
        "authors": "Kenneth Gee, Sai Ravela",
        "url": "http://arxiv.org/abs/2601.08116v1",
        "pdf_url": "https://arxiv.org/pdf/2601.08116v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "本文提出了一个通过数据学习热带气旋增强的10项三次随机微分方程（SDE）模型。它展示了如何从数据中学习可解释的、物理风格的复杂地球系统动力学模型。其“随机微分方程模型”、“系统识别技术”以及对物理系统进行严谨统计建模的方法，与您的兴趣高度契合。"
    },
    {
        "id": "http://arxiv.org/abs/2601.08100v1",
        "title": "Towards A Unified PAC-Bayesian Framework for Norm-based Generalization Bounds",
        "summary": "Understanding the generalization behavior of deep neural networks remains a fundamental challenge in modern statistical learning theory. Among existing approaches, PAC-Bayesian norm-based bounds have demonstrated particular promise due to their data-dependent nature and their ability to capture algorithmic and geometric properties of learned models. However, most existing results rely on isotropic Gaussian posteriors, heavy use of spectral-norm concentration for weight perturbations, and largely architecture-agnostic analyses, which together limit both the tightness and practical relevance of the resulting bounds. To address these limitations, in this work, we propose a unified framework for PAC-Bayesian norm-based generalization by reformulating the derivation of generalization bounds as a stochastic optimization problem over anisotropic Gaussian posteriors. The key to our approach is a sensitivity matrix that quantifies the network outputs with respect to structured weight perturbations, enabling the explicit incorporation of heterogeneous parameter sensitivities and architectural structures. By imposing different structural assumptions on this sensitivity matrix, we derive a family of generalization bounds that recover several existing PAC-Bayesian results as special cases, while yielding bounds that are comparable to or tighter than state-of-the-art approaches. Such a unified framework provides a principled and flexible way for geometry-/structure-aware and interpretable generalization analysis in deep learning.",
        "authors": "Xinping Yi, Gaojie Jin, Xiaowei Huang, Shi Jin",
        "url": "http://arxiv.org/abs/2601.08100v1",
        "pdf_url": "https://arxiv.org/pdf/2601.08100v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "该论文提出了一个统一的PAC-贝叶斯框架，用于基于范数的泛化界限，通过将泛化界限的推导重新表述为随机优化问题，并引入敏感性矩阵。其对“PAC-贝叶斯泛化界限”、“随机优化问题”、“各向异性高斯后验”和“敏感性矩阵”的深入数学分析，提供了严谨的统计保证和理论基础。"
    },
    {
        "id": "http://arxiv.org/abs/2601.08784v1",
        "title": "On the use of graph models to achieve individual and group fairness",
        "summary": "Machine Learning algorithms are ubiquitous in key decision-making contexts such as justice, healthcare and finance, which has spawned a great demand for fairness in these procedures. However, the theoretical properties of such models in relation with fairness are still poorly understood, and the intuition behind the relationship between group and individual fairness is still lacking. In this paper, we provide a theoretical framework based on Sheaf Diffusion to leverage tools based on dynamical systems and homology to model fairness. Concretely, the proposed method projects input data into a bias-free space that encodes fairness constrains, resulting in fair solutions. Furthermore, we present a collection of network topologies handling different fairness metrics, leading to a unified method capable of dealing with both individual and group bias. The resulting models have a layer of interpretability in the form of closed-form expressions for their SHAP values, consolidating their place in the responsible Artificial Intelligence landscape. Finally, these intuitions are tested on a simulation study and standard fairness benchmarks, where the proposed methods achieve satisfactory results. More concretely, the paper showcases the performance of the proposed models in terms of accuracy and fairness, studying available trade-offs on the Pareto frontier, checking the effects of changing the different hyper-parameters, and delving into the interpretation of its outputs.",
        "authors": "Arturo Pérez-Peralta, Sandra Benítez-Peña, Rosa E. Lillo",
        "url": "http://arxiv.org/abs/2601.08784v1",
        "pdf_url": "https://arxiv.org/pdf/2601.08784v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "本文提出了一个基于Sheaf Diffusion的理论框架，利用动力系统和同调工具来建模公平性，并将输入数据投影到一个编码公平性约束的无偏空间。其“理论框架”、“Sheaf Diffusion”、“动力系统”、“同调”和“SHAP值的闭式表达式”等，提供了强大的数学和因果逻辑基础。"
    },
    {
        "id": "http://arxiv.org/abs/2601.08700v1",
        "title": "Novel Dynamical Systems with Finite-Time and Fixed-Time Stability for Generalized Inverse Mixed Variational Inequality Problems",
        "summary": "This paper investigates a class of generalized inverse mixed variational inequality problems (GIMVIPs), which consist in finding a vector $\\overline{w}\\in \\R^d$ such that \\[ F(\\bar w)\\in Ω\\quad \\text{and} \\quad \\langle h(\\bar w), v-F(\\bar w) \\rangle + g(v)-g(F(\\bar w)) \\ge 0, \\quad \\forall v\\in Ω, \\] where \\(h,F:\\R^d\\to\\R^d\\) are single-valued operators, \\(g:Ω\\to\\R\\cup\\{+\\infty\\}\\) is a proper function, and \\(Ω\\) is a closed convex set.   Two novel continuous-time dynamical systems are proposed to analyze the finite-time and fixed-time stability of solutions to GIMVIPs in finite-dimensional Hilbert spaces. Under suitable assumptions on the operators and model parameters, Lyapunov-based techniques are employed to establish finite-time and fixed-time convergence of the generated trajectories.   While both systems exhibit accelerated convergence, the settling time of the finite-time stable system depends on the initial condition, whereas the fixed-time stable system admits a uniform upper bound on the convergence time that is independent of the initial state. Moreover, an explicit forward Euler discretization of the continuous-time dynamics leads to a proximal point-type algorithm that preserves the fixed-time convergence property. Rigorous convergence analysis of the resulting iterative scheme is provided. A numerical experiment is presented to demonstrate the effectiveness of the proposed methods.",
        "authors": "Nam Van Tran",
        "url": "http://arxiv.org/abs/2601.08700v1",
        "pdf_url": "https://arxiv.org/pdf/2601.08700v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 2,
            "Clarity": 4
        },
        "reason_zh": "该论文提出了两种新颖的连续时间动力系统，用于分析广义逆混合变分不等式问题的有限时间稳定性和固定时间稳定性，并利用Lyapunov技术建立了收敛性。其“动力系统”、“有限时间/固定时间稳定性”、“Lyapunov技术”和“严格收敛性分析”等，是纯粹的理论优化和数学推导工作。"
    },
    {
        "id": "http://arxiv.org/abs/2601.08600v1",
        "title": "Flexible modeling of nonnegative continuous data: Box-Cox symmetric regression and its zero-adjusted extension",
        "summary": "The Box-Cox symmetric distributions constitute a broad class of probability models for positive continuous data, offering flexibility in modeling skewness and tail behavior. Their parameterization allows a straightforward quantile-based interpretation, which is particularly useful in regression modeling. Despite their potential, only a few specific distributions within this class have been explored in regression contexts, and zero-adjusted extensions have not yet been formally addressed in the literature. This paper formalizes the class of Box-Cox symmetric regression models and introduces a new zero-adjusted extension suitable for modeling data with a non-negligible proportion of observations equal to zero. We discuss maximum likelihood estimation, assess finite-sample performance through simulations, and develop diagnostic tools including residual analysis, local influence measures, and goodness-of-fit statistics. An empirical application on basic education expenditure illustrates the models' ability to capture complex patterns in zero-inflated and highly skewed nonnegative data. To support practical use, we developed the new BCSreg R package, which implements all proposed methods.",
        "authors": "Rodrigo M. R. de Medeiros, Francisco F. Queiroz",
        "url": "http://arxiv.org/abs/2601.08600v1",
        "pdf_url": "https://arxiv.org/pdf/2601.08600v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "本文形式化了Box-Cox对称回归模型，并引入了新的零调整扩展，适用于建模具有零观测值的非负连续数据。它讨论了最大似然估计、有限样本性能评估和诊断工具。其对“概率模型”、“最大似然估计”、“有限样本性能”和“诊断工具”的严谨统计学处理，与您的统计学背景高度相关。"
    },
    {
        "id": "http://arxiv.org/abs/2601.08547v1",
        "title": "Convergence of gradient flow for learning convolutional neural networks",
        "summary": "Convolutional neural networks are widely used in imaging and image recognition. Learning such networks from training data leads to the minimization of a non-convex function. This makes the analysis of standard optimization methods such as variants of (stochastic) gradient descent challenging. In this article we study the simplified setting of linear convolutional networks. We show that the gradient flow (to be interpreted as an abstraction of gradient descent) applied to the empirical risk defined via certain loss functions including the square loss always converges to a critical point, under a mild condition on the training data.",
        "authors": "Jona-Maria Diederen, Holger Rauhut, Ulrich Terstiege",
        "url": "http://arxiv.org/abs/2601.08547v1",
        "pdf_url": "https://arxiv.org/pdf/2601.08547v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 2,
            "Clarity": 4
        },
        "reason_zh": "该论文研究了线性卷积网络学习中梯度流的收敛性，证明了在特定损失函数下，梯度流总是收敛到临界点。其对“梯度流”、“非凸函数最小化”和“收敛性分析”的数学严谨性，是您所寻求的优化收敛性理论研究。"
    },
    {
        "id": "http://arxiv.org/abs/2601.08362v1",
        "title": "Stratification for Nonlinear Semidefinite Programming",
        "summary": "This paper introduces a stratification framework for nonlinear semidefinite programming (NLSDP) that reveals and utilizes the geometry behind the nonsmooth KKT system. Based on the \\emph{index stratification} of $\\mathbb{S}^n$ and its lift to the primal--dual space, a stratified variational analysis is developed. Specifically, we define the stratum-restricted regularity property, characterize it by the verifiable weak second order condition (W-SOC) and weak strict Robinson constraint qualification (W-SRCQ), and interpret the W-SRCQ geometrically via transversality, which provides its genericity over ambient space and stability along strata. The interactions of these properties across neighboring strata are further examined, leading to the conclusion that classical strong-form regularity conditions correspond to the local uniform validity of stratum-restricted counterparts. On the algorithmic side, a stratified Gauss--Newton method with normal steps and a correction mechanism is proposed for globally solving the KKT equation through a least-squares merit function. We demonstrate that the algorithm converges globally to directional stationary points. Moreover, under the W-SOC and the strict Robinson constraint qualification (SRCQ), it achieves local quadratic convergence to KKT pairs and eventually identifies the active stratum.",
        "authors": "Chenglong Bao, Chao Ding, Fuxiaoyue Feng, Jingyu Li",
        "url": "http://arxiv.org/abs/2601.08362v1",
        "pdf_url": "https://arxiv.org/pdf/2601.08362v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 2,
            "Clarity": 4
        },
        "reason_zh": "本文为非线性半定规划（NLSDP）引入了一个分层框架，揭示并利用了非光滑KKT系统背后的几何结构，并提出了一个分层高斯-牛顿方法。其对“分层框架”、“非光滑KKT系统”、“变分分析”、“二阶条件”和“二次收敛性”的深入数学分析，是纯粹的理论优化工作。"
    },
    {
        "id": "http://arxiv.org/abs/2601.08556v1",
        "title": "EviNAM: Intelligibility and Uncertainty via Evidential Neural Additive Models",
        "summary": "Intelligibility and accurate uncertainty estimation are crucial for reliable decision-making. In this paper, we propose EviNAM, an extension of evidential learning that integrates the interpretability of Neural Additive Models (NAMs) with principled uncertainty estimation. Unlike standard Bayesian neural networks and previous evidential methods, EviNAM enables, in a single pass, both the estimation of the aleatoric and epistemic uncertainty as well as explicit feature contributions. Experiments on synthetic and real data demonstrate that EviNAM matches state-of-the-art predictive performance. While we focus on regression, our method extends naturally to classification and generalized additive models, offering a path toward more intelligible and trustworthy predictions.",
        "authors": "Sören Schleibaum, Anton Frederik Thielmann, Julian Teusch, Benjamin Säfken, Jörg P. Müller",
        "url": "http://arxiv.org/abs/2601.08556v1",
        "pdf_url": "https://arxiv.org/pdf/2601.08556v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "本文提出了EviNAM，一个结合了神经加性模型（NAMs）可解释性和证据学习原理不确定性估计的框架。它能够在单次前向传播中同时估计偶然不确定性和认知不确定性，并提供明确的特征贡献。其“证据学习”、“原理不确定性估计”和“明确特征贡献”等，提供了坚实的统计学和可解释性理论基础。"
    },
    {
        "id": "http://arxiv.org/abs/2601.08441v1",
        "title": "YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation",
        "summary": "Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a \\textit{reference-free} method that learns \\textit{sparse steering vectors} in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly available\\footnote{https://github.com/MBZUAI-Paris/YaPO}.",
        "authors": "Abdelaziz Bounhar, Rania Hossam Elmohamady Elbadry, Hadi Abdine, Preslav Nakov, Michalis Vazirgiannis, Guokan Shang",
        "url": "http://arxiv.org/abs/2601.08441v1",
        "pdf_url": "https://arxiv.org/pdf/2601.08441v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文提出了YaPO，一种无参考方法，用于在稀疏自编码器的潜在空间中学习稀疏转向向量，以实现LLM的领域适应。它通过优化稀疏代码，生成解耦、可解释且高效的转向方向。其“策略优化”、“稀疏自编码器”和“解耦、可解释的转向方向”等，在优化和表示学习方面具有较强的理论严谨性。"
    }
]