import os
import json
import arxiv
import logging 
import re
import time
import random
from google import genai
from google.genai import types
from datetime import date, timedelta

# --- 0. ä¾èµ–æ£€æŸ¥ ---
try:
    import json5 
except ImportError:
    import json as json5

# --- 1. é…ç½® Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- 2. æ ¸å¿ƒé…ç½® ---
GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
ARCHIVE_DIR = "archive"

ARXIV_CATEGORIES = ['stat.ML', 'cs.LG', 'math.OC', 'cs.NE', 'cs.AI', 'math.NA']
TUTORIAL_KEYWORDS = ['tutorial', 'survey', '"lecture notes"', 'review', '"book chapter"']

# --- 3. æŠ“å–å‡½æ•° (ä¿æŒ V19 æ ‡å‡†) ---
def fetch_weekly_tutorials(target_date):
    logger.info(f"--- æ­£åœ¨ä¸º {target_date} æ‰€åœ¨å‘¨æŠ“å–æ•™ç¨‹ (éé‡‘è) ---")
    
    one_week_ago = target_date - timedelta(days=6)
    start_str = one_week_ago.strftime("%Y%m%d")
    end_str = target_date.strftime("%Y%m%d")
    date_filter = f"submittedDate:[{start_str}0000 TO {end_str}2359]"
    
    category_query = " OR ".join([f"cat:{cat}" for cat in ARXIV_CATEGORIES])
    keyword_query = " OR ".join([f'(ti:{kw} OR abs:{kw})' for kw in TUTORIAL_KEYWORDS])
    full_query = f"({category_query}) AND ({keyword_query}) AND {date_filter}"
    
    search = arxiv.Search(
        query=full_query,
        max_results=80, #ç¨å¾®å¢åŠ ä¸€ç‚¹æŠ“å–é‡ä»¥ç¡®ä¿æœ‰è¶³å¤Ÿçš„åŸºç¡€å†…å®¹
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending
    )
    
    papers_list = []
    try:
        client = arxiv.Client()
        for result in client.results(search):
            papers_list.append({
                'id': result.entry_id,
                'title': result.title,
                'summary': result.summary.replace("\n", " "),
                'authors': ", ".join([a.name for a in result.authors]),
                'url': result.entry_id,
                'pdf_url': result.pdf_url
            })
        logger.info(f"æœ¬å‘¨å…±æŠ“å–åˆ° {len(papers_list)} ç¯‡æ•™ç¨‹/ç»¼è¿°ã€‚")
        return papers_list
    except Exception as e:
        logger.error(f"æŠ“å–æ•™ç¨‹å¤±è´¥: {e}")
        return []

# --- 4. AI æ•™ç¨‹æ€»ç¼–è¾‘ (V20 - æ··åˆç­–ç•¥ 3+3) ---
def get_ai_tutorial_pick(papers, user_preference_prompt):
    if not papers:
        logger.info("æ²¡æœ‰è®ºæ–‡å¯ä¾› AI åˆ†æã€‚")
        return None
    if not GEMINI_API_KEY:
        logger.error("æœªæ‰¾åˆ° GEMINI_API_KEYã€‚")
        return None

    client = genai.Client()
    prompt_papers = "\n".join([
        f"--- æ•™ç¨‹ {i+1} ---\nID: {p['id']}\næ ‡é¢˜: {p['title']}\næ‘˜è¦: {p['summary']}\n"
        for i, p in enumerate(papers)
    ])

    system_prompt = f"""
    ä½ æ˜¯æˆ‘ï¼ˆç»Ÿè®¡å­¦ç¡•å£«ï¼‰çš„ç§äººç ”ç©¶åŠ©æ‰‹ï¼Œä¸€ä¸ªâ€œAI æ€»ç¼–è¾‘â€ã€‚
    æˆ‘ä»Šå¤©çš„ä»»åŠ¡æ˜¯åˆ†æ "æœ¬å‘¨æ•™ç¨‹ä¸ç»¼è¿°" é¢†åŸŸã€‚
    ä¸ªäººåå¥½ï¼š"{user_preference_prompt}"
    
    ä¸‹é¢æ˜¯ {len(papers)} ç¯‡æ•™ç¨‹ã€‚
    
    **ä»»åŠ¡ï¼šè¯·ä¸ºæˆ‘ç²¾é€‰ Top 10 ç¯‡æ•™ç¨‹ï¼Œå¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹â€œæ··åˆé…æ¯”â€ï¼š**
    
    1.  **å‰æ²¿/æ·±åº¦ç±» (5 ç¯‡):** é’ˆå¯¹é«˜ç»´ç»Ÿè®¡ã€RLã€LLM ç­‰é¢†åŸŸçš„æœ€æ–°ã€æœ€æ·±å…¥çš„ç»¼è¿°ã€‚è¦æ±‚ç†è®ºæ·±åº¦é«˜ã€‚
    2.  **åŸºç¡€/å…¥é—¨ç±» (5 ç¯‡):** é’ˆå¯¹æŸä¸€å…·ä½“æ•°å­¦æ¦‚å¿µæˆ–ç®—æ³•çš„â€œå…¥é—¨æ•™ç¨‹ (Tutorial)â€æˆ–â€œè®²ä¹‰ (Lecture Notes)â€ã€‚è¦æ±‚é€»è¾‘æå…¶æ¸…æ™°ï¼Œé€‚åˆå¤¯å®åŸºç¡€ã€‚
    
    **è¯„åˆ†æ ‡å‡† (1-5åˆ†):**
    - Novelty (åˆ›æ–°æ€§)
    - Rigor (ç†è®ºä¸¥è°¨æ€§)
    - Clarity (æ¸…æ™°åº¦ - åŸºç¡€ç±»æ•™ç¨‹æ­¤é¡¹æƒé‡æœ€é«˜)
    - Utility (å®ç”¨æ€§ - æ˜¯å¦é€‚åˆå­¦ä¹ )

    è¯·è¿”å›ä¸€ä¸ª JSON **åˆ—è¡¨**ã€‚å¦‚æœæ‰¾ä¸åˆ°è¶³å¤Ÿçš„ï¼Œè¯·å°½å¯èƒ½å¤šé€‰ï¼Œä½†ä¸è¦å‡‘æ•°ã€‚
    
    JSON æ ¼å¼ç¤ºä¾‹:
    [
      {{
        "id": "è®ºæ–‡ID",
        "type": "åŸºç¡€å…¥é—¨" æˆ– "å‰æ²¿æ·±åº¦", 
        "scores": {{ "Novelty": 3, "Rigor": 5, "Clarity": 5, "Utility": 5 }},
        "core_value_zh": "ä¸€å¥è¯æ ¸å¿ƒä»·å€¼...",
        "reason_zh": "è¯¦ç»†æ¨èç†ç”±..."
      }}
    ]
    """
    
    full_prompt = f"{system_prompt}\n\n--- æ•™ç¨‹åˆ—è¡¨ ---\n{prompt_papers}"

    max_retries = 5
    base_delay = 10

    for attempt in range(max_retries):
        try:
            logger.info(f"ğŸš€ è¯·æ±‚ AI æ•™ç¨‹åˆ†æ (æ··åˆç­–ç•¥, ç¬¬ {attempt + 1}/{max_retries} æ¬¡)...")
            
            response = client.models.generate_content(
                model='gemini-2.5-flash',
                contents=full_prompt,
                config=types.GenerateContentConfig(temperature=0.4) # ç¨å¾®æé«˜æ¸©åº¦ä»¥å¢åŠ å¤šæ ·æ€§
            )
            
            cleaned = response.text.strip()
            if cleaned.startswith("```"):
                cleaned = re.sub(r"^```\w*\n", "", cleaned)
                cleaned = re.sub(r"\n```$", "", cleaned)
            cleaned = cleaned.strip()

            match = re.search(r'(\[.*\])', cleaned, re.DOTALL)
            if match:
                cleaned = match.group(1)
            
            ai_picks_list = json5.loads(cleaned)
            logger.info(f"âœ… AI æˆåŠŸé€‰å‡º {len(ai_picks_list)} ç¯‡æ··åˆæ•™ç¨‹ã€‚")
            return ai_picks_list

        except Exception as e:
            logger.warning(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
            if attempt < max_retries - 1:
                wait_time = base_delay * (2 ** attempt) + random.uniform(0, 3)
                time.sleep(wait_time)
            else:
                return None

# --- 5. å†™å…¥ JSON ---
def write_to_json(data_to_save, file_path):
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=4)
    except Exception as e:
        logger.error(f"å†™å…¥æ•™ç¨‹ JSON å¤±è´¥: {e}")

# --- 6. ä¸»å‡½æ•° ---
if __name__ == "__main__":
    target_date = date.today()
    logger.info(f"--- æ•™ç¨‹è„šæœ¬å¼€å§‹è¿è¡Œï¼Œç›®æ ‡å‘¨: {target_date.isoformat()} ---")

    # (V21) åå¥½å‡çº§ï¼šå°†â€œåŸºç¡€â€æ˜ç¡®å®šä¹‰ä¸ºâ€œç ”ç©¶ç”Ÿæ ¸å¿ƒè¯¾â€
    my_tutorial_preference = """
    æˆ‘æ˜¯ä¸€åæ•°ç†ç»Ÿè®¡åšå£«ç”Ÿï¼Œæˆ‘çš„å­¦ä¹ éœ€æ±‚åˆ†ä¸ºä¸¤ç±»ï¼ˆè¯·å„é€‰ 5 ç¯‡ï¼‰ï¼š
    
    1. **å‰æ²¿ç ”ç©¶ (Research Frontier):** - å…³æ³¨é«˜ç»´ç»Ÿè®¡ã€å› æœæ¨æ–­ã€Offline RLã€LLM ç†è®ºæœºåˆ¶ã€åŠ å¯†è´§å¸é‡åŒ–ç­‰é¢†åŸŸçš„æœ€æ–°ç»¼è¿°ã€‚
       - è¦æ±‚ï¼šè§†é‡å¼€é˜”ï¼Œèƒ½æŒ‡å¼•æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚
    
    2. **ç ”ç©¶ç”Ÿæ ¸å¿ƒåŸºç¡€ (Graduate Core Foundations):** - æˆ‘éœ€è¦é’ˆå¯¹**ç»Ÿè®¡å­¦ä¸æ•°æ®ç§‘å­¦ç ”ç©¶ç”Ÿ**çš„**åŸºç¡€æ ¸å¿ƒè¯¾ç¨‹**çº§åˆ«çš„æ•™ç¨‹ (Tutorials) æˆ–è®²ä¹‰ (Lecture Notes)ã€‚
       - **æ ¸å¿ƒä¸»é¢˜ï¼š** çŸ©é˜µåˆ†æ (Matrix Analysis)ã€å‡¸ä¼˜åŒ– (Convex Optimization)ã€é«˜ç­‰æ¦‚ç‡è®º/éšæœºè¿‡ç¨‹ã€è´å¶æ–¯æ¨æ–­åŸºç¡€ã€ä¿¡æ¯è®ºç­‰ã€‚
       - **è¦æ±‚ï¼š** å¿…é¡»å…·æœ‰**æ•°å­¦ä¸¥è°¨æ€§ (Mathematical Rigor)**ï¼Œæ–‡ç¬”æµç•…ï¼Œé€»è¾‘æ¸…æ™°ã€‚**ä¸è¦**ç§‘æ™®è¯»ç‰©ï¼Œæˆ‘è¦çš„æ˜¯èƒ½å¸®æˆ‘è¡¥å…¨åšå£«æ•°å­¦æ‹¼å›¾çš„ç¡¬æ ¸èµ„æ–™ã€‚
    
    è¯·åŠ¡å¿…ä¸¥æ ¼æŒ‰ç…§ 50% å‰æ²¿ + 50% æ ¸å¿ƒåŸºç¡€çš„æ¯”ä¾‹è¿›è¡Œç­›é€‰ã€‚
    """
   
    papers = fetch_weekly_tutorials(target_date)
    pick_json_list = get_ai_tutorial_pick(papers, my_tutorial_preference)
   
    final_data_to_save = []
    if pick_json_list:
        for pick_item in pick_json_list:
            full_paper = next((p for p in papers if p['id'] == pick_item['id']), None)
            if full_paper:
                final_data_to_save.append({**full_paper, **pick_item})
    
    if not final_data_to_save:
         final_data_to_save = None
           
    week_number = target_date.isocalendar()[1]
    year = target_date.isocalendar()[0]
    output_filename = f"{year}-W{week_number:02d}.json"
    output_path = os.path.join(ARCHIVE_DIR, "tutorials", output_filename)
   
    write_to_json(final_data_to_save, output_path)
    logger.info(f"\n--- æ•™ç¨‹è„šæœ¬å¤„ç†å®Œæ¯• ---")
