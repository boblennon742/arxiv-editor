[
    {
        "id": "http://arxiv.org/abs/2601.15690v1",
        "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
        "summary": "While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \\textbf{advanced reasoning} to optimize computation and trigger self-correction; in \\textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \\textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.",
        "authors": "Jiaxin Zhang, Wendi Cui, Zhuohang Li, Lifu Huang, Bradley Malin, Caiming Xiong, Chien-Sheng Wu",
        "url": "http://arxiv.org/abs/2601.15690v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15690v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 5,
            "Clarity": 5
        },
        "reason_zh": "这篇综述性论文的核心在于将不确定性量化（UQ）从被动指标提升为主动控制信号，并将其理论基础明确地根植于贝叶斯方法和共形预测等严谨的统计框架。这与您对强大理论基础和现代AI系统应用的偏好高度契合，提供了对未来研究方向的深刻洞察。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16152v1",
        "title": "Substrate Stability Under Persistent Disagreement: Structural Constraints for Neutral Ontological Substrates",
        "summary": "Modern data systems increasingly operate under conditions of persistent legal, political, and analytic disagreement. In such settings, interoperability cannot rely on shared interpretation, negotiated semantics, or centralized authority. Instead, representations must function as neutral substrates that preserve stable reference across incompatible extensions. This paper investigates the structural constraints imposed on ontological design by this requirement. Building on a neutrality framework that treats interpretive non-commitment and stability under extension as explicit design constraints, we ask what minimal ontological structure is forced if accountability relationships are to remain referable and comparable under disagreement. Minimality here is not mere parsimony: a reduction is admissible only if it does not reintroduce stability-critical distinctions as hidden roles, flags, or contextual predicates. We establish a conditional lower-bound result: any ontology capable of supporting accountability under persistent disagreement must realize at least six distinct identity-and-persistence regimes. We further show that a construction with exactly six such regimes is sufficient to satisfy the stated requirements without embedding causal or normative commitments in the substrate. The result is not a proposal for a universal ontology, but a constraint on what is possible when neutrality and stable reference are treated as non-negotiable design goals.",
        "authors": "Denise M. Case",
        "url": "http://arxiv.org/abs/2601.16152v1",
        "pdf_url": "https://arxiv.org/pdf/2601.16152v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "论文通过形式化方法，深入探讨了在持续分歧下本体论设计的结构性约束，提出了条件下界结果和充分性证明。其对“中立基底”和“稳定引用”的严谨逻辑分析，以及对六种身份-持久性机制的理论推导，展现了极高的理论严谨性，非常符合您对严谨数学逻辑和清晰数学推导的要求。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16070v1",
        "title": "On damage of interpolation to adversarial robustness in regression",
        "summary": "Deep neural networks (DNNs) typically involve a large number of parameters and are trained to achieve zero or near-zero training error. Despite such interpolation, they often exhibit strong generalization performance on unseen data, a phenomenon that has motivated extensive theoretical investigations. Comforting results show that interpolation indeed may not affect the minimax rate of convergence under the squared error loss. In the mean time, DNNs are well known to be highly vulnerable to adversarial perturbations in future inputs. A natural question then arises: Can interpolation also escape from suboptimal performance under a future $X$-attack? In this paper, we investigate the adversarial robustness of interpolating estimators in a framework of nonparametric regression. A finding is that interpolating estimators must be suboptimal even under a subtle future $X$-attack, and achieving perfect fitting can substantially damage their robustness. An interesting phenomenon in the high interpolation regime, which we term the curse of simple size, is also revealed and discussed. Numerical experiments support our theoretical findings.",
        "authors": "Jingfu Peng, Yuhong Yang",
        "url": "http://arxiv.org/abs/2601.16070v1",
        "pdf_url": "https://arxiv.org/pdf/2601.16070v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文通过理论研究，揭示了插值估计器在对抗性攻击下的次优性，并提出了“简单尺寸的诅咒”现象。其对非参数回归中对抗鲁棒性的深入数学分析，以及对极小极大收敛率的探讨，完美契合您对强大理论基础和数学推导的偏好。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15880v1",
        "title": "A two-sample pseudo-observation-based regression approach for the relative treatment effect",
        "summary": "The relative treatment effect is an effect measure for the order of two sample-specific outcome variables. It has the interpretation of a probability and also a connection to the area under the ROC curve. In the literature it has been considered for both ordinal or right-censored time-to-event outcomes. For both cases, the present paper introduces a distribution-free regression model that relates the relative treatment effect to a linear combination of covariates. To fit the model, we develop a pseudo-observation-based procedure yielding consistent and asymptotically normal coefficient estimates. In addition, we propose bootstrap-based hypothesis tests to infer the effects of the covariates on the relative treatment effect. A simulation study compares the novel method to Cox regression, demonstrating that the proposed hypothesis tests have high power and keep up with the z-test of the Cox model even in scenarios where the latter is specified correctly. The new methods are used to re-analyze data from the SUCCESS-A trial for progression-free survival of breast cancer patients.",
        "authors": "Dennis Dobler, Alina Schenk, Matthias Schmid",
        "url": "http://arxiv.org/abs/2601.15880v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15880v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文提出了一种基于伪观测的无分布回归模型，用于相对治疗效果的估计。其推导出的系数估计量具有一致性和渐近正态性，并提出了基于Bootstrap的假设检验。这篇论文在统计理论和方法学上具有极高的严谨性，是数理统计领域的典型研究，与您的专业背景高度匹配。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15751v1",
        "title": "Tabular Incremental Inference",
        "summary": "Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.",
        "authors": "Xinda Chen, Xing Zhen, Hanyu Zhang, Weimin Tan, Bo Yan",
        "url": "http://arxiv.org/abs/2601.15751v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15751v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文引入了表格增量推理这一新任务，并将其明确地框定为基于信息瓶颈理论的优化问题。通过最小化互信息和最大化表示与标签之间的互信息，展现了严谨的信息论和统计学基础，非常符合您对强大理论基础和数学推导的要求。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15652v1",
        "title": "Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models",
        "summary": "Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).   Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises (\"Sycophancy\").   This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.",
        "authors": "Manish Bhatt",
        "url": "http://arxiv.org/abs/2601.15652v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15652v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文提出了一种结合预测编码和信息瓶颈理论的幻觉检测框架。其信号设计根植于信息论和神经科学的严谨理论，并进行了系统性分析，包括负面结果的讨论，展现了深厚的理论基础和严谨的科学方法。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15599v1",
        "title": "Autonomous Business System via Neuro-symbolic AI",
        "summary": "Current business environments require organizations to continuously reconfigure cross-functional processes, yet enterprise systems are still organized around siloed departments, rigid workflows, and hard-coded automation. Meanwhile large language models (LLMs) excel at interpreting natural language and unstructured data but lack deterministic, verifiable execution of complex business logic. To address this gap, here we introduce AUTOBUS, an Autonomous Business System that integrates LLM-based AI agents, predicate-logic programming, and business-semantics-centric enterprise data into a coherent neuro-symbolic AI architecture for orchestrating end-to-end business initiatives. AUTOBUS models an initiative as a network of tasks with explicit pre/post conditions, required data, evaluation rules, and API-level actions. Enterprise data is organized as a knowledge graph whose entities, relationships, and constraints are translated into logic facts and foundational rules, providing the semantic grounding for task reasoning. Core AI agents synthesize task instructions, enterprise semantics, and available tools into task-specific logic programs, which are executed by a logic engine that enforces constraints, coordinates auxiliary tools, and orchestrate execution of actions and outcomes. Humans define and maintain the semantics, policies and task instructions, curate tools, and supervise high-impact or ambiguous decisions, ensuring accountability and adaptability. We detail the AUTOBUS architecture, the anatomy of the AI agent generated logic programs, and the role of humans and auxiliary tools in the lifecycle of a business initiative.",
        "authors": "Cecil Pang, Hiroki Sayama",
        "url": "http://arxiv.org/abs/2601.15599v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15599v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文提出了一个结合LLM代理和谓词逻辑编程的神经符号AI架构。其将业务逻辑和数据转化为逻辑事实和规则，并通过逻辑引擎进行约束和协调，展现了形式逻辑和符号AI的强大理论严谨性，是现代AI系统中结合严谨逻辑的优秀范例。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16174v1",
        "title": "Beyond Predictive Uncertainty: Reliable Representation Learning with Structural Constraints",
        "summary": "Uncertainty estimation in machine learning has traditionally focused on the prediction stage, aiming to quantify confidence in model outputs while treating learned representations as deterministic and reliable by default. In this work, we challenge this implicit assumption and argue that reliability should be regarded as a first-class property of learned representations themselves. We propose a principled framework for reliable representation learning that explicitly models representation-level uncertainty and leverages structural constraints as inductive biases to regularize the space of feasible representations. Our approach introduces uncertainty-aware regularization directly in the representation space, encouraging representations that are not only predictive but also stable, well-calibrated, and robust to noise and structural perturbations. Structural constraints, such as sparsity, relational structure, or feature-group dependencies, are incorporated to define meaningful geometry and reduce spurious variability in learned representations, without assuming fully correct or noise-free structure. Importantly, the proposed framework is independent of specific model architectures and can be integrated with a wide range of representation learning methods.",
        "authors": "Yiyao Yang",
        "url": "http://arxiv.org/abs/2601.16174v1",
        "pdf_url": "https://arxiv.org/pdf/2601.16174v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文提出了一个用于可靠表示学习的原则性框架，通过不确定性感知正则化和结构约束来提高表示的稳定性。其对表示层不确定性的建模和结构约束的引入，体现了对模型可靠性和理论基础的关注。"
    },
    {
        "id": "http://arxiv.org/abs/2601.16045v1",
        "title": "AgriPINN: A Process-Informed Neural Network for Interpretable and Scalable Crop Biomass Prediction Under Water Stress",
        "summary": "Accurate prediction of crop above-ground biomass (AGB) under water stress is critical for monitoring crop productivity, guiding irrigation, and supporting climate-resilient agriculture. Data-driven models scale well but often lack interpretability and degrade under distribution shift, whereas process-based crop models (e.g. DSSAT, APSIM, LINTUL5) require extensive calibration and are difficult to deploy over large spatial domains. To address these limitations, we propose AgriPINN, a process-informed neural network that integrates a biophysical crop-growth differential equation as a differentiable constraint within a deep learning backbone. This design encourages physiologically consistent biomass dynamics under water-stress conditions while preserving model scalability for spatially distributed AGB prediction. AgriPINN recovers latent physiological variables, including leaf area index (LAI), absorbed photosynthetically active radiation (PAR), radiation use efficiency (RUE), and water-stress factors, without requiring direct supervision. We pretrain AgriPINN on 60 years of historical data across 397 regions in Germany and fine-tune it on three years of field experiments under controlled water treatments. Results show that AgriPINN consistently outperforms state-of-the-art deep-learning baselines (ConvLSTM-ViT, SLTF, CNN-Transformer) and the process-based LINTUL5 model in terms of accuracy (RMSE reductions up to $43\\%$) and computational efficiency. By combining the scalability of deep learning with the biophysical rigor of process-based modeling, AgriPINN provides a robust and interpretable framework for spatio-temporal AGB prediction, offering practical value for planning of irrigation infrastructure, yield forecasting, and climate-adaptation planning.",
        "authors": "Yue Shi, Liangxiu Han, Xin Zhang, Tam Sobeih, Thomas Gaiser, Nguyen Huu Thuy, Dominik Behrend, Amit Kumar Srivastava, Krishnagopal Halder, Frank Ewert",
        "url": "http://arxiv.org/abs/2601.16045v1",
        "pdf_url": "https://arxiv.org/pdf/2601.16045v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文将生物物理作物生长微分方程作为可微分约束集成到深度学习模型中，实现了过程信息神经网络（PINN）。这种将领域知识（物理/生物学）以严谨数学形式融入AI模型的方法，体现了强大的理论基础和跨学科的严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15888v1",
        "title": "Understanding the Transfer Limits of Vision Foundation Models",
        "summary": "Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.",
        "authors": "Shiqi Huang, Yipei Wang, Natasha Thorley, Alexander Ng, Shaheer Saeed, Mark Emberton, Shonit Punwani, Veeru Kasivisvanathan, Dean Barratt, Daniel Alexander, Yipeng Hu",
        "url": "http://arxiv.org/abs/2601.15888v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15888v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文通过使用最大均值差异（MMD）等统计散度指标，系统性地分析了视觉基础模型的迁移限制。这种以严谨统计量化方法来理解现代AI系统行为的研究，与您的偏好高度一致。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15859v1",
        "title": "Uncertainty-guided Generation of Dark-field Radiographs",
        "summary": "X-ray dark-field radiography provides complementary diagnostic information to conventional attenuation imaging by visualizing microstructural tissue changes through small-angle scattering. However, the limited availability of such data poses challenges for developing robust deep learning models. In this work, we present the first framework for generating dark-field images directly from standard attenuation chest X-rays using an Uncertainty-Guided Progressive Generative Adversarial Network. The model incorporates both aleatoric and epistemic uncertainty to improve interpretability and reliability. Experiments demonstrate high structural fidelity of the generated images, with consistent improvement of quantitative metrics across stages. Furthermore, out-of-distribution evaluation confirms that the proposed model generalizes well. Our results indicate that uncertainty-guided generative modeling enables realistic dark-field image synthesis and provides a reliable foundation for future clinical applications.",
        "authors": "Lina Felsner, Henriette Bast, Tina Dorosti, Florian Schaff, Franz Pfeiffer, Daniela Pfeiffer, Julia Schnabel",
        "url": "http://arxiv.org/abs/2601.15859v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15859v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文在生成模型中融入了偶然不确定性（aleatoric uncertainty）和认知不确定性（epistemic uncertainty），以提高生成结果的解释性和可靠性。对不确定性的显式建模和利用，是统计学在现代AI系统中的重要应用。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15801v1",
        "title": "Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models",
        "summary": "While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \\textbf{G}lobal \\textbf{O}ptimization for \\textbf{S}afety \\textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.",
        "authors": "Fengheng Chu, Jiahao Chen, Yuhong Wang, Jun Wang, Zhihui Fu, Shouling Ji, Songze Li",
        "url": "http://arxiv.org/abs/2601.15801v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15801v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文通过全局优化方法识别LLM中安全关键的注意力头，揭示了模型内部的安全机制。全局优化本身就是严谨的数学工具，用于理解复杂AI系统的内部工作原理，符合您对严谨数学逻辑应用于AI系统的兴趣。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15778v1",
        "title": "Agentic Confidence Calibration",
        "summary": "AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.",
        "authors": "Jiaxin Zhang, Caiming Xiong, Chien-Sheng Wu",
        "url": "http://arxiv.org/abs/2601.15778v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15778v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文提出了Agentic Confidence Calibration问题，并设计了Holistic Trajectory Calibration框架，旨在提高AI代理在多步骤任务中的可靠性。对置信度校准（一个核心统计概念）的关注，以及对ECE等指标的优化，体现了统计学在AI代理中的应用。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15703v1",
        "title": "Agentic Uncertainty Quantification",
        "summary": "Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.",
        "authors": "Jiaxin Zhang, Prafulla Kumar Choubey, Kung-Hsiang Huang, Caiming Xiong, Chien-Sheng Wu",
        "url": "http://arxiv.org/abs/2601.15703v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15703v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文提出了双进程Agentic UQ框架，将口头表达的不确定性转化为主动控制信号。其对不确定性量化和轨迹级校准的关注，以及“原则性框架”的提出，与您对统计保证和严谨理论的追求相符。"
    },
    {
        "id": "http://arxiv.org/abs/2601.15625v1",
        "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
        "summary": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.",
        "authors": "Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang, Jiakang Wang, Yao Hu, Shaosheng Cao, Kam-Fai Wong",
        "url": "http://arxiv.org/abs/2601.15625v1",
        "pdf_url": "https://arxiv.org/pdf/2601.15625v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文提出了Fission-GRPO框架，通过将执行错误转化为强化学习训练循环中的纠正监督，提高了LLM工具使用的鲁棒性。强化学习（RL）本身具有深厚的优化理论基础，而GRPO（广义策略优化）更是其中的严谨算法，体现了对优化收敛性和鲁棒性的关注。"
    }
]