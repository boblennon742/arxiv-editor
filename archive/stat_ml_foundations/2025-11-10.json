{
    "id": "http://arxiv.org/abs/2511.06639v1",
    "title": "Bernstein-von Mises for Adaptively Collected Data",
    "summary": "Uncertainty quantification (UQ) for adaptively collected data, such as that coming from adaptive experiments, bandits, or reinforcement learning, is necessary for critical elements of data collection such as ensuring safety and conducting after-study inference. The data's adaptivity creates significant challenges for frequentist UQ, yet Bayesian UQ remains the same as if the data were independent and identically distributed (i.i.d.), making it an appealing and commonly used approach. Bayesian UQ requires the (correct) specification of a prior distribution while frequentist UQ does not, but for i.i.d. data the celebrated Bernstein-von Mises theorem shows that as the sample size grows, the prior 'washes out' and Bayesian UQ becomes frequentist-valid, implying that the choice of prior need not be a major impediment to Bayesian UQ as it makes no difference asymptotically. This paper for the first time extends the Bernstein-von Mises theorem to adaptively collected data, proving asymptotic equivalence between Bayesian UQ and Wald-type frequentist UQ in this challenging setting. Our result showing this asymptotic agreement does not require the standard stability condition required by works studying validity of Wald-type frequentist UQ; in cases where stability is satisfied, our results combined with these prior studies of frequentist UQ imply frequentist validity of Bayesian UQ. Counterintuitively however, they also provide a negative result that Bayesian UQ is not asymptotically frequentist valid when stability fails, despite the fact that the prior washes out and Bayesian UQ asymptotically matches standard Wald-type frequentist UQ. We empirically validate our theory (positive and negative) via a range of simulations.",
    "authors": "Kevin Du, Yash Nair, Lucas Janson",
    "url": "http://arxiv.org/abs/2511.06639v1",
    "pdf_url": "http://arxiv.org/pdf/2511.06639v1",
    "reason_zh": "这篇论文完美契合您作为统计学博士生的研究兴趣。它直接关注“不确定性量化 (UQ)”，这是提供机器学习算法“可靠性、可解释性或置信区间”的关键组成部分。论文核心在于将著名的“Bernstein-von Mises 定理”扩展到“自适应收集数据”这一现代机器学习（如强化学习、多臂老虎机）中的挑战性场景。这充分体现了“强统计理论基础”。摘要明确提及“证明渐近等价”、“理论界限”（通过对稳定性条件的讨论以及正向和负向结果），并强调了“数学证明”，这些都完全符合您对“清晰数学证明和理论界限”的偏好。它不仅提供了贝叶斯 UQ 和频率论 UQ 之间的渐近一致性，还探讨了何时这种一致性可能失效，这对于理解现代 ML 算法的统计性质及其局限性至关重要。",
    "reason_en": "This paper perfectly aligns with your research interests as a PhD student in statistics. It directly addresses \"Uncertainty Quantification (UQ),\" which is a crucial component for providing \"reliability, interpretability, or confidence intervals\" for machine learning algorithms. The core of the paper extends the celebrated \"Bernstein-von Mises theorem\" to the challenging setting of \"adaptively collected data,\" commonly found in modern machine learning contexts like reinforcement learning and bandit problems. This demonstrates a \"strong statistical theory foundation.\" The abstract explicitly mentions \"proving asymptotic equivalence,\" \"theoretical bounds\" (through discussion of stability conditions and both positive and negative results), and emphasizes \"mathematical proofs,\" all of which perfectly match your preference for \"clear mathematical proof and theoretical bounds.\" It not only establishes asymptotic agreement between Bayesian UQ and frequentist UQ but also explores cases where this agreement might fail, which is vital for understanding the statistical properties and limitations of modern ML algorithms."
}