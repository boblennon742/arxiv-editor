{
    "id": "http://arxiv.org/abs/2511.06675v1",
    "title": "Adam symmetry theorem: characterization of the convergence of the stochastic Adam optimizer",
    "summary": "Beside the standard stochastic gradient descent (SGD) method, the Adam optimizer due to Kingma & Ba (2014) is currently probably the best-known optimization method for the training of deep neural networks in artificial intelligence (AI) systems. Despite the popularity and the success of Adam it remains an \\emph{open research problem} to provide a rigorous convergence analysis for Adam even for the class of strongly convex SOPs. In one of the main results of this work we establish convergence rates for Adam in terms of the number of gradient steps (convergence rate \\nicefrac{1}{2} w.r.t. the size of the learning rate), the size of the mini-batches (convergence rate 1 w.r.t. the size of the mini-batches), and the size of the second moment parameter of Adam (convergence rate 1 w.r.t. the distance of the second moment parameter to 1) for the class of strongly convex SOPs. In a further main result of this work, which we refer to as \\emph{Adam symmetry theorem}, we illustrate the optimality of the established convergence rates by proving for a special class of simple quadratic strongly convex SOPs that Adam converges as the number of gradient steps increases to infinity to the solution of the SOP (the unique minimizer of the strongly convex objective function) if and \\emph{only} if the random variables in the SOP (the data in the SOP) are \\emph{symmetrically distributed}. In particular, in the standard case where the random variables in the SOP are not symmetrically distributed we \\emph{disprove} that Adam converges to the minimizer of the SOP as the number of Adam steps increases to infinity. We also complement the conclusions of our convergence analysis and the Adam symmetry theorem by several numerical simulations that indicate the sharpness of the established convergence rates and that illustrate the practical appearance of the phenomena revealed in the \\emph{Adam symmetry theorem}.",
    "authors": "Steffen Dereich, Thang Do, Arnulf Jentzen, Philippe von Wurstemberger",
    "url": "http://arxiv.org/abs/2511.06675v1",
    "pdf_url": "http://arxiv.org/pdf/2511.06675v1",
    "reason_zh": "这篇论文完美契合您的需求，因为它直接深入探讨了深度学习领域最常用的优化器之一——Adam的收敛性。论文建立了Adam在强凸随机优化问题上的收敛速率，并进一步提出了“Adam对称定理”，详细阐述了Adam收敛到全局最小值的充分必要条件，甚至在某些标准情况下反驳了其收敛性。这不仅提供了具体的收敛界限（收敛速率），更重要的是，通过揭示Adam的内在机制和其在非对称数据条件下的行为，提供了对算法稳定性及其局限性“深入理解”的关键洞察。这与您对“深度学习模型收敛性、泛化能力、或训练效率”的关注，以及“对算法稳定性或界限有深入理解”的需求高度吻合。",
    "reason_en": "This paper perfectly aligns with your preferences as it delves deeply into the convergence properties of Adam, one of the most widely used optimizers in deep learning. It establishes rigorous convergence rates for Adam in strongly convex stochastic optimization problems and, crucially, introduces the 'Adam symmetry theorem'. This theorem characterizes the necessary and sufficient conditions for Adam's convergence to the global minimum, even disproving its convergence under certain standard conditions. This not only provides concrete convergence bounds (rates) but, more importantly, offers a 'deep understanding' of the algorithm's stability and limitations by revealing its internal mechanics and behavior under asymmetric data conditions. This directly addresses your focus on 'convergence, generalization ability, or training efficiency' for deep learning models, and your specific need for 'deep understanding of algorithm stability or bounds'."
}