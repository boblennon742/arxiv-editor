[
    {
        "id": "http://arxiv.org/abs/2602.21031v1",
        "title": "Exchangeable Gaussian Processes for Staggered-Adoption Policy Evaluation",
        "summary": "We study the use of exchangeable multi-task Gaussian processes (GPs) for causal inference in panel data, applying the framework to two settings: one with a single treated unit subject to a once-and-for-all treatment and another with multiple treated units and staggered treatment adoption. Our approach models the joint evolution of outcomes for treated and control units through a GP prior that ensures exchangeability across units while allowing for flexible nonlinear trends over time. The resulting posterior predictive distribution for the untreated potential outcomes of the treated unit provides a counterfactual path, from which we derive pointwise and cumulative treatment effects, along with credible intervals to quantify uncertainty. We implement several variations of the exchangeable GP model using different kernel functions. To assess prediction accuracy, we conduct a placebo-style validation within the pre-intervention window by selecting a ``fake'' intervention date. Ultimately, this study illustrates how exchangeable GPs serve as a flexible tool for policy evaluation in panel data settings and proposes a novel approach to staggered-adoption designs with a large number of treated and control units.",
        "authors": "Hayk Gevorgyan, Konstantinos Kalogeropoulos, Angelos Alexopoulos",
        "url": "http://arxiv.org/abs/2602.21031v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21031v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文将可交换高斯过程应用于面板数据的因果推断，特别是在交错采用政策评估中。它强调了高斯过程先验的交换性、后验预测分布、反事实路径以及可信区间来量化不确定性，完美契合了您对统计保证和因果逻辑的偏好。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20921v1",
        "title": "On the Generalization Behavior of Deep Residual Networks From a Dynamical System Perspective",
        "summary": "Deep neural networks (DNNs) have significantly advanced machine learning, with model depth playing a central role in their successes. The dynamical system modeling approach has recently emerged as a powerful framework, offering new mathematical insights into the structure and learning behavior of DNNs. In this work, we establish generalization error bounds for both discrete- and continuous-time residual networks (ResNets) by combining Rademacher complexity, flow maps of dynamical systems, and the convergence behavior of ResNets in the deep-layer limit. The resulting bounds are of order $O(1/\\sqrt{S})$ with respect to the number of training samples $S$, and include a structure-dependent negative term, yielding depth-uniform and asymptotic generalization bounds under milder assumptions. These findings provide a unified understanding of generalization across both discrete- and continuous-time ResNets, helping to close the gap in both the order of sample complexity and assumptions between the discrete- and continuous-time settings.",
        "authors": "Jinshu Huang, Mingfei Sun, Chunlin Wu",
        "url": "http://arxiv.org/abs/2602.20921v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20921v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "这篇论文从动力系统角度深入分析了深度残差网络的泛化行为，建立了基于Rademacher复杂度、流映射和收敛行为的泛化误差界限。其对深度学习理论基础的严谨数学推导和统一理解，与您的数理统计背景高度吻合。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20795v1",
        "title": "Hawkes Identification with a Prescribed Causal Basis: Closed-Form Estimators and Asymptotics",
        "summary": "Driven by the recent surge in neural-inspired modeling, point processes have gained significant traction in systems and control. While the Hawkes process is the standard model for characterizing random event sequences with memory, identifying its unknown kernels is often hindered by nonlinearity. Approaches using prescribed basis kernels have emerged to enable linear parameterization, yet they typically rely on iterative likelihood methods and lack rigorous analysis under model misspecification. This paper justifies a closed-form Least Squares identification framework for Hawkes processes with prescribed kernels. We guarantee estimator existence via the almost-sure positive definiteness of the empirical Gram matrix and prove convergence to the true parameters under correct specification, or to well-defined pseudo-true parameters under misspecification. Furthermore, we derive explicit Central Limit Theorems for both regimes, providing a complete and interpretable asymptotic theory. We demonstrate these theoretical findings through comparative numerical simulations.",
        "authors": "Xinhui Rong, Girish N. Nair",
        "url": "http://arxiv.org/abs/2602.20795v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20795v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "论文为具有预设因果基的霍克斯过程提供了闭式最小二乘识别框架，并严格证明了估计器的存在性、收敛性（包括模型误设定下的伪真参数）以及明确的中心极限定理。其完整的渐近理论和可解释性，是统计学理论的典范。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20729v1",
        "title": "Fuz-RL: A Fuzzy-Guided Robust Framework for Safe Reinforcement Learning under Uncertainty",
        "summary": "Safe Reinforcement Learning (RL) is crucial for achieving high performance while ensuring safety in real-world applications. However, the complex interplay of multiple uncertainty sources in real environments poses significant challenges for interpretable risk assessment and robust decision-making. To address these challenges, we propose Fuz-RL, a fuzzy measure-guided robust framework for safe RL. Specifically, our framework develops a novel fuzzy Bellman operator for estimating robust value functions using Choquet integrals. Theoretically, we prove that solving the Fuz-RL problem (in Constrained Markov Decision Process (CMDP) form) is equivalent to solving distributionally robust safe RL problems (in robust CMDP form), effectively avoiding min-max optimization. Empirical analyses on safe-control-gym and safety-gymnasium scenarios demonstrate that Fuz-RL effectively integrates with existing safe RL baselines in a model-free manner, significantly improving both safety and control performance under various types of uncertainties in observation, action, and dynamics.",
        "authors": "Xu Wan, Chao Yang, Cheng Yang, Jie Song, Mingyang Sun",
        "url": "http://arxiv.org/abs/2602.20729v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20729v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文提出了一个模糊测度引导的鲁棒安全强化学习框架，引入了基于Choquet积分的模糊Bellman算子，并从理论上证明了其与分布鲁棒安全RL问题的等价性，有效避免了min-max优化。这体现了强大的理论基础和数学推导能力。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20652v1",
        "title": "DANCE: Doubly Adaptive Neighborhood Conformal Estimation",
        "summary": "The recent developments of complex deep learning models have led to unprecedented ability to accurately predict across multiple data representation types. Conformal prediction for uncertainty quantification of these models has risen in popularity, providing adaptive, statistically-valid prediction sets. For classification tasks, conformal methods have typically focused on utilizing logit scores. For pre-trained models, however, this can result in inefficient, overly conservative set sizes when not calibrated towards the target task. We propose DANCE, a doubly locally adaptive nearest-neighbor based conformal algorithm combining two novel nonconformity scores directly using the data's embedded representation. DANCE first fits a task-adaptive kernel regression model from the embedding layer before using the learned kernel space to produce the final prediction sets for uncertainty quantification. We test against state-of-the-art local, task-adapted and zero-shot conformal baselines, demonstrating DANCE's superior blend of set size efficiency and robustness across various datasets.",
        "authors": "Brandon R. Feng, Brian J. Reich, Daniel Beaglehole, Xihaier Luo, David Keetae Park, Shinjae Yoo, Zhechao Huang, Xueyu Mao, Olcay Boz, Jungeum Kim",
        "url": "http://arxiv.org/abs/2602.20652v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20652v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "论文提出了一个双重局部自适应的邻域共形预测算法，旨在为复杂深度学习模型提供自适应、统计有效的预测集。它引入了新颖的非一致性分数和任务自适应核回归模型，其对不确定性量化的统计严谨性非常符合您的偏好。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20651v1",
        "title": "Sparse Bayesian Deep Functional Learning with Structured Region Selection",
        "summary": "In modern applications such as ECG monitoring, neuroimaging, wearable sensing, and industrial equipment diagnostics, complex and continuously structured data are ubiquitous, presenting both challenges and opportunities for functional data analysis. However, existing methods face a critical trade-off: conventional functional models are limited by linearity, whereas deep learning approaches lack interpretable region selection for sparse effects. To bridge these gaps, we propose a sparse Bayesian functional deep neural network (sBayFDNN). It learns adaptive functional embeddings through a deep Bayesian architecture to capture complex nonlinear relationships, while a structured prior enables interpretable, region-wise selection of influential domains with quantified uncertainty. Theoretically, we establish rigorous approximation error bounds, posterior consistency, and region selection consistency. These results provide the first theoretical guarantees for a Bayesian deep functional model, ensuring its reliability and statistical rigor. Empirically, comprehensive simulations and real-world studies confirm the effectiveness and superiority of sBayFDNN. Crucially, sBayFDNN excels in recognizing intricate dependencies for accurate predictions and more precisely identifies functionally meaningful regions, capabilities fundamentally beyond existing approaches.",
        "authors": "Xiaoxian Zhu, Yingmeng Li, Shuangge Ma, Mengyun Wu",
        "url": "http://arxiv.org/abs/2602.20651v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20651v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文提出了稀疏贝叶斯深度函数神经网络，并首次为贝叶斯深度函数模型建立了严格的近似误差界、后验一致性和区域选择一致性等理论保证。其结合了深度学习、贝叶斯统计和函数数据分析的严谨理论，是您寻求的典范。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20605v1",
        "title": "Quantum circuit design from a retraction-based Riemannian optimization framework",
        "summary": "Designing quantum circuits for ground state preparation is a fundamental task in quantum information science. However, standard Variational Quantum Algorithms (VQAs) are often constrained by limited ansatz expressivity and difficult optimization landscapes. To address these issues, we adopt a geometric perspective, formulating the problem as the minimization of an energy cost function directly over the unitary group. We establish a retraction-based Riemannian optimization framework for this setting, ensuring that all algorithmic procedures are implementable on quantum hardware. Within this framework, we unify existing randomized gradient approaches under a Riemannian Random Subspace Gradient Projection (RRSGP) method. While recent geometric approaches have predominantly focused on such first-order gradient descent techniques, efficient second-order methods remain unexplored. To bridge this gap, we derive explicit expressions for the Riemannian Hessian and show that it can be estimated directly on quantum hardware via parameter-shift rules. Building on this, we propose the Riemannian Random Subspace Newton (RRSN) method, a scalable second-order algorithm that constructs a Newton system from measurement data. Numerical simulations indicate that RRSN achieves quadratic convergence, yielding high-precision ground states in significantly fewer iterations compared to both existing first-order approaches and standard VQA baselines. Ultimately, this work provides a systematic foundation for applying a broader class of efficient Riemannian algorithms to quantum circuit design.",
        "authors": "Zhijian Lai, Hantao Nie, Jiayuan Wu, Dong An",
        "url": "http://arxiv.org/abs/2602.20605v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20605v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文将量子电路设计问题转化为酉群上的黎曼优化问题，并推导了黎曼Hessian的显式表达式，提出了可扩展的二阶算法，实现了二次收敛。其在黎曼优化和量子计算中的深厚数学理论和严谨推导，非常符合您的研究方向。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20585v1",
        "title": "Characterizing Online and Private Learnability under Distributional Constraints via Generalized Smoothness",
        "summary": "Understanding minimal assumptions that enable learning and generalization is perhaps the central question of learning theory. Several celebrated results in statistical learning theory, such as the VC theorem and Littlestone's characterization of online learnability, establish conditions on the hypothesis class that allow for learning under independent data and adversarial data, respectively. Building upon recent work bridging these extremes, we study sequential decision making under distributional adversaries that can adaptively choose data-generating distributions from a fixed family $U$ and ask when such problems are learnable with sample complexity that behaves like the favorable independent case. We provide a near complete characterization of families $U$ that admit learnability in terms of a notion known as generalized smoothness i.e. a distribution family admits VC-dimension-dependent regret bounds for every finite-VC hypothesis class if and only if it is generalized smooth. Further, we give universal algorithms that achieve low regret under any generalized smooth adversary without explicit knowledge of $U$. Finally, when $U$ is known, we provide refined bounds in terms of a combinatorial parameter, the fragmentation number, that captures how many disjoint regions can carry nontrivial mass under $U$. These results provide a nearly complete understanding of learnability under distributional adversaries. In addition, building upon the surprising connection between online learning and differential privacy, we show that the generalized smoothness also characterizes private learnability under distributional constraints.",
        "authors": "Moïse Blanchard, Abhishek Shetty, Alexander Rakhlin",
        "url": "http://arxiv.org/abs/2602.20585v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20585v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文对分布约束下的在线和私有可学习性进行了近乎完整的刻画，引入了广义平滑度概念，并建立了VC维相关的遗憾界和组合参数相关的精炼界。它还揭示了在线学习与差分隐私之间的联系，是学习理论中具有强大理论基础的杰作。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20555v1",
        "title": "Standard Transformers Achieve the Minimax Rate in Nonparametric Regression with $C^{s,λ}$ Targets",
        "summary": "The tremendous success of Transformer models in fields such as large language models and computer vision necessitates a rigorous theoretical investigation. To the best of our knowledge, this paper is the first work proving that standard Transformers can approximate Hölder functions $ C^{s,λ}\\left([0,1]^{d\\times n}\\right) $$ (s\\in\\mathbb{N}_{\\geq0},0<λ\\leq1) $ under the $L^t$ distance ($t \\in [1, \\infty]$) with arbitrary precision. Building upon this approximation result, we demonstrate that standard Transformers achieve the minimax optimal rate in nonparametric regression for Hölder target functions. It is worth mentioning that, by introducing two metrics: the size tuple and the dimension vector, we provide a fine-grained characterization of Transformer structures, which facilitates future research on the generalization and optimization errors of Transformers with different structures. As intermediate results, we also derive the upper bounds for the Lipschitz constant of standard Transformers and their memorization capacity, which may be of independent interest. These findings provide theoretical justification for the powerful capabilities of Transformer models.",
        "authors": "Yanming Lai, Defeng Sun",
        "url": "http://arxiv.org/abs/2602.20555v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20555v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "论文首次证明了标准Transformer模型在非参数回归中能达到Hölder目标函数的极小极大最优率，并提供了Lipschitz常数和记忆容量的上限。这是对Transformer模型强大能力进行严格理论验证的开创性工作，具有极高的数学严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20541v1",
        "title": "Maximin Share Guarantees via Limited Cost-Sensitive Sharing",
        "summary": "We study the problem of fairly allocating indivisible goods when limited sharing is allowed, that is, each good may be allocated to up to $k$ agents, while incurring a cost for sharing. While classic maximin share (MMS) allocations may not exist in many instances, we demonstrate that allowing controlled sharing can restore fairness guarantees that are otherwise unattainable in certain scenarios. (1) Our first contribution shows that exact maximin share (MMS) allocations are guaranteed to exist whenever goods are allowed to be cost-sensitively shared among at least half of the agents and the number of agents is even; for odd numbers of agents, we obtain a slightly weaker MMS guarantee. (2) We further design a Shared Bag-Filling Algorithm that guarantees a $(1 - C)(k - 1)$-approximate MMS allocation, where $C$ is the maximum cost of sharing a good. Notably, when $(1 - C)(k - 1) \\geq 1$, our algorithm recovers an exact MMS allocation. (3) We additionally introduce the Sharing Maximin Share (SMMS) fairness notion, a natural extension of MMS to the $k$-sharing setting. (4) We show that SMMS allocations always exist under identical utilities and for instances with two agents. (5) We construct a counterexample to show the impossibility of the universal existence of an SMMS allocation. (6) Finally, we establish a connection between SMMS and constrained MMS (CMMS), yielding approximation guarantees for SMMS via existing CMMS results. These contributions provide deep theoretical insights for the problem of fair resource allocation when a limited sharing of resources are allowed in multi-agent environments.",
        "authors": "Hana Salavcova, Martin Černý, Arpita Biswas",
        "url": "http://arxiv.org/abs/2602.20541v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20541v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "论文深入研究了有限成本敏感共享下不可分物品的公平分配问题，证明了精确Maximin Share (MMS) 分配的存在性，并为新的公平概念SMMS提供了近似保证。其在离散数学和公平分配理论中的严谨数学证明，符合您对理论基础的追求。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20498v1",
        "title": "Fast Algorithms for Exact Confidence Intervals in Randomized Experiments with Binary Outcomes",
        "summary": "We construct exact confidence intervals for the average treatment effect in randomized experiments with binary outcomes using sequences of randomization tests. Our approach does not rely on large-sample approximations and is valid for all sample sizes. Under a balanced Bernoulli design or a matched-pairs design, we show that exact confidence intervals can be computed using only $O(\\log n)$ randomization tests, yielding an exponential reduction in the number of tests compared to brute-force. We further prove an information-theoretic lower bound showing that this rate is optimal. In contrast, under balanced complete randomization, the most efficient known procedures require $O(n\\log n)$ randomization tests (Aronow et al., 2023), establishing a sharp separation between these designs. In addition, we extend our algorithm to general Bernoulli designs using $O(n^2)$ randomization tests.",
        "authors": "Peng Zhang",
        "url": "http://arxiv.org/abs/2602.20498v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20498v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "论文为随机实验中二元结果的平均处理效应构建了精确置信区间，不依赖大样本近似，对所有样本量都有效，并证明了算法的$O(log n)$最优率和信息论下界。这在因果推断和统计保证方面具有极高的严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20463v1",
        "title": "A Long-Short Flow-Map Perspective for Drifting Models",
        "summary": "This paper provides a reinterpretation of the Drifting Model~\\cite{deng2026generative} through a semigroup-consistent long-short flow-map factorization. We show that a global transport process can be decomposed into a long-horizon flow map followed by a short-time terminal flow map admitting a closed-form optimal velocity representation, and that taking the terminal interval length to zero recovers exactly the drifting field together with a conservative impulse term required for flow-map consistency. Based on this perspective, we propose a new likelihood learning formulation that aligns the long-short flow-map decomposition with density evolution under transport. We validate the framework through both theoretical analysis and empirical evaluations on benchmark tests, and further provide a theoretical interpretation of the feature-space optimization while highlighting several open problems for future study.",
        "authors": "Zhiqi Li, Bo Zhu",
        "url": "http://arxiv.org/abs/2602.20463v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20463v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "论文通过半群一致的长短流映射分解，对漂移模型进行了重新解释，并推导了闭式最优速度表示，确保了流映射一致性。其深厚的数学重解释和理论分析，为理解生成模型提供了严谨的视角。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20878v1",
        "title": "Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs",
        "summary": "Large Vision-Language Models (LVLMs) achieve strong performance on visual question answering benchmarks, yet often rely on spurious correlations rather than genuine causal reasoning. Existing evaluations primarily assess the correctness of the answers, making it unclear whether failures arise from limited reasoning capability or from misidentifying causally relevant information. We introduce Vision-Language Causal Graphs (VLCGs), a structured, query-conditioned representation that explicitly encodes causally relevant objects, attributes, relations, and scene-grounded assumptions. Building on this representation, we present ViLCaR, a diagnostic benchmark comprising tasks for Causal Attribution, Causal Inference, and Question Answering, along with graph-aligned evaluation metrics that assess relevance identification beyond final answer accuracy. Experiments in state-of-the-art LVLMs show that injecting structured relevance information significantly improves attribution and inference consistency compared to zero-shot and standard in-context learning. These findings suggest that current limitations in LVLM causal reasoning stem primarily from insufficient structural guidance rather than a lack of reasoning capacity.",
        "authors": "Dhita Putri Pratama, Soyeon Caren Han, Yihao Ding",
        "url": "http://arxiv.org/abs/2602.20878v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20878v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "论文引入了视觉-语言因果图（VLCGs）来诊断视觉-语言模型中的因果推理能力，明确编码了因果相关信息，并提出了图对齐的评估指标。这直接关注因果逻辑在AI系统中的应用和评估，与您的偏好高度相关。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20571v1",
        "title": "CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation",
        "summary": "Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems.",
        "authors": "Ayush Sawarni, Jiyuan Tan, Vasilis Syrgkanis",
        "url": "http://arxiv.org/abs/2602.20571v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20571v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "论文提出了一个用于因果推理的基准，旨在解耦因果识别和估计的评估，这对于严格评估自动化因果推理系统至关重要。其基准设计本身就体现了对因果逻辑的深刻理解和严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2602.20511v1",
        "title": "Leveraging Causal Reasoning Method for Explaining Medical Image Segmentation Models",
        "summary": "Medical image segmentation plays a vital role in clinical decision-making, enabling precise localization of lesions and guiding interventions. Despite significant advances in segmentation accuracy, the black-box nature of most deep models has raised growing concerns about their trustworthiness in high-stakes medical scenarios. Current explanation techniques have primarily focused on classification tasks, leaving the segmentation domain relatively underexplored. We introduced an explanation model for segmentation task which employs the causal inference framework and backpropagates the average treatment effect (ATE) into a quantification metric to determine the influence of input regions, as well as network components, on target segmentation areas. Through comparison with recent segmentation explainability techniques on two representative medical imaging datasets, we demonstrated that our approach provides more faithful explanations than existing approaches. Furthermore, we carried out a systematic causal analysis of multiple foundational segmentation models using our method, which reveals significant heterogeneity in perceptual strategies across different models, and even between different inputs for the same model. Suggesting the potential of our method to provide notable insights for optimizing segmentation models. Our code can be found at https://github.com/lcmmai/PdCR.",
        "authors": "Limai Jiang, Ruitao Xie, Bokai Yang, Huazhen Huang, Juan He, Yufu Huo, Zikai Wang, Yang Wei, Yunpeng Cai",
        "url": "http://arxiv.org/abs/2602.20511v1",
        "pdf_url": "https://arxiv.org/pdf/2602.20511v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Impact": 3,
            "Clarity": 4
        },
        "reason_zh": "论文将因果推断框架应用于医学图像分割模型的解释，利用平均处理效应（ATE）作为量化指标来确定输入区域的影响。这展示了将因果逻辑应用于现代AI系统以提高可解释性的严谨方法。"
    }
]