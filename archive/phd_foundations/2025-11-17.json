[
    {
        "id": "http://arxiv.org/abs/2511.13608v1",
        "title": "A Gentle Introduction to Conformal Time Series Forecasting",
        "summary": "Conformal prediction is a powerful post-hoc framework for uncertainty quantification that provides distribution-free coverage guarantees. However, these guarantees crucially rely on the assumption of exchangeability. This assumption is fundamentally violated in time series data, where temporal dependence and distributional shifts are pervasive. As a result, classical split-conformal methods may yield prediction intervals that fail to maintain nominal validity. This review unifies recent advances in conformal forecasting methods specifically designed to address nonexchangeable data. We first present a theoretical foundation, deriving finite-sample guarantees for split-conformal prediction under mild weak-dependence conditions. We then survey and classify state-of-the-art approaches that mitigate serial dependence by reweighting calibration data, dynamically updating residual distributions, or adaptively tuning target coverage levels in real time. Finally, we present a comprehensive simulation study that compares these techniques in terms of empirical coverage, interval width, and computational cost, highlighting practical trade-offs and open research directions.",
        "authors": "M. Stocker, W. Małgorzewicz, M. Fontana, S. Ben Taieb",
        "url": "http://arxiv.org/abs/2511.13608v1",
        "pdf_url": "https://arxiv.org/pdf/2511.13608v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "这篇论文为共形时间序列预测提供了一个坚实的理论基础，特别强调了在时间序列数据中非可交换性假设被违反时的有限样本保证。作为一名数理统计博士生，我非常看重其在不损失普遍性的前提下，为时间序列数据提供了分布无关的覆盖保证和清晰的数学推导。论文提出的理论框架和对弱依赖条件的探讨，完美契合了对统计保证的追求，且其“深入浅出”的风格保证了清晰度。"
    },
    {
        "id": "http://arxiv.org/abs/2511.12869v1",
        "title": "On the Fundamental Limits of LLMs at Scale",
        "summary": "Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.",
        "authors": "Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Zeeshan Memon, Muhammad Ibtsaam Qadir, Sagnik Bhattacharya, Hassan Rizwan, Abhiram R. Gorle, Maahe Zehra Kazmi, Ayesha Mohsin, Muhammad Usman Rafique, Zihao He, Pulkit Mehta, Muhammad Ali Jamshed, John M. Cioffi",
        "url": "http://arxiv.org/abs/2511.12869v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12869v1",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文对大型语言模型 (LLMs) 的根本局限性进行了系统的、基于证明的理论综合分析。它从可计算性、信息论和统计学等角度，正式阐述了LLMs在幻觉、上下文压缩、推理退化等方面的固有理论上限。这种对现代AI系统底层理论界限的严谨数学逻辑分析，以及对定理和经验证据的结合，对于理解和指导未来AI研究具有深远意义，完全符合我对强大理论基础和清晰数学推导的要求。"
    },
    {
        "id": "http://arxiv.org/abs/2511.13895v2",
        "title": "On robust Bayesian causal inference",
        "summary": "This paper develops a Bayesian framework for robust causal inference from longitudinal observational data. Many contemporary methods rely on structural assumptions, such as factor models, to adjust for unobserved confounding, but they can lead to biased causal estimands when mis-specified. We focus on directly estimating time--unit--specific causal effects and use generalised Bayesian inference to quantify model mis-specification and adjust for it, while retaining interpretable posterior inference. We select the learning rate~$ω$ based on a proper scoring rule that jointly evaluates point and interval accuracy of the causal estimand, thus providing a coherent, decision-theoretic foundation for tuning~$ω$. Simulation studies and applications to real data demonstrate improved calibration, sharpness, and robustness in estimating causal effects.",
        "authors": "Angelos Alexopoulos, Nikolaos Demiris",
        "url": "http://arxiv.org/abs/2511.13895v2",
        "pdf_url": "https://arxiv.org/pdf/2511.13895v2",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文提出了一种用于稳健贝叶斯因果推断的新型贝叶斯框架，通过广义贝叶斯推断量化并调整模型误设，同时保留可解释的后验推断。其基于决策理论基础选择学习率，并证明了在估计因果效应方面的校准、尖锐性和鲁棒性。这种对因果逻辑的深入探讨、统计保证的提供，以及其严谨的贝叶斯方法论，对于我专注于将严谨数学逻辑应用于现代AI系统的研究方向非常重要。"
    },
    {
        "id": "http://arxiv.org/abs/2511.13015v2",
        "title": "Geometry Meets Light: Leveraging Geometric Priors for Universal Photometric Stereo under Limited Multi-Illumination Cues",
        "summary": "Universal Photometric Stereo is a promising approach for recovering surface normals without strict lighting assumptions. However, it struggles when multi-illumination cues are unreliable, such as under biased lighting or in shadows or self-occluded regions of complex in-the-wild scenes. We propose GeoUniPS, a universal photometric stereo network that integrates synthetic supervision with high-level geometric priors from large-scale 3D reconstruction models pretrained on massive in-the-wild data. Our key insight is that these 3D reconstruction models serve as visual-geometry foundation models, inherently encoding rich geometric knowledge of real scenes. To leverage this, we design a Light-Geometry Dual-Branch Encoder that extracts both multi-illumination cues and geometric priors from the frozen 3D reconstruction model. We also address the limitations of the conventional orthographic projection assumption by introducing the PS-Perp dataset with realistic perspective projection to enable learning of spatially varying view directions. Extensive experiments demonstrate that GeoUniPS delivers state-of-the-arts performance across multiple datasets, both quantitatively and qualitatively, especially in the complex in-the-wild scenes.",
        "authors": "King-Man Tam, Satoshi Ikehata, Yuta Asano, Zhaoyi An, Rei Kawakami",
        "url": "http://arxiv.org/abs/2511.13015v2",
        "pdf_url": "https://arxiv.org/pdf/2511.13015v2",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文对网络因果推断中的R-学习器进行了系统的、机制性的剖析，并首次严格证明了“表征瓶颈”的存在，具有高度统计显著性 (p < 0.001)。它提供了R-学习器在图结构数据上性能驱动因素的机制性解释，并将GNN的过压问题与“恼人瓶颈”相关联。这种对因果推理方法核心缺陷的理论洞察和严格验证，特别是结合图神经网络的分析，完美符合我对因果逻辑、统计保证和严谨数学推导的偏好。"
    }
]