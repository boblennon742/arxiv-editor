import os
import json
import arxiv
import re
import logging
import time
import random
from google import genai
from google.genai import types
from datetime import date, timedelta

# --- 0. ä¾èµ–æ£€æŸ¥ ---
try:
    import json5 
except ImportError:
    import json as json5
    logger.warning("æœªæ‰¾åˆ° json5 åº“ï¼Œæ­£åœ¨ä½¿ç”¨æ ‡å‡† json åº“ã€‚")

# --- 1. é…ç½® Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- 2. æ ¸å¿ƒé…ç½® ---
GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
ARCHIVE_DIR = "archive"

# 3ä¸ªè¶…çº§æ ¸å¿ƒé…ç½® (V19)
YOUR_DOMAINS_OF_INTEREST = {
    "phd_foundations": {
        "name_zh": "AI ç†è®ºä¸ç»Ÿè®¡åŸºç¡€",
        "name_en": "AI Theory & Statistical Foundations",
        "categories": ['stat.ML', 'cs.LG', 'stat.ME', 'math.ST', 'cs.AI', 'cs.CY', 'math.OC', 'stat.TH', 'cs.CV'],
        "search_query": (
            '("statistical learning theory" OR "nonparametric regression" OR "model selection" OR "high-dimensional inference" OR "uncertainty quantification") OR '
            '("causal inference" OR "fairness" OR "explainable AI" OR "interpretability" OR "treatment effect") OR '
            '("generalization bound" OR "optimization landscape" OR "convergence analysis" OR "deep neural network theory") OR '
            '("high-dimensional statistics" OR "nonparametric estimation" OR "minimax rate" OR "statistical guarantees") OR '
            '("representation learning" OR "metric learning" OR "contrastive learning" OR "self-supervised learning" OR "information bottleneck")'
        ),
        "ai_preference_prompt": """
        æˆ‘æ˜¯ä¸€åæ•°ç†ç»Ÿè®¡åšå£«ç”Ÿï¼Œä¸“æ³¨äºå°†ä¸¥è°¨çš„æ•°å­¦é€»è¾‘åº”ç”¨äºç°ä»£ AI ç³»ç»Ÿã€‚
        æˆ‘å¯»æ±‚çš„è®ºæ–‡å¿…é¡»å…·å¤‡**å¼ºå¤§çš„ç†è®ºåŸºç¡€**ï¼ˆå¦‚ç»Ÿè®¡ä¿è¯ã€ä¼˜åŒ–æ”¶æ•›æ€§ã€å› æœé€»è¾‘ï¼‰å’Œ**æ¸…æ™°çš„æ•°å­¦æ¨å¯¼**ã€‚
        """
    },
    "phd_methods": {
        "name_zh": "å‰æ²¿ AI æ¨¡å‹ä¸åº”ç”¨",
        "name_en": "Frontier AI Models & Applications",
        "categories": ['cs.LG', 'cs.AI', 'cs.SY', 'cs.CL', 'stat.AP', 'cs.CV', 'eess.IV', 'cs.AR'],
        "search_query": (
            '("Offline Reinforcement Learning" OR "Safe RL" OR "exploration" OR "Multi-Agent" OR "Model-Based RL") OR '
            '("Large Language Model" OR "prompt engineering" OR "RAG system" OR "in-context learning" OR "LLM for data analysis") OR '
            '("Vision Transformer" OR "Diffusion Model" OR "Graph Neural Network" OR "multimodal learning") OR '
            '("efficient AI" OR "model compression" OR "knowledge distillation" OR "on-device inference" OR "low-resource ML")'
        ),
        "ai_preference_prompt": """
        æˆ‘æ˜¯ä¸€åæ•°ç†ç»Ÿè®¡åšå£«ç”Ÿï¼Œä¸“æ³¨äº AI çš„å‰æ²¿ç®—æ³•å’Œæ¶æ„ã€‚
        æˆ‘å¯»æ±‚çš„è®ºæ–‡å¿…é¡»**é€»è¾‘æ¸…æ™°**ï¼Œå¹¶èƒ½**è§£å†³å®é™…åº”ç”¨ç“¶é¢ˆ**ï¼ˆå¦‚æ•°æ®æ•ˆç‡ã€æ¨¡å‹å‹ç¼©ã€LLM åº”ç”¨ï¼‰ã€‚
        æˆ‘**ä¸**å–œæ¬¢çº¯ç²¹çš„å·¥ç¨‹å †ç Œï¼Œæ–¹æ³•å¿…é¡»å…·æœ‰**ç†è®ºåˆ›æ–°æ€§**ã€‚
        """
    },
    "quant_crypto": {
        "name_zh": "é‡åŒ–é‡‘è (Crypto)",
        "name_en": "Quantitative Finance (Crypto)",
        "categories": ['q-fin.ST', 'q-fin.CP', 'q-fin.PM', 'cs.CE', 'stat.ML'],
        "search_query": '("cryptocurrency" OR "digital asset" OR "factor investing" OR "algorithmic trading" OR "market microstructure")',
        "ai_preference_prompt": """
        æˆ‘æ­£åœ¨å¸®åŠ©åŒå­¦**æ„é€ åŠ å¯†è´§å¸å¸‚åœºçš„é‡åŒ–å› å­**ã€‚
        æˆ‘éœ€è¦å¯¹**è¿™ä¸ªå…·ä½“ä»»åŠ¡**ï¼ˆå› å­æ„é€ ã€å›æµ‹ã€ç­–ç•¥è®¾è®¡ï¼‰**æœ€æœ‰å¸®åŠ©**çš„è®ºæ–‡ã€‚
        """
    }
}

# --------------------------------------------------------------------------
# æŠ“å–å‡½æ•° (V20 - æŠ—é™æµå¢å¼º)
# --------------------------------------------------------------------------
def fetch_papers_for_domain(domain_name, categories, extra_query, target_date):
    logger.info(f"--- æ­£åœ¨ä¸ºé¢†åŸŸ {domain_name} (æ—¥æœŸ {target_date}) æŠ“å–è®ºæ–‡ ---")
    
    date_str = target_date.strftime("%Y%m%d")
    date_filter = f"submittedDate:[{date_str}0000 TO {date_str}2359]"
    
    category_query = " OR ".join([f"cat:{cat}" for cat in categories])
    full_query = f"({category_query}) AND ({extra_query}) AND {date_filter}"
    
    search = arxiv.Search(
        query=full_query,
        max_results=120, # ç»´æŒ 120 ç¯‡
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending
    )

    papers_list = []
    try:
        # --- å…³é”®ä¿®å¤ï¼šé…ç½® Client å¢åŠ å»¶è¿Ÿå’Œé‡è¯• ---
        client = arxiv.Client(
            page_size=100,
            delay_seconds=8.0,  # å…³é”®ï¼šæ¯æ¬¡ API è¯·æ±‚ç­‰å¾… 8 ç§’
            num_retries=5       # å¢åŠ é‡è¯•æ¬¡æ•°
        )
        # ----------------------------------------------
        
        for result in client.results(search):
            papers_list.append({
                'id': result.entry_id,
                'title': result.title,
                'summary': result.summary.replace("\n", " "),
                'authors': ", ".join([a.name for a in result.authors]),
                'url': result.entry_id,
                'pdf_url': result.pdf_url
            })
        logger.info(f"ä¸º {domain_name} æŠ“å–åˆ° {len(papers_list)} ç¯‡è®ºæ–‡ã€‚")
        return papers_list
    except Exception as e:
        logger.error(f"æŠ“å– arXiv å¤±è´¥: {e}")
        return []

# --------------------------------------------------------------------------
# (V19) AI åˆ†æå‡½æ•° - å¸¦æ™ºèƒ½é‡è¯•æœºåˆ¶ (Top 10-15)
# --------------------------------------------------------------------------
def get_ai_editor_pick(papers, domain_name, user_preference_prompt):
    if not papers:
        logger.info("æ²¡æœ‰è®ºæ–‡å¯ä¾› AI åˆ†æã€‚")
        return None
    if not GEMINI_API_KEY:
        logger.error("æœªæ‰¾åˆ° GEMINI_API_KEYã€‚")
        return None

    client = genai.Client()
    
    prompt_papers = "\n".join(
        [f"--- è®ºæ–‡ {i+1} ---\nID: {p['id']}\næ ‡é¢˜: {p['title']}\næ‘˜è¦: {p['summary']}\n"
         for i, p in enumerate(papers)]
    )

    system_prompt = f"""
    ä½ æ˜¯æˆ‘ï¼ˆç»Ÿè®¡å­¦ç¡•å£«ï¼‰çš„ç§äººç ”ç©¶åŠ©æ‰‹ã€‚
    æˆ‘çš„ä¸ªäººåå¥½ï¼š"{user_preference_prompt}"
    
    ä¸‹é¢æ˜¯ {len(papers)} ç¯‡è®ºæ–‡ã€‚
    ä½ çš„ä»»åŠ¡æ˜¯â€œæ‰¹é‡è¯„åˆ†å’Œç­›é€‰â€ï¼š
    
    1. **è¯„åˆ†ï¼š** æ ¹æ®ä»¥ä¸‹ 4 ä¸ªæ ‡å‡†ï¼ˆ1-5åˆ†ï¼‰ä¸ºæ¯ä¸€ç¯‡è®ºæ–‡æ‰“åˆ†ï¼š
        - Novelty (åˆ›æ–°æ€§): æå‡ºæ–°æ–¹æ³•æˆ–æ–°è§†è§’ (1-5åˆ†)
        - Rigor (ç†è®ºä¸¥è°¨æ€§): æ•°å­¦/ç»Ÿè®¡æ¨å¯¼æ˜¯å¦ä¸¥è°¨ (1-5åˆ†)
        - Impact (å®è·µå½±å“åŠ›): æ˜¯å¦å¯è½åœ°ã€èƒ½æé«˜æ•ˆæœ (1-5åˆ†)
        - Clarity (æ¸…æ™°åº¦): æ˜¯å¦æ·±å…¥æµ…å‡ºã€é€»è¾‘è„‰ç»œæ¸…æ™° (1-5åˆ†)
    2. **ä¼˜é€‰ Top 15**ï¼šè¯·æ ¹æ®æˆ‘çš„åå¥½ï¼ŒæŒ‘é€‰å‡º**æ€»åˆ†æœ€é«˜çš„ 10 åˆ° 15 ç¯‡**è®ºæ–‡ã€‚
    3. **è¯„åˆ†**ï¼šä¸ºæ¯ç¯‡é€‰ä¸­çš„è®ºæ–‡æ‰“åˆ† (Novelty, Rigor, Impact, Clarity)ã€‚
    
    è¯·è¿”å›ä¸€ä¸ª JSON **åˆ—è¡¨**ã€‚å¦‚æœå®åœ¨æ²¡æœ‰å€¼å¾—è¯»çš„ï¼Œè¿”å› `null`ã€‚
    
    JSON æ ¼å¼ç¤ºä¾‹:
    [
      {{
        "id": "è®ºæ–‡ID",
        "scores": {{ "Novelty": 5, "Rigor": 4, "Impact": 5, "Clarity": 4 }},
        "reason_zh": "æ¨èç†ç”±..."
      }}
    ]
    """
    
    full_prompt = f"{system_prompt}\n\n--- è®ºæ–‡åˆ—è¡¨ ---\n{prompt_papers}"

    # --- (V19) å¢å¼ºçš„é‡è¯•é€»è¾‘ ---
    max_retries = 5
    base_delay = 10

    for attempt in range(max_retries):
        try:
            logger.info(f"ğŸš€ è¯·æ±‚ AI åˆ†æ (ç¬¬ {attempt + 1}/{max_retries} æ¬¡å°è¯•)...")
            
            response = client.models.generate_content(
                model='gemini-2.5-flash',
                contents=full_prompt,
                config=types.GenerateContentConfig(temperature=0.3)
            )
            
            cleaned = response.text.strip()
            if cleaned.startswith("```"):
                cleaned = re.sub(r"^```\w*\n", "", cleaned)
                cleaned = re.sub(r"\n```$", "", cleaned)
            cleaned = cleaned.strip()

            if cleaned.lower() == 'null':
                logger.info("AI æ˜ç¡®è¡¨ç¤ºæ²¡æœ‰æ¨è (NULL)ã€‚")
                return None

            match = re.search(r'(\[.*\])', cleaned, re.DOTALL)
            if match:
                cleaned = match.group(1)
            
            # ä½¿ç”¨ json5 å®½å®¹è§£æ
            ai_picks_list = json5.loads(cleaned)
            
            logger.info(f"âœ… AI æˆåŠŸé€‰å‡º {len(ai_picks_list)} ç¯‡ä»Šæ—¥æœ€ä½³ã€‚")
            return ai_picks_list

        except Exception as e:
            logger.warning(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
            if attempt < max_retries - 1:
                # æŒ‡æ•°é€€é¿
                wait_time = base_delay * (2 ** attempt) + random.uniform(0, 3)
                logger.info(f"â³ ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                logger.error("âŒ æ‰€æœ‰é‡è¯•å‡å¤±è´¥ã€‚")
                return None

# --------------------------------------------------------------------------
# å†™å…¥ JSON
# --------------------------------------------------------------------------
def write_to_json(data_to_save, file_path):
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=4)
            if data_to_save:
                logger.info(f"æˆåŠŸå°† {len(data_to_save)} ç¯‡â€œç²¾é€‰â€å†™å…¥ {file_path}")
            else:
                logger.info(f"æ ‡è®° {file_path} ä¸ºâ€œæ— ç²¾é€‰â€ã€‚")
    except Exception as e:
        logger.error(f"å†™å…¥ JSON æ–‡ä»¶å¤±è´¥: {e}")

# --------------------------------------------------------------------------
# ä¸»å‡½æ•°
# --------------------------------------------------------------------------
if __name__ == "__main__":
    target_date = date.today() - timedelta(days=1)
    
    logger.info(f"--- è„šæœ¬å¼€å§‹è¿è¡Œ (V20 æŠ—é™æµç‰ˆ)ï¼Œç›®æ ‡æ—¥æœŸ: {target_date.isoformat()} ---")

    for domain_key, config in YOUR_DOMAINS_OF_INTEREST.items():
        logger.info(f"\n--- å¤„ç†é¢†åŸŸ: {config['name_en']} ---")
        
        papers = fetch_papers_for_domain(
            domain_name=config["name_en"],
            categories=config["categories"], 
            extra_query=config["search_query"], 
            target_date=target_date
        )
        
        picks_list_json = get_ai_editor_pick(papers, config["name_en"], config["ai_preference_prompt"])

        final_data_list = []
        if picks_list_json:
            for pick_item in picks_list_json:
                full_paper = next((p for p in papers if p['id'] == pick_item.get('id')), None)
                if full_paper:
                    final_data_list.append({**full_paper, **pick_item})
        
        if not final_data_list:
             final_data_list = None 

        output_path = os.path.join(ARCHIVE_DIR, domain_key, f"{target_date.isoformat()}.json")
        write_to_json(final_data_list, output_path)

    logger.info(f"\n--- æ‰€æœ‰é¢†åŸŸå¤„ç†å®Œæ¯•: {target_date.isoformat()} ---")
