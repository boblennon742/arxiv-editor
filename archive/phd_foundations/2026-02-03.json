[
    {
        "id": "http://arxiv.org/abs/2602.03823v1",
        "title": "Preference-based Conditional Treatment Effects and Policy Learning",
        "summary": "We introduce a new preference-based framework for conditional treatment effect estimation and policy learning, built on the Conditional Preference-based Treatment Effect (CPTE). CPTE requires only that outcomes be ranked under a preference rule, unlocking flexible modeling of heterogeneous effects with multivariate, ordinal, or preference-driven outcomes. This unifies applications such as conditional probability of necessity and sufficiency, conditional Win Ratio, and Generalized Pairwise Comparisons. Despite the intrinsic non-identifiability of comparison-based estimands, CPTE provides interpretable targets and delivers new identifiability conditions for previous unidentifiable estimands. We present estimation strategies via matching, quantile, and distributional regression, and further design efficient influence-function estimators to correct plug-in bias and maximize policy value. Synthetic and semi-synthetic experiments demonstrate clear performance gains and practical impact.",
        "authors": "Dovid Parnas, Mathieu Even, Julie Josse, Uri Shalit",
        "url": "http://arxiv.org/abs/2602.03823v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03823v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文引入了一个新的基于偏好的条件处理效应（CPTE）框架，用于因果效应估计和策略学习。它明确讨论了比较估计量的固有不可识别性，并提供了新的可识别性条件。论文提出了基于高效影响函数（efficient influence-function）的估计策略，旨在纠正偏差并最大化策略价值，这体现了强大的统计理论基础和严谨的数学推导，非常符合您对因果逻辑和统计保证的偏好。"
    },
    {
        "id": "http://arxiv.org/abs/2602.03756v1",
        "title": "Bayesian variable and hazard structure selection in the General Hazard model",
        "summary": "The proportional hazards (PH) and accelerated failure time (AFT) models are the most widely used hazard structures for analysing time-to-event data. When the goal is to identify variables associated with event times, variable selection is typically performed within a single hazard structure, imposing strong assumptions on how covariates affect the hazard function. To allow simultaneous selection of relevant variables and the hazard structure itself, we develop a Bayesian variable selection approach within the general hazard (GH) model, which includes the PH, AFT, and other structures as special cases. We propose two types of g-priors for the regression coefficients that enable tractable computation and show that both lead to consistent model selection. We also introduce a hierarchical prior on the model space that accounts for multiplicity and penalises model complexity. To efficiently explore the GH model space, we extend the Add-Delete-Swap algorithm to jointly sample variable inclusion indicators and hazard structures. Simulation studies show accurate recovery of both the true hazard structure and active variables across different sample sizes and censoring levels. Two real-data applications are presented to illustrate the use of the proposed methodology and to compare it with existing variable selection methods.",
        "authors": "Yulong Chen, Jim Griffin, Francisco Javier Rubio",
        "url": "http://arxiv.org/abs/2602.03756v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03756v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该研究在广义风险（General Hazard, GH）模型中开发了一种贝叶斯变量选择方法，允许同时选择相关变量和风险结构。论文提出了两种g-先验（g-priors）以实现可计算性，并证明了它们能实现一致的模型选择。此外，还引入了分层先验来处理多重性和模型复杂性。这篇论文在贝叶斯统计推断和模型选择方面具有高度的理论严谨性，提供了明确的统计保证。"
    },
    {
        "id": "http://arxiv.org/abs/2602.03517v1",
        "title": "Rank-Learner: Orthogonal Ranking of Treatment Effects",
        "summary": "Many decision-making problems require ranking individuals by their treatment effects rather than estimating the exact effect magnitudes. Examples include prioritizing patients for preventive care interventions, or ranking customers by the expected incremental impact of an advertisement. Surprisingly, while causal effect estimation has received substantial attention in the literature, the problem of directly learning rankings of treatment effects has largely remained unexplored. In this paper, we introduce Rank-Learner, a novel two-stage learner that directly learns the ranking of treatment effects from observational data. We first show that naive approaches based on precise treatment effect estimation solve a harder problem than necessary for ranking, while our Rank-Learner optimizes a pairwise learning objective that recovers the true treatment effect ordering, without explicit CATE estimation. We further show that our Rank-Learner is Neyman-orthogonal and thus comes with strong theoretical guarantees, including robustness to estimation errors in the nuisance functions. In addition, our Rank-Learner is model-agnostic, and can be instantiated with arbitrary machine learning models (e.g., neural networks). We demonstrate the effectiveness of our method through extensive experiments where Rank-Learner consistently outperforms standard CATE estimators and non-orthogonal ranking methods. Overall, we provide practitioners with a new, orthogonal two-stage learner for ranking individuals by their treatment effects.",
        "authors": "Henri Arno, Dennis Frauen, Emil Javurek, Thomas Demeester, Stefan Feuerriegel",
        "url": "http://arxiv.org/abs/2602.03517v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03517v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文引入了Rank-Learner，一个新颖的两阶段学习器，用于直接从观测数据中学习处理效应的排序。它强调了其Neyman-orthogonal特性，并提供了强大的理论保证，包括对混杂函数估计误差的鲁棒性。这直接解决了因果推断中的一个重要问题，并提供了严谨的统计理论和数学推导，非常符合您的研究兴趣。"
    },
    {
        "id": "http://arxiv.org/abs/2602.03511v1",
        "title": "CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains",
        "summary": "Robust disturbance rejection remains a longstanding challenge in humanoid locomotion, particularly on unstructured terrains where sensing is unreliable and model mismatch is pronounced. While perception information, such as height map, enhances terrain awareness, sensor noise and sim-to-real gaps can destabilize policies in practice. In this work, we provide theoretical analysis that bounds the return gap under observation noise, when the induced latent dynamics are contractive. Furthermore, we present Contractive Mapping for Robustness (CMR) framework that maps high-dimensional, disturbance-prone observations into a latent space, where local perturbations are attenuated over time. Specifically, this approach couples contrastive representation learning with Lipschitz regularization to preserve task-relevant geometry while explicitly controlling sensitivity. Notably, the formulation can be incorporated into modern deep reinforcement learning pipelines as an auxiliary loss term with minimal additional technical effort required. Further, our extensive humanoid experiments show that CMR potently outperforms other locomotion algorithms under increased noise.",
        "authors": "Qixin Zeng, Hongyin Zhang, Shangke Lyu, Junxi Jin, Donglin Wang, Chao Huang",
        "url": "http://arxiv.org/abs/2602.03511v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03511v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该研究提出了Contractive Mapping for Robustness (CMR) 框架，用于人形机器人非结构化地形上的鲁棒运动。论文提供了理论分析，在观测噪声下，当诱导的潜在动力学是收缩的时，可以限制回报差距。通过结合对比表示学习和Lipschitz正则化，它显式地控制了敏感性。这篇论文将严格的控制理论和鲁棒性分析应用于现代AI系统（强化学习），具有很强的理论基础。"
    },
    {
        "id": "http://arxiv.org/abs/2602.03459v1",
        "title": "Causal Inference on Networks under Misspecified Exposure Mappings: A Partial Identification Framework",
        "summary": "Estimating treatment effects in networks is challenging, as each potential outcome depends on the treatments of all other nodes in the network. To overcome this difficulty, existing methods typically impose an exposure mapping that compresses the treatment assignments in the network into a low-dimensional summary. However, if this mapping is misspecified, standard estimators for direct and spillover effects can be severely biased. We propose a novel partial identification framework for causal inference on networks to assess the robustness of treatment effects under misspecifications of the exposure mapping. Specifically, we derive sharp upper and lower bounds on direct and spillover effects under such misspecifications. As such, our framework presents a novel application of causal sensitivity analysis to exposure mappings. We instantiate our framework for three canonical exposure settings widely used in practice: (i) weighted means of the neighborhood treatments, (ii) threshold-based exposure mappings, and (iii) truncated neighborhood interference in the presence of higher-order spillovers. Furthermore, we develop orthogonal estimators for these bounds and prove that the resulting bound estimates are valid, sharp, and efficient. Our experiments show the bounds remain informative and provide reliable conclusions under misspecification of exposure mappings.",
        "authors": "Maresa Schröder, Miruna Oprescu, Stefan Feuerriegel, Nathan Kallus",
        "url": "http://arxiv.org/abs/2602.03459v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03459v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文提出了一个新颖的网络因果推断部分识别框架，用于评估在暴露映射（exposure mapping）被错误指定时处理效应的鲁棒性。它推导了直接和溢出效应的尖锐上下界，并开发了正交估计器，证明其有效性、尖锐性和效率。这在因果推断领域提供了非常强大的理论贡献和统计保证，是您会非常感兴趣的论文。"
    },
    {
        "id": "http://arxiv.org/abs/2602.03392v1",
        "title": "On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models",
        "summary": "Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.",
        "authors": "Shumin Wang, Yuexiang Xie, Wenhao Zhang, Yuchang Sun, Yanxi Chen, Yaliang Li, Yanyong Zhang",
        "url": "http://arxiv.org/abs/2602.03392v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03392v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文为大型语言模型（LLMs）的强化微调（RFT）过程中的熵动态建立了理论框架。它推导了量化熵变化的判别式和一阶表达式，并将其扩展到Group Relative Policy Optimization (GRPO) 的更新公式。这项工作对LLM训练动力学提供了深刻的理论见解，并为优化探索-利用平衡提供了策略，完美契合您对优化收敛性和理论基础的偏好。"
    },
    {
        "id": "http://arxiv.org/abs/2602.03386v1",
        "title": "An Approximate Ascent Approach To Prove Convergence of PPO",
        "summary": "Proximal Policy Optimization (PPO) is among the most widely used deep reinforcement learning algorithms, yet its theoretical foundations remain incomplete. Most importantly, convergence and understanding of fundamental PPO advantages remain widely open. Under standard theory assumptions we show how PPO's policy update scheme (performing multiple epochs of minibatch updates on multi-use rollouts with a surrogate gradient) can be interpreted as approximated policy gradient ascent. We show how to control the bias accumulated by the surrogate gradients and use techniques from random reshuffling to prove a convergence theorem for PPO that sheds light on PPO's success. Additionally, we identify a previously overlooked issue in truncated Generalized Advantage Estimation commonly used in PPO. The geometric weighting scheme induces infinite mass collapse onto the longest $k$-step advantage estimator at episode boundaries. Empirical evaluations show that a simple weight correction can yield substantial improvements in environments with strong terminal signal, such as Lunar Lander.",
        "authors": "Leif Doering, Daniel Schmidt, Moritz Melcher, Sebastian Kassing, Benedikt Wille, Tilman Aach, Simon Weissmann",
        "url": "http://arxiv.org/abs/2602.03386v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03386v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文通过将PPO的策略更新方案解释为近似策略梯度上升，为PPO的收敛性提供了理论证明。它展示了如何控制替代梯度累积的偏差，并利用随机重排技术证明了PPO的收敛定理。这直接解决了深度强化学习算法PPO的理论基础问题，提供了重要的优化收敛性证明，是您会非常感兴趣的论文。"
    },
    {
        "id": "http://arxiv.org/abs/2602.03317v1",
        "title": "Multiparameter Uncertainty Mapping in Quantitative Molecular MRI using a Physics-Structured Variational Autoencoder (PS-VAE)",
        "summary": "Quantitative imaging methods, such as magnetic resonance fingerprinting (MRF), aim to extract interpretable pathology biomarkers by estimating biophysical tissue parameters from signal evolutions. However, the pattern-matching algorithms or neural networks used in such inverse problems often lack principled uncertainty quantification, which limits the trustworthiness and transparency, required for clinical acceptance. Here, we describe a physics-structured variational autoencoder (PS-VAE) designed for rapid extraction of voxelwise multi-parameter posterior distributions. Our approach integrates a differentiable spin physics simulator with self-supervised learning, and provides a full covariance that captures the inter-parameter correlations of the latent biophysical space. The method was validated in a multi-proton pool chemical exchange saturation transfer (CEST) and semisolid magnetization transfer (MT) molecular MRF study, across in-vitro phantoms, tumor-bearing mice, healthy human volunteers, and a subject with glioblastoma. The resulting multi-parametric posteriors are in good agreement with those calculated using a brute-force Bayesian analysis, while providing an orders-of-magnitude acceleration in whole brain quantification. In addition, we demonstrate how monitoring the multi-parameter posterior dynamics across progressively acquired signals provides practical insights for protocol optimization and may facilitate real-time adaptive acquisition.",
        "authors": "Alex Finkelstein, Ron Moneta, Or Zohar, Michal Rivlin, Moritz Zaiss, Dinora Friedmann Morvinski, Or Perlman",
        "url": "http://arxiv.org/abs/2602.03317v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03317v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该研究提出了一个物理结构变分自编码器（PS-VAE），用于定量分子MRI中的多参数不确定性量化。它将可微分的自旋物理模拟器与自监督学习相结合，并提供了捕获潜在生物物理空间参数间相关性的完整协方差。这项工作在贝叶斯推断、不确定性量化和物理模型集成方面具有高度的理论严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2602.03246v1",
        "title": "Joint Network-and-Server Congestion in Multi-Source Traffic Allocation: A Convex Formulation and Price-Based Decentralization",
        "summary": "This paper studies an important rate allocation problem that arises in many networked and distributed systems: steady-state traffic rate allocation from multiple sources to multiple service nodes when both (i) the access-path delay on each source-node route is rate-dependent (capacity-constrained) and convex, and (ii) each service node (also capacity-constrained) experiences a load-dependent queueing delay driven by aggregate load from all sources. We show that the resulting flow-weighted end-to-end delay minimization is a convex program, yielding a global system-optimal solution characterized by KKT conditions that equalize total marginal costs (a path marginal access term plus a node congestion price) across all utilized routes. This condition admits a Wardrop-type interpretation: for each source, all utilized options equalize total marginal cost, while any option with strictly larger total marginal cost receives no flow. Building on this structure, we develop a lightweight distributed pricing-based algorithm in which each service node locally computes and broadcasts a scalar congestion price from its observed aggregate load, while each source updates its traffic split by solving a small separable convex allocation problem under the advertised prices. Numerical illustrations demonstrate convergence of the distributed iteration to the centralized optimum and highlight the trade-offs induced by jointly modeling access and service congestion.",
        "authors": "Tamoghna Sarkar, Bhaskar Krishnamachari",
        "url": "http://arxiv.org/abs/2602.03246v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03246v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "这篇论文将多源流量分配中的网络和服务器拥塞问题建模为一个凸规划问题，得到了由KKT条件表征的全局系统最优解。它进一步开发了一种轻量级的分布式基于价格的算法，并证明了其收敛性。这项工作在优化理论和分布式控制方面具有极高的数学严谨性，且清晰度极高，是您会非常欣赏的论文。"
    },
    {
        "id": "http://arxiv.org/abs/2602.03218v1",
        "title": "Blinded sample size re-estimation accounting for uncertainty in mid-trial estimation",
        "summary": "For randomized controlled trials to be conclusive, it is important to set the target sample size accurately at the design stage. Comparing two normal populations, the sample size calculation requires specification of the variance other than the treatment effect and misspecification can lead to underpowered studies. Blinded sample size re-estimation is an approach to minimize the risk of inconclusive studies. Existing methods proposed to use the total (one-sample) variance that is estimable from blinded data without knowledge of the treatment allocation. We demonstrate that, since the expectation of this estimator is greater than or equal to the true variance, the one-sample variance approach can be regarded as providing an upper bound of the variance in blind reviews. This worst-case evaluation can likely reduce a risk of underpowered studies. However, blinded reviews of small sample size may still lead to underpowered studies. We propose a refined method accounting for estimation error in blind reviews using an upper confidence limit of the variance. A similar idea had been proposed in the setting of external pilot studies. Furthermore, we developed a method to select an appropriate confidence level so that the re-estimated sample size attains the target power. Numerical studies showed that our method works well and outperforms existing methods. The proposed procedure is motivated and illustrated by recent randomized clinical trials.",
        "authors": "Hirotada Maeda, Satoshi Hattori, Tim Friede",
        "url": "http://arxiv.org/abs/2602.03218v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03218v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文提出了一种改进的盲法样本量再估计方法，用于随机对照试验，该方法考虑了中期估计中的不确定性。它利用方差的上限置信区间，并开发了一种选择适当置信水平以达到目标功效的方法。这项工作在统计推断和实验设计方面提供了严谨的理论和方法，具有重要的统计保证。"
    },
    {
        "id": "http://arxiv.org/abs/2602.03119v1",
        "title": "Function-Space Empirical Bayes Regularisation with Large Vision-Language Model Priors",
        "summary": "Bayesian deep learning (BDL) provides a principled framework for reliable uncertainty quantification by combining deep neural networks with Bayesian inference. A central challenge in BDL lies in the design of informative prior distributions that scale effectively to high-dimensional data. Recent functional variational inference (VI) approaches address this issue by imposing priors directly in function space; however, most existing methods rely on Gaussian process (GP) priors, whose expressiveness and generalisation capabilities become limited in high-dimensional regimes. In this work, we propose VLM-FS-EB, a novel function-space empirical Bayes regularisation framework, leveraging large vision-language models (VLMs) to generates semantically meaningful context points. These synthetic samples are then used VLMs for embeddings to construct expressive functional priors. Furthermore, the proposed method is evaluated against various baselines, and experimental results demonstrate that our method consistently improves predictive performance and yields more reliable uncertainty estimates, particularly in out-of-distribution (OOD) detection tasks and data-scarce regimes.",
        "authors": "Pengcheng Hao, Huaze Tang, Ercan Engin Kuruoglu, Wenbo Ding",
        "url": "http://arxiv.org/abs/2602.03119v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03119v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该研究提出了VLM-FS-EB，一个新颖的函数空间经验贝叶斯正则化框架，利用大型视觉语言模型（VLMs）生成语义上有意义的上下文点，以构建富有表现力的函数先验。它为贝叶斯深度学习提供了原则性的不确定性量化框架，解决了高维数据中信息性先验设计的挑战，具有强大的统计和理论严谨性。"
    },
    {
        "id": "http://arxiv.org/abs/2602.03061v1",
        "title": "Evaluating LLMs When They Do Not Know the Answer: Statistical Evaluation of Mathematical Reasoning via Comparative Signals",
        "summary": "Evaluating mathematical reasoning in LLMs is constrained by limited benchmark sizes and inherent model stochasticity, yielding high-variance accuracy estimates and unstable rankings across platforms. On difficult problems, an LLM may fail to produce a correct final answer, yet still provide reliable pairwise comparison signals indicating which of two candidate solutions is better. We leverage this observation to design a statistically efficient evaluation framework that combines standard labeled outcomes with pairwise comparison signals obtained by having models judge auxiliary reasoning chains. Treating these comparison signals as control variates, we develop a semiparametric estimator based on the efficient influence function (EIF) for the setting where auxiliary reasoning chains are observed. This yields a one-step estimator that achieves the semiparametric efficiency bound, guarantees strict variance reduction over naive sample averaging, and admits asymptotic normality for principled uncertainty quantification. Across simulations, our one-step estimator substantially improves ranking accuracy, with gains increasing as model output noise grows. Experiments on GPQA Diamond, AIME 2025, and GSM8K further demonstrate more precise performance estimation and more reliable model rankings, especially in small-sample regimes where conventional evaluation is pretty unstable.",
        "authors": "Zihan Dong, Zhixian Zhang, Yang Zhou, Can Jin, Ruijia Wu, Linjun Zhang",
        "url": "http://arxiv.org/abs/2602.03061v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03061v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "这篇论文设计了一个统计高效的评估框架，用于评估LLMs的数学推理能力，即使它们不知道最终答案。它利用模型判断辅助推理链获得的成对比较信号作为控制变量，开发了一个基于高效影响函数（EIF）的半参数估计器，该估计器达到了半参数效率界限，保证了严格的方差减少，并允许渐近正态性进行不确定性量化。这是对LLM评估的强大统计学贡献。"
    },
    {
        "id": "http://arxiv.org/abs/2602.03004v1",
        "title": "Causal Graph Spatial-Temporal Autoencoder for Reliable and Interpretable Process Monitoring",
        "summary": "To improve the reliability and interpretability of industrial process monitoring, this article proposes a Causal Graph Spatial-Temporal Autoencoder (CGSTAE). The network architecture of CGSTAE combines two components: a correlation graph structure learning module based on spatial self-attention mechanism (SSAM) and a spatial-temporal encoder-decoder module utilizing graph convolutional long-short term memory (GCLSTM). The SSAM learns correlation graphs by capturing dynamic relationships between variables, while a novel three-step causal graph structure learning algorithm is introduced to derive a causal graph from these correlation graphs. The algorithm leverages a reverse perspective of causal invariance principle to uncover the invariant causal graph from varying correlations. The spatial-temporal encoder-decoder, built with GCLSTM units, reconstructs time-series process data within a sequence-to-sequence framework. The proposed CGSTAE enables effective process monitoring and fault detection through two statistics in the feature space and residual space. Finally, we validate the effectiveness of CGSTAE in process monitoring through the Tennessee Eastman process and a real-world air separation process.",
        "authors": "Xiangrui Zhang, Chunyue Song, Wei Dai, Zheng Zhang, Kaihua Gao, Furong Gao",
        "url": "http://arxiv.org/abs/2602.03004v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03004v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该论文提出了因果图时空自编码器（CGSTAE），用于可靠和可解释的过程监控。它引入了一种新颖的三步因果图结构学习算法，利用因果不变性原理的逆向视角来发现不变的因果图。这项工作在因果逻辑和图理论方面具有强大的理论基础，并专注于可解释性，非常符合您的偏好。"
    },
    {
        "id": "http://arxiv.org/abs/2602.02987v1",
        "title": "Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control",
        "summary": "Large Language Models (LLMs) are rapidly becoming critical infrastructure for enterprise applications, driving unprecedented demand for GPU-based inference services. A key operational challenge arises from the two-phase nature of LLM inference: a compute-intensive \\emph{prefill} phase that processes user input, followed by a memory-bound \\emph{decode} phase that generates output tokens. When these phases share GPU resources, prefill tasks throttle the processing speed of concurrent decodes, creating state-dependent contention. This contention is further complicated by workload heterogeneity, as different applications exhibit vastly different input and output lengths. We develop a stochastic control framework for scheduling heterogeneous LLM workloads across large GPU clusters. We formulate LLM inference as a multiclass many-server queueing network with state-dependent service rates, grounded in empirical iteration-time measurements. We analyze the fluid approximation of this system and solve steady-state linear programs that characterize optimal resource allocation. We design gate-and-route policies that regulate prefill admission and decode routing, and prove that they are asymptotically optimal in the many-GPU limit under both bundled and separate token-pricing schemes. We further extend the framework to incorporate Service Level Indicators (SLIs) such as latency and fairness, providing a general approach to constrained scheduling. Numerical experiments calibrated to empirical iteration-time data demonstrate that our policies outperform standard serving heuristics.",
        "authors": "Ruihan Lin, Zezhen Ding, Zean Han, Jiheng Zhang",
        "url": "http://arxiv.org/abs/2602.02987v1",
        "pdf_url": "https://arxiv.org/pdf/2602.02987v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 5
        },
        "reason_zh": "这篇论文为大型GPU集群上的异构LLM工作负载调度开发了一个随机控制框架。它将LLM推理建模为具有状态依赖服务速率的多类多服务器排队网络，并分析了其流体近似，解决了表征最优资源分配的稳态线性规划。论文设计了门控和路由策略，并证明它们在多GPU极限下是渐近最优的。这在优化、控制理论和排队论方面具有极高的数学严谨性，且清晰度极高。"
    },
    {
        "id": "http://arxiv.org/abs/2602.02948v1",
        "title": "Variational Sparse Paired Autoencoders (vsPAIR) for Inverse Problems and Uncertainty Quantification",
        "summary": "Inverse problems are fundamental to many scientific and engineering disciplines; they arise when one seeks to reconstruct hidden, underlying quantities from noisy measurements. Many applications demand not just point estimates but interpretable uncertainty. Providing fast inference alongside uncertainty estimates remains challenging yet desirable in numerous applications.   We propose the Variational Sparse Paired Autoencoder (vsPAIR) to address this challenge. The architecture pairs a standard VAE encoding observations with a sparse VAE encoding quantities of interest, connected through a learned latent mapping. The variational structure enables uncertainty estimation, the paired architecture encourages interpretability by anchoring QoI representations to clean data, and sparse encodings provide structure by concentrating information into identifiable factors rather than diffusing across all dimensions. We also propose modifications to existing sparse VAE methods: a hard-concrete spike-and-slab relaxation for differentiable training and a beta hyperprior for adaptive sparsity levels. To validate the effectiveness of our proposed architecture, we conduct experiments on blind inpainting and computed tomography, demonstrating that vsPAIR is a capable inverse problem solver that can provide interpretable and structured uncertainty estimates.",
        "authors": "Jack Michael Solomon, Rishi Leburu, Matthias Chung",
        "url": "http://arxiv.org/abs/2602.02948v1",
        "pdf_url": "https://arxiv.org/pdf/2602.02948v1",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Impact": 4,
            "Clarity": 4
        },
        "reason_zh": "该研究提出了变分稀疏配对自编码器（vsPAIR），用于逆问题和不确定性量化。该架构将标准VAE与稀疏VAE配对，并通过学习的潜在映射连接。变分结构实现了不确定性估计，配对架构通过将感兴趣量（QoI）表示锚定到干净数据来增强可解释性，稀疏编码通过将信息集中到可识别因子中来提供结构。这在贝叶斯推断和不确定性量化方面具有强大的理论严谨性。"
    }
]