[
    {
        "id": "http://arxiv.org/abs/2512.04747v1",
        "title": "A Tutorial on Regression Analysis: From Linear Models to Deep Learning -- Lecture Notes on Artificial Intelligence",
        "summary": "This article serves as the regression analysis lecture notes in the Intelligent Computing course cluster (including the courses of Artificial Intelligence, Data Mining, Machine Learning, and Pattern Recognition). It aims to provide students -- who are assumed to possess only basic university-level mathematics (i.e., with prerequisite courses in calculus, linear algebra, and probability theory) -- with a comprehensive and self-contained understanding of regression analysis without requiring any additional references. The lecture notes systematically introduce the fundamental concepts, modeling components, and theoretical foundations of regression analysis, covering linear regression, logistic regression, multinomial logistic regression, polynomial regression, basis-function models, kernel-based methods, and neural-network-based nonlinear regression. Core methodological topics include loss-function design, parameter-estimation principles, ordinary least squares, gradient-based optimization algorithms and their variants, as well as regularization techniques such as Ridge and LASSO regression. Through detailed mathematical derivations, illustrative examples, and intuitive visual explanations, the materials help students understand not only how regression models are constructed and optimized, but also how they reveal the underlying relationships between features and response variables. By bridging classical statistical modeling and modern machine-learning practice, these lecture notes aim to equip students with a solid conceptual and technical foundation for further study in advanced artificial intelligence models.",
        "authors": "Jingyuan Wang, Jiahao Ji",
        "url": "http://arxiv.org/abs/2512.04747v1",
        "pdf_url": "https://arxiv.org/pdf/2512.04747v1",
        "type": "基础核心",
        "scores": {
            "Novelty": 3,
            "Rigor": 5,
            "Clarity": 5,
            "Utility": 5
        },
        "core_value_zh": "涵盖经典统计回归到深度学习的严谨教程，是补全统计与数据科学核心基础的理想讲义。",
        "reason_zh": "这篇讲义明确为“回归分析教程”，内容从线性模型到深度学习，涵盖损失函数、参数估计、优化算法和正则化等核心方法。它强调数学推导和理论基础，并旨在为具备基础数学背景的学生提供全面、自洽的理解，非常符合博士生对数学严谨性、逻辑清晰且能补全基础的“硬核资料”需求。"
    },
    {
        "id": "http://arxiv.org/abs/2512.03768v1",
        "title": "Deep Unfolding: Recent Developments, Theory, and Design Guidelines",
        "summary": "Optimization methods play a central role in signal processing, serving as the mathematical foundation for inference, estimation, and control. While classical iterative optimization algorithms provide interpretability and theoretical guarantees, they often rely on surrogate objectives, require careful hyperparameter tuning, and exhibit substantial computational latency. Conversely, machine learning (ML ) offers powerful data-driven modeling capabilities but lacks the structure, transparency, and efficiency needed for optimization-driven inference. Deep unfolding has recently emerged as a compelling framework that bridges these two paradigms by systematically transforming iterative optimization algorithms into structured, trainable ML architectures. This article provides a tutorial-style overview of deep unfolding, presenting a unified perspective of methodologies for converting optimization solvers into ML models and highlighting their conceptual, theoretical, and practical implications. We review the foundations of optimization for inference and for learning, introduce four representative design paradigms for deep unfolding, and discuss the distinctive training schemes that arise from their iterative nature. Furthermore, we survey recent theoretical advances that establish convergence and generalization guarantees for unfolded optimizers, and provide comparative qualitative and empirical studies illustrating their relative trade-offs in complexity, interpretability, and robustness.",
        "authors": "Nir Shlezinger, Santiago Segarra, Yi Zhang, Dvir Avrahami, Zohar Davidov, Tirza Routtenberg, Yonina C. Eldar",
        "url": "http://arxiv.org/abs/2512.03768v1",
        "pdf_url": "https://arxiv.org/pdf/2512.03768v1",
        "type": "基础核心",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "深度展开的教程，系统阐述优化算法与机器学习融合的理论基础、收敛性和泛化保证。",
        "reason_zh": "这篇是关于“深度展开”的教程，它将迭代优化算法转化为可训练的ML架构，并深入探讨其理论基础、收敛性和泛化保证。对于数理统计博士生而言，这不仅提供了优化方法与机器学习结合的前沿视角，更以严谨的数学方式补全了在凸优化和ML理论交叉领域的核心基础。"
    },
    {
        "id": "http://arxiv.org/abs/2512.02970v1",
        "title": "Identification of Multivariate Measurement Error Models",
        "summary": "This paper develops new identification results for multidimensional continuous measurement-error models where all observed measurements are contaminated by potentially correlated errors and none provides an injective mapping of the latent distribution. Using third order cross moments, the paper constructs a three way tensor whose unique decomposition, guaranteed by Kruskal theorem, identifies the factor loading matrices. Starting with a linear structure, the paper recovers the full distribution of latent factors by constructing suitable measurements and applying scalar or multivariate versions of Kotlarski identity. As a result, the joint distribution of the latent vector and measurement errors is fully identified without requiring injective measurements, showing that multivariate latent structure can be recovered in broader settings than previously believed. Under injectivity, the paper also provides user-friendly testable conditions for identification. Finally, this paper provides general identification results for nonlinear models using a newly-defined generalized Kruskal rank - signal rank - of intergral operators. These results have wide applicability in empirical work involving noisy or indirect measurements, including factor models, survey data with reporting errors, mismeasured regressors in econometrics, and multidimensional latent-trait models in psychology and marketing, potentially enabling more robust estimation and interpretation when clean measurements are unavailable.",
        "authors": "Yingyao Hu",
        "url": "http://arxiv.org/abs/2512.02970v1",
        "pdf_url": "https://arxiv.org/pdf/2512.02970v1",
        "type": "基础核心",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "针对多变量测量误差模型识别的严谨数学方法，利用高阶矩和张量分解等工具，补全高维统计和因果推断的数学拼图。",
        "reason_zh": "这篇论文提出了多变量测量误差模型的新识别结果，利用三阶交叉矩、Kruskal定理和Kotlarski恒等式等高级数学工具，严谨地识别潜在因子分布。其数学深度和对统计模型识别这一核心问题的探讨，使其成为补全博士在高维统计和因果推断领域数学基础的硬核资料。"
    },
    {
        "id": "http://arxiv.org/abs/2512.02914v1",
        "title": "Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning",
        "summary": "Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of a reasoning process.",
        "authors": "Zhonghao He, Tianyi Qiu, Hirokazu Shirado, Maarten Sap",
        "url": "http://arxiv.org/abs/2512.02914v1",
        "pdf_url": "https://arxiv.org/pdf/2512.02914v1",
        "type": "基础核心",
        "scores": {
            "Novelty": 4,
            "Rigor": 5,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "运用贝叶斯统计中的鞅性质，严谨评估LLM推理的理性，深化对高等概率论和贝叶斯推断的理解。",
        "reason_zh": "该研究利用贝叶斯统计中的鞅性质来评估LLM推理的贝叶斯理性，提出了无监督的鞅分数。它不仅将高等概率论（鞅）和贝叶斯推断的核心概念应用于前沿LLM领域，其对“理性信念更新”的数学阐释，也为博士生提供了理解这些基础概念的严谨视角和实用案例。"
    },
    {
        "id": "http://arxiv.org/abs/2512.03719v1",
        "title": "Over-the-Air Federated Learning: Rethinking Edge AI Through Signal Processing",
        "summary": "Over-the-Air Federated Learning (AirFL) is an emerging paradigm that tightly integrates wireless signal processing and distributed machine learning to enable scalable AI at the network edge. By leveraging the superposition property of wireless signals, AirFL performs communication and model aggregation of the learning process simultaneously, significantly reducing latency, bandwidth, and energy consumption. This article offers a tutorial treatment of AirFL, presenting a novel classification into three design approaches: CSIT-aware, blind, and weighted AirFL. We provide a comprehensive guide to theoretical foundations, performance analysis, complexity considerations, practical limitations, and prospective research directions.",
        "authors": "Seyed Mohammad Azimi-Abarghouyi, Carlo Fischione, Kaibin Huang",
        "url": "http://arxiv.org/abs/2512.03719v1",
        "pdf_url": "https://arxiv.org/pdf/2512.03719v1",
        "type": "基础核心",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Clarity": 4,
            "Utility": 4
        },
        "core_value_zh": "深入探讨空口联邦学习的理论基础，融合无线信号处理与分布式机器学习，为边缘AI提供严谨的系统级理解。",
        "reason_zh": "这篇教程对空口联邦学习（AirFL）进行了系统性的理论处理，深入探讨了其理论基础、性能分析和复杂性考量，并将其与无线信号处理和分布式机器学习紧密结合。对于数理统计博士生而言，它提供了一个严谨的框架来理解前沿分布式AI系统的数学和工程基础，有助于补全在高级优化和分布式系统方面的知识拼图。"
    },
    {
        "id": "http://arxiv.org/abs/2512.04841v1",
        "title": "SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security",
        "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the causal factors behind such vulnerabilities is essential for building reliable defenses.   In this work, we introduce a unified causality analysis framework that systematically supports all levels of causal investigation in LLMs, ranging from token-level, neuron-level, and layer-level interventions to representation-level analysis. The framework enables consistent experimentation and comparison across diverse causality-based attack and defense methods. Accompanying this implementation, we provide the first comprehensive survey of causality-driven jailbreak studies and empirically evaluate the framework on multiple open-weight models and safety-critical benchmarks including jailbreaks, hallucination detection, backdoor identification, and fairness evaluation. Our results reveal that: (1) targeted interventions on causally critical components can reliably modify safety behavior; (2) safety-related mechanisms are highly localized (i.e., concentrated in early-to-middle layers with only 1--2\\% of neurons exhibiting causal influence); and (3) causal features extracted from our framework achieve over 95\\% detection accuracy across multiple threat types.   By bridging theoretical causality analysis and practical model safety, our framework establishes a reproducible foundation for research on causality-based attacks, interpretability, and robust attack detection and mitigation in LLMs. Code is available at https://github.com/Amadeuszhao/SOK_Casuality.",
        "authors": "Wei Zhao, Zhe Li, Jun Sun",
        "url": "http://arxiv.org/abs/2512.04841v1",
        "pdf_url": "https://arxiv.org/pdf/2512.04841v1",
        "type": "前沿深度",
        "scores": {
            "Novelty": 5,
            "Rigor": 5,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "针对LLM安全性的统一因果分析框架，深入剖析LLM理论机制，为未来研究提供方向。",
        "reason_zh": "这篇是关于LLM安全性的“知识体系”（SoK）论文，提出了一个统一的因果分析框架，系统性地支持LLM在不同层面的因果研究。它不仅是LLM理论机制和因果推断的完美结合，其对安全机制局部化和因果特征检测的发现，也为未来的LLM安全研究指明了方向，具有极高的前沿性和理论深度。"
    },
    {
        "id": "http://arxiv.org/abs/2512.04770v1",
        "title": "Embodied Co-Design for Rapidly Evolving Agents: Taxonomy, Frontiers, and Challenges",
        "summary": "Brain-body co-evolution enables animals to develop complex behaviors in their environments. Inspired by this biological synergy, embodied co-design (ECD) has emerged as a transformative paradigm for creating intelligent agents-from virtual creatures to physical robots-by jointly optimizing their morphologies and controllers rather than treating control in isolation. This integrated approach facilitates richer environmental interactions and robust task performance. In this survey, we provide a systematic overview of recent advances in ECD. We first formalize the concept of ECD and position it within related fields. We then introduce a hierarchical taxonomy: a lower layer that breaks down agent design into three fundamental components-controlling brain, body morphology, and task environment-and an upper layer that integrates these components into four major ECD frameworks: bi-level, single-level, generative, and open-ended. This taxonomy allows us to synthesize insights from more than one hundred recent studies. We further review notable benchmarks, datasets, and applications in both simulated and real-world scenarios. Finally, we identify significant challenges and offer insights into promising future research directions. A project associated with this survey has been created at https://github.com/Yuxing-Wang-THU/SurveyBrainBody.",
        "authors": "Yuxing Wang, Zhiyu Chen, Tiantian Zhang, Qiyue Yin, Yongzhe Chang, Zhiheng Li, Liang Wang, Xueqian Wang",
        "url": "http://arxiv.org/abs/2512.04770v1",
        "pdf_url": "https://arxiv.org/pdf/2512.04770v1",
        "type": "前沿深度",
        "scores": {
            "Novelty": 4,
            "Rigor": 4,
            "Clarity": 5,
            "Utility": 5
        },
        "core_value_zh": "具身协同设计（ECD）的系统综述，为智能体形态与控制器联合优化提供分类、前沿和挑战，指引RL研究方向。",
        "reason_zh": "这篇综述系统性地概述了具身协同设计（ECD）的最新进展，提出了分层分类法，并总结了该领域的前沿和挑战。对于关注Offline RL和AI智能体的博士生而言，它提供了开阔的视野，帮助理解智能体设计与控制的未来方向，并指引潜在的研究问题。"
    },
    {
        "id": "http://arxiv.org/abs/2512.03967v1",
        "title": "Technical Report on Text Dataset Distillation",
        "summary": "In the vision domain, dataset distillation arises as a technique to condense a large dataset into a smaller synthetic one that exhibits a similar result in the training process. While image data presents an extensive literature of distillation methods, text dataset distillation has fewer works in comparison. Text dataset distillation initially grew as an adaptation of efforts from the vision universe, as the particularities of the modality became clear obstacles, it rose into a separate branch of research. Several milestones mark the development of this area, such as the introduction of methods that use transformer models, the generation of discrete synthetic text, and the scaling to decoder-only models with over 1B parameters. Despite major advances in modern approaches, the field remains in a maturing phase, with room for improvement on benchmarking standardization, approaches to overcome the discrete nature of text, handling complex tasks, and providing explicit examples of real-world applications. In this report, we review past and recent advances in dataset distillation for text, highlighting different distillation strategies, key contributions, and general challenges.",
        "authors": "Keith Ando Ogawa, Bruno Lopes Yamamoto, Lucas Lauton de Alcantara, Victor Zacarias, Edson Bollis, Lucas Pellicer, Rosimeire Pereira Costa, Anna Helena Reali Costa, Artur Jordao",
        "url": "http://arxiv.org/abs/2512.03967v1",
        "pdf_url": "https://arxiv.org/pdf/2512.03967v1",
        "type": "前沿深度",
        "scores": {
            "Novelty": 4,
            "Rigor": 3,
            "Clarity": 5,
            "Utility": 5
        },
        "core_value_zh": "文本数据集蒸馏的全面技术报告，深入探讨LLM数据效率与合成数据生成的前沿进展与挑战。",
        "reason_zh": "这份技术报告全面回顾了文本数据集蒸馏的进展，包括不同蒸馏策略、关键贡献和面临的挑战。对于关注LLM理论机制的博士生，了解数据蒸馏如何影响模型训练和性能至关重要，它提供了该领域的前沿视角和未来研究方向。"
    },
    {
        "id": "http://arxiv.org/abs/2512.02527v1",
        "title": "A Concise Review of Hallucinations in LLMs and their Mitigation",
        "summary": "Traditional language models face a challenge from hallucinations. Their very presence casts a large, dangerous shadow over the promising realm of natural language processing. It becomes crucial to understand the various kinds of hallucinations that occur nowadays, their origins, and ways of reducing them. This document provides a concise and straightforward summary of that. It serves as a one-stop resource for a general understanding of hallucinations and how to mitigate them.",
        "authors": "Parth Pulkundwar, Vivek Dhanawade, Rohit Yadav, Minal Sonkar, Medha Asurlekar, Sarita Rathod",
        "url": "http://arxiv.org/abs/2512.02527v1",
        "pdf_url": "https://arxiv.org/pdf/2512.02527v1",
        "type": "前沿深度",
        "scores": {
            "Novelty": 3,
            "Rigor": 3,
            "Clarity": 5,
            "Utility": 5
        },
        "core_value_zh": "LLM幻觉现象及其缓解策略的精炼综述，为理解LLM理论机制和挑战提供快速入口。",
        "reason_zh": "这篇综述简洁明了地总结了LLM幻觉的种类、来源及缓解方法。对于数理统计博士生而言，理解LLM幻觉是深入研究LLM理论机制的关键一环，这篇综述提供了一个高效且全面的前沿知识概览，有助于快速把握该领域的最新进展和挑战。"
    },
    {
        "id": "http://arxiv.org/abs/2512.01354v2",
        "title": "The Necessity of Imperfection:Reversing Model Collapse via Simulating Cognitive Boundedness",
        "summary": "Although synthetic data is widely promoted as a remedy, its prevailing production paradigm -- one optimizing for statistical smoothness -- systematically removes the long-tail, cognitively grounded irregularities that characterize human text. Prolonged training on such statistically optimal but cognitively impoverished data accelerates model collapse.   This paper proposes a paradigm shift: instead of imitating the surface properties of data, we simulate the cognitive processes that generate human text. We introduce the Prompt-driven Cognitive Computing Framework (PMCSF), whose core consists of a Cognitive State Decoder (CSD) that reverse-engineers unstructured text into structured cognitive vectors, and a Cognitive Text Encoder (CTE) that re-materializes these states into text enriched with human-typical imperfections via mathematically defined Cognitive Perturbation Operators.   The framework is validated through a two-stage objective evaluation pipeline. First, in cognitive codec verification, CTE text yields a Jensen-Shannon divergence of 0.0614 from human text (vs. 0.4431 for standard LLM output), passes double-blind professional media review, and achieves an intraclass correlation coefficient ICC > 0.9 for cognitive profile alignment across heterogeneous models. Second, in functional gain evaluation, isomorphic stress tests in the A-share market show that strategies incorporating CTE-generated data reduce maximum drawdown by 47.4% during the 2015 crash and deliver 8.6% Defensive Alpha, exceeding transaction costs by a factor of 33.   Our findings demonstrate that modelling human cognitive limitations -- not copying surface data -- enables synthetic data with genuine functional gain, offering a viable technical pathway toward resolving the AI data-collapse crisis.",
        "authors": "Zhongjie Jiang",
        "url": "http://arxiv.org/abs/2512.01354v2",
        "pdf_url": "https://arxiv.org/pdf/2512.01354v2",
        "type": "前沿深度",
        "scores": {
            "Novelty": 5,
            "Rigor": 4,
            "Clarity": 4,
            "Utility": 5
        },
        "core_value_zh": "通过模拟认知有限性生成合成数据，逆转LLM模型崩溃的新范式，为LLM理论机制研究提供创新方向。",
        "reason_zh": "这篇论文提出了一种创新范式，通过模拟人类认知局限性来生成合成数据，以解决LLM模型崩溃问题。它引入了认知计算框架和数学定义的认知扰动算子，具有极高的创新性和理论深度，直接触及LLM理论机制的核心挑战，为博士生提供了开创性的研究思路。"
    }
]